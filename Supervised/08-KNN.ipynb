{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fbb517",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2025-06-05\n",
    "\n",
    "**Last update:** 2025-09-24\n",
    "\n",
    "**MIT License** ‚Äî Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Ume√• University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f4257",
   "metadata": {},
   "source": [
    "üì¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget üôÇ\n",
    "\n",
    "* Your answers for the \"‚ö° Mandatory\" sections of each lab <span style=\"color:red\"><strong>must be submitted before the start of the next lab session</strong></span>.\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dab76",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Purpose and Learning Outcomes:\n",
    "\n",
    "In this lab, you will learn about the K-Nearest Neighbors (KNN) algorithm for regression and classification tasks. You will understand how KNN works, how to implement it from scratch, and how to use it from scikit-learn, and how to evaluate its performance on a dataset.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "from notebook_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05a494",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN) \n",
    "\n",
    "**Overview:** k-NN is a simple, non-parametric, and instance-based method used for classification and regression tasks. Instead of learning an explicit model, k-NN makes predictions by finding the k closest data points (neighbors) to a given query point, using a distance metric such as Euclidean distance. \n",
    "\n",
    "This lab consists of two parts: Regression and Classification.\n",
    "\n",
    "## k-NN for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a65455",
   "metadata": {},
   "source": [
    "You are familiar with the functions below from linear regression labs. The functions generate data for the motion of a projectile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the true y values for the motion of a projectile\n",
    "# y = -(1/2)*g*t^2 + v0*t + y0\n",
    "#\n",
    "def projectile_true(t, g=9.8, v0=25, y0=0):\n",
    "    # The projectile equation is y = -(1/2)*g*t^2 + v0*t + y0\n",
    "    return -0.5*g*t**2 + v0*t + y0 # true trajectory\n",
    "\n",
    "def projectile_motion(g, v0, y0, time_range=(0, 5), num_points=100, noise_level=5, seed=42):\n",
    "    np.random.seed(seed)  # For reproducability\n",
    "    \n",
    "    # time for the projectile motsion\n",
    "    t = np.linspace(time_range[0], time_range[1], num_points)\n",
    " \n",
    "    y_true  = projectile_true(t, g, v0, y0) # true trajectory\n",
    "    y_noisy = y_true + np.random.normal(0, noise_level, num_points) # noisy data\n",
    "    return t, y_true, y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a238f3",
   "metadata": {},
   "source": [
    "Let's create some data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afe847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projectile(t, y_true, y_noisy):\n",
    "    plt.figure()\n",
    "    plt.scatter(t, y_noisy, s=50, color=colors[7], edgecolors='k', alpha=0.4, label=\"Noisy data\")\n",
    "    plt.plot(t, y_true, color=colors[0], linestyle=\"--\", linewidth=2, label=\"Actual trajectory\")\n",
    "\n",
    "    # Customize plot appearance\n",
    "    plt.xlabel(\"Time (s)\", fontsize=14)\n",
    "    plt.ylabel(\"Displacement (m)\", fontsize=14)\n",
    "    plt.title(\"Projectile motion\", fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Generate data and split\n",
    "# Parameters for the projectile motion simulation\n",
    "v0 = 20.0  # initial velocity (m/s)\n",
    "g  = 9.8   # gravity acceleration (m/s^2)\n",
    "y0 = 0.0   # initial position/height/displacement (m)\n",
    "n  = 200   # number of points (#)\n",
    "time_range  = (0, 5)  # time range from 0 to 5 seconds\n",
    "noise_level = 3  # standard deviation of the noise\n",
    "t, y_true, y_noisy = projectile_motion(g=g, v0=v0, y0=y0, \n",
    "                                       time_range=time_range, \n",
    "                                       num_points=n, \n",
    "                                       noise_level=noise_level)\n",
    "\n",
    "plot_projectile(t, y_true, y_noisy)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0520f2",
   "metadata": {},
   "source": [
    "Previously, and in linear regression labs, you have seen how to fit a linear model to data. However, not all relationships in data are linear. In this section, we will explore a general technique called k-NN regression that can be applied to both linear and non-linear data. When using k-NN regression, we do not assume any specific form for the relationship between input features and the target variable. Instead, we rely on the proximity of data points to make predictions.\n",
    "\n",
    "The code section below implements a custom k-NN regression model to predict the displacement of a projectile over time, based on noisy simulated data. The `KNNRegressor` class contains methods to `fit` the model by storing training data, and to make `predictions` for new input points by calculating the distance between each test point and all training points, identifying the k closest neighbors, and averaging their target values to generate the prediction. \n",
    "\n",
    "The script splits the data into training and testing sets. It trains the k-NN regressor on the training data, predicts the displacement over a dense range of time points to produce a smooth curve, and finally visualizes the true motion, noisy training data, and the model's predictions. Here, I've not used the test data, because I want to show you the smooth curve of predictions over the entire time range. Therefore, I've generated a dense set of time points for prediction. The test data, however, will be used later to evaluate the model's performance.\n",
    "\n",
    "I suppose you are familiar with the general concepts of OOP (Object-Oriented Programming) in Python. However, if you still do not feel confident, what you should do it to move to the first line after the class definition and start analyzing the code line by line from there and find the associated function in the class.\n",
    "\n",
    "As stated in the lecture notes, k-NN is computationally intensive during the prediction phase. This you can already see by comparing the `fit` and `predict` methods. The `fit` method simply stores the training data, while the `predict` method involves calculating distances and sorting to find the nearest neighbors for each test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Own developed k-Nearest Neighbors Regressor\n",
    "class KNNRegressor:\n",
    "    def __init__(self, k):\n",
    "        self.k = k  # number of nearest neighbors\n",
    "        \n",
    "    # Take a copy of X_train and y_train values.\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train.flatten() #  Convert a 2D array made by train_test_split (n_samples, 1) to a 1D array of shape (n_samples)\n",
    "        self.y_train = y_train # y_train is already in 1D format; no need to be flattened.\n",
    "    \n",
    "    # generates predicted values for each input using the kNN method.\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "\n",
    "        # Walk through all test samples. You see that this is a loop over all test samples, \n",
    "        # which we have replaced with dense X.\n",
    "        for x in X_test.flatten():\n",
    "            # Step 1/5: Compute the absolute distance (here, 1D Euclidean) between each \n",
    "            # training point and the test point x.\n",
    "            distances = np.abs(self.X_train - x)  # 1D Euclidean distance. In higher dimensions, use np.linalg.norm(self.X_train - x, axis=1)\n",
    "            \n",
    "            # Step 2/5 and 3/5: Sorts the distances from smallest to largest, and \n",
    "            # return the indices of the k nearest neighbors\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            \n",
    "            # Step 4/5: Extract the target values (y_train) of the k nearest neighbors.\n",
    "            target_values = self.y_train[k_indices]\n",
    "\n",
    "            # Step 5/5: compute the majority voting. Here I've used the average (mean) of \n",
    "            # the target values (y_train) of the k nearest neighbors.\n",
    "            y_pred = np.mean(target_values)\n",
    "\n",
    "            predictions.append(y_pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    # Plotting function\n",
    "    def plot(self, x_true, y_true, x_train, y_train, x_pred, y_pred, k):\n",
    "        plt.figure()\n",
    "        plt.scatter(x_train, y_train, s=50, color=colors[7], edgecolors='k', alpha=0.4, label=\"Training noisy data\")\n",
    "        plt.plot(x_true, y_true, color=colors[0], linestyle=\"--\", linewidth=2, label=\"Actual trajectory\")\n",
    "        plt.plot(x_pred, y_pred, color=colors[1], label=f\"kNN Prediction (k={k})\")\n",
    "\n",
    "        # Customize plot appearance\n",
    "        plt.xlabel(\"Time (s)\", fontsize=14)\n",
    "        plt.ylabel(\"Displacement (m)\", fontsize=14)\n",
    "        plt.title(f\"kNN Prediction (k={k})\", fontsize=16)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# ========== MAIN ==========\n",
    "# Reshape t for compatibility with sklearn's train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(t.reshape(-1, 1), y_noisy, test_size=0.3, random_state=42)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "k = 5  # number of nearest neighbors\n",
    "\n",
    "# Create the model\n",
    "knn_model = KNNRegressor(k=k)  # only the __init__ method from the class is called here.\n",
    "\n",
    "# Fit the model\n",
    "knn_model.fit(x_train, y_train)\n",
    "\n",
    "# Create dense X values to show smooth prediction curve\n",
    "x_dense = np.linspace(time_range[0], time_range[1], n).reshape(-1, 1)\n",
    "\n",
    "# Predict y values for the dense X values (not the test data)\n",
    "y_dense_pred = knn_model.predict(x_dense)\n",
    "\n",
    "# Plot the results\n",
    "knn_model.plot(t, y_true, x_train, y_train, x_dense, y_dense_pred, k)  # data prediction and visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8d679",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Carefully read and analyze the `predict` method of the KNNRegressor class and compare the steps with what you have learned in the lecture notes (see KNN algorithm slide and mainly see the \"Steps\" section).\n",
    "\n",
    "- Change the value of `k` to 1, 2, 5, 9, 25, and 50. Observe how the predictions change with different values of `k`. What do you notice about the model's behavior as `k` increases or decreases? Any signs of overfitting or underfitting? Explain your observations.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46186468",
   "metadata": {},
   "source": [
    "We want to evaluate the model's performance on the test data. For this, we will use different metrics. Below, I've developed another class that computes and visualizes these metrics. I've limited the metrics to three common ones: Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. You can, of course, add more metrics if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Validtor for any KNN model. I've made it generic to be used for any KNN model\n",
    "class KNNValidator:\n",
    "    # x_train, y_train: training data\n",
    "    # x_test,  y_test : test data\n",
    "    # model_class: the KNN model class to be evaluated, e.g., KNNRegressor\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, model_class):\n",
    "        # Take an instance copy (with reference) of the input parameters.\n",
    "        self.x_train     = x_train\n",
    "        self.y_train     = y_train\n",
    "        self.x_test      = x_test\n",
    "        self.y_test      = y_test\n",
    "        self.model_class = model_class\n",
    "\n",
    "    # For a range of k-values, perform model evaluation and\n",
    "    # the required metrics, like MSE, MAE, amd R2\n",
    "    def evaluate(self, k_values):\n",
    "        self.results = []       # Combine all results\n",
    "        \n",
    "        # For different k values, predict values and measure model accuracy\n",
    "        for k in k_values:\n",
    "            # Instance of the model class (e.g., KNNRegressor)\n",
    "            model = self.model_class(k=k)\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "            \n",
    "            # Predict valus\n",
    "            y_pred = model.predict(self.x_test)\n",
    "            \n",
    "            mse = mean_squared_error (self.y_test , y_pred)\n",
    "            mae = mean_absolute_error(self.y_test , y_pred)\n",
    "            r2  = r2_score(self.y_test, y_pred)\n",
    "\n",
    "            self.results.append({\n",
    "                'k'     : k,\n",
    "                'mse'   : mse,\n",
    "                'mae'   : mae,\n",
    "                'r2'    : r2,\n",
    "                'y_pred': y_pred\n",
    "            })\n",
    "                \n",
    "        return self.results\n",
    "\n",
    "    # Plot all metrics\n",
    "    def plot_metrics(self):\n",
    "        ks   = [r['k']   for r in self.results]\n",
    "        mses = [r['mse'] for r in self.results]\n",
    "        maes = [r['mae'] for r in self.results]\n",
    "        r2s  = [r['r2']  for r in self.results]\n",
    "\n",
    "        # Plot all metrics\n",
    "        plt.figure(figsize=(10, 3), dpi=200)\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(ks, mses, marker='o')\n",
    "        plt.xlabel(\"k\", fontsize=14)\n",
    "        plt.ylabel(\"MSE\", fontsize=14)\n",
    "        plt.title(f\"Mean Squared Error\", fontsize=16)\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(ks, maes, marker='o', color='orange')\n",
    "        plt.xlabel(\"k\", fontsize=14)\n",
    "        plt.ylabel(\"MAE\", fontsize=14)\n",
    "        plt.title(\"Mean Absolute Error\", fontsize=16)\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(ks, r2s, marker='o', color='green')\n",
    "        plt.xlabel(\"k\", fontsize=14)\n",
    "        plt.ylabel(\"R2 Score\", fontsize=14)\n",
    "        plt.title(\"R2 Score\", fontsize=16)\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# ========== MAIN ==========\n",
    "# Validate across multiple k values\n",
    "validator = KNNValidator(x_train, y_train, x_test, y_test, KNNRegressor)\n",
    "k_values = list(range(1, 31, 2))  # A range of k values to be evaluated; only odd k-values are used for safety! \n",
    "results = validator.evaluate(k_values)\n",
    "\n",
    "validator.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d979384",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "- Analyze the plots of the error metrics. What do they tell you about the model's performance across different `k` values? How do these metrics help in understanding the model's accuracy and reliability?\n",
    "\n",
    "### ‚ö° Mandatory submission\n",
    "- Find the best `k` value based on the error metrics. How did you find it? Explain your reasoning.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c02bd8",
   "metadata": {},
   "source": [
    "In the code section below, I've used sklearn and developed another class that in practice it does exactly the same as the KNNRegressor class. The purpose is to show you how to use sklearn's KNeighborsRegressor class. You can compare the results of the two classes. They should be very similar, if not identical. The major differences include: \n",
    "- sklearn's class is highly optimized and should run faster than mine (you notice the difference for very large datasets), and\n",
    "- in my implementation, I've not normalized the data, while in the sklearn's class, the data is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b379bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SklearnKNNRegressor:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "        # Create the pipeline\n",
    "        self.model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('knn', KNeighborsRegressor(n_neighbors=self.k))\n",
    "        ])\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "    # Plotting function\n",
    "    def plot(self, x_true, y_true, x_train, y_train, x_pred, y_pred, k):\n",
    "        plt.figure()\n",
    "        plt.scatter(x_train, y_train, s=50, color=colors[7], edgecolors='k', alpha=0.4, label=\"Training noisy data\")\n",
    "        plt.plot(x_true, y_true, color=colors[0], linestyle=\"--\", linewidth=2, label=\"Actual trajectory\")\n",
    "        plt.plot(x_pred, y_pred, color=colors[1], label=f\"kNN Prediction (k={k})\")\n",
    "\n",
    "        # Customize plot appearance\n",
    "        plt.xlabel(\"Time (s)\", fontsize=14)\n",
    "        plt.ylabel(\"Displacement (m)\", fontsize=14)\n",
    "        plt.title(f\"kNN Prediction (k={k})\", fontsize=16)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# ========== MAIN ==========   \n",
    "k = 5\n",
    "# Instantiate and use the sklearn-based kNN regressor\n",
    "sklearn_knn_model = SklearnKNNRegressor(k)\n",
    "sklearn_knn_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict on dense time points\n",
    "y_dense_pred_sklearn = sklearn_knn_model.predict(x_dense)\n",
    "\n",
    "# Plot predictions\n",
    "sklearn_knn_model.plot(t, y_true, x_train, y_train, x_dense, y_dense_pred_sklearn, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d973e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate across multiple k values\n",
    "sklearn_validator = KNNValidator(x_train, y_train, x_test, y_test, SklearnKNNRegressor)\n",
    "k_values = list(range(1, 31, 2))\n",
    "results = sklearn_validator.evaluate(k_values)\n",
    "\n",
    "sklearn_validator.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a13b2",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "- Compare the results of the custom k-NN regressor (KNNRegressor) with those from sklearn's implementation (SklearnKNNRegressor). What similarities and differences do you observe in the predictions and performance metrics? Discuss any discrepancies and potential reasons.\n",
    "\n",
    "- You can also try to modify the SklearnKNNRegressor class to exclude scaling the data. How does scaling affect the model's performance? Is it something that you expected? Explain your findings and discuss it with your neighbor classmaters.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ce2f8",
   "metadata": {},
   "source": [
    "## Cross-Validation Analysis\n",
    "\n",
    "In the code section below, I've demonstrated how to use sklearn's `cross_val_score` and `cross_val_predict` functions to perform cross-validation on the k-NN regression model.\n",
    "\n",
    "`cross_val_score` is used to evaluate a model's performance using cross-validation. It splits the dataset into multiple folds (k-folds which is a different k than the one used in k-NN), trains the model on some folds, and tests it on the remaining fold. This process is repeated so every fold gets used for testing once, and the function returns an array of scores (like accuracy or MSE) for each fold. By averaging these scores, you can estimate how well the model is expected to generalize to new/unseen data.\n",
    "\n",
    "`cross_val_predict` is used to generate cross-validated predictions for every data point in the dataset. Instead of returning evaluation scores, it returns the predicted labels or values, where each prediction is made on a fold that did not include that data point in training. This is useful when you want to analyze predictions directly e.g., in making a confusion matrix, plotting predicted vs. actual values, or creating residual plots in regression. Read more about these functions in the [sklearn documentation](https://scikit-learn.org/stable/modules/cross_validation.html) and [cross_val_predict documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9707c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# Cross Validation\n",
    "cv_scores = cross_val_score(sklearn_knn_model.model, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores *= -1  # Convert negative MSE to positive MSE\n",
    "print(\"Cross validation results:\")\n",
    "print(f\"    MSE scores: {[f'{score:.2f}' for score in cv_scores]}\")\n",
    "print(f\"    Average MSE: {np.mean(cv_scores):.2f}\")\n",
    "\n",
    "# Cross validated predictions\n",
    "y_cv_pred = cross_val_predict(sklearn_knn_model.model, x_train, y_train, cv=5)\n",
    "mse_cv = mean_squared_error(y_train, y_cv_pred)\n",
    "r2_cv = r2_score(y_train, y_cv_pred)\n",
    "print(f\"Cross-validated predictions:\")\n",
    "print(f\"    MSE: {mse_cv:.2f}\")\n",
    "print(f\"    R2:  {r2_cv :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676e013",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Analyze the cross-validation results. For which `k` value are the cross-validation metrics obtained?\n",
    "\n",
    "### ‚ö° Mandatory submission\n",
    "- Write a code that performs cross-validation for different `k` values (e.g., 1, 3, 5, 7, 9) and computes the average MSE for each `k`. Plot the average MSE against `k` to visualize how the choice of `k` affects model performance. Identify the optimal `k` value based on this analysis. You should not submit your code to me, but you need to include your plot with the optimal `k` value in your submission. Justify your choice of the optimal `k` in a short paragraph.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c14698",
   "metadata": {},
   "source": [
    "## k-NN for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5a0a0",
   "metadata": {},
   "source": [
    "Now we move to classification. We aim to apply k-NN to classify data points into different categories based on their features. \n",
    "\n",
    "Open your previous lab notebook file for Classification. I hope you remember what you've done there. If not, you can quickly browse through it again. \n",
    "\n",
    "In the code block below, I've simply copied the code from the classification lab that generates different datasets for classification tasks. In addition, I've also added a new function called `make_three_classes_modified` that generates three clusters of data. If you remember from that lab, we could not classify non-linearly separable data with a linear model, like Perceptron. Here, we will see how k-NN can handle such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_circles, make_moons, make_blobs\n",
    "\n",
    "# This function uses sklearn \"make_classification\" to generate a dataset with \n",
    "# two informative features, and one cluster per class.\n",
    "# I've intentionally used a non-42 rand state here! Do not change it!\n",
    "def make_regular_data(n_samples=500, rand_state=90):\n",
    "    x, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0,\n",
    "                               n_informative=2, n_clusters_per_class=1, random_state=rand_state)\n",
    "    return x, y\n",
    "\n",
    "# This function creates points uniformly distributed in a square and \n",
    "# labels them based on whether they lie above or below the line y = x, \n",
    "# producing a simple linear decision boundary.\n",
    "def make_diagonal_data(n_samples=500, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Generate uniform data in range [0, 5]\n",
    "    x = np.random.uniform(0, 5, size=(n_samples, 2))\n",
    "\n",
    "    # true labels based on y > x\n",
    "    y = (x[:, 1] > x[:, 0]).astype(int)\n",
    "    return x, y\n",
    "\n",
    "# This function generates a dataset with a non-linear XOR pattern such that points are labeled \n",
    "# as class 1 if exactly one of their coordinates is greater than 2.5, and class 0 otherwise. \n",
    "# This creates a checkerboard-like separation.\n",
    "def make_xor_data(n_samples=500, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Generate uniform data in range [0, 5]\n",
    "    x = np.random.uniform(0, 5, size=(n_samples, 2))\n",
    "\n",
    "    # XOR condition: label is 1 if one of the coordinates is >2.5 and the other is <=2.5\n",
    "    y = (((x[:, 0] >  2.5) & (x[:, 1] <= 2.5)) | \n",
    "         ((x[:, 0] <= 2.5) & (x[:, 1] >  2.5))).astype(int)\n",
    "    return x, y\n",
    "\n",
    "# This function produces a dataset consisting of two noisy concentric circles: \n",
    "# an inner and outer ring are labeled as different classes. \n",
    "# This is a classic example of a non-linearly separable dataset, useful for testing non-linear classifiers.\n",
    "def make_concentric_circles(n_samples=500, factor=0.3, noise=0.1):\n",
    "    x, y = make_circles(n_samples=n_samples, factor=factor, noise=noise, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "# Similar to the circle function, this function makes two interleaving half circles.\n",
    "def make_two_half_moons(n_samples=500, noise=0.2):\n",
    "    x, y = make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "# This function generates a dataset with three distinct clusters using Gaussian blobs.\n",
    "def make_three_classes(n_samples=500, noise=0.7):\n",
    "    x, y = make_blobs(n_samples=n_samples,\n",
    "                    centers=[[0, 5], [2, 0], [5, 4]], #[[0, 5], [2, 0], [5, 4], [2, 6]],\n",
    "                    cluster_std=noise,\n",
    "                    random_state=42)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Create 3 clusters of data with make_blobs\n",
    "def make_three_classes_modified(samples=[50, 80, 80, 40]):\n",
    "    centers = [[0, 5], [2, 0], [5, 4], [4, -1]]\n",
    "    noise   = [2.0   , 2.0   , 2.0   , 1.0 ]\n",
    "    x, y = make_blobs(n_samples=samples,\n",
    "                    centers=centers, \n",
    "                    cluster_std=noise,\n",
    "                    random_state=42)\n",
    "    \n",
    "    # Relabel the 4th blob (label == 3) to have the same label as the 1st blob (label == 0)\n",
    "    y[y == 3] = 0\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02271a",
   "metadata": {},
   "source": [
    "In the Classification lab, we did not use non-linearly separable data. \n",
    "In that lab, scroll down to \"Multiclass Classifiers\" section and in the first code-block in that section, instead of \n",
    "```python\n",
    "x, y = make_three_classes( noise=1.0)\n",
    "```\n",
    "use\n",
    "\n",
    "```python\n",
    "x, y = make_xor_data()\n",
    "```\n",
    "and re-run that section and all code blocks below it. Do you remember what is being done? How did the linear model perform?\n",
    "\n",
    "Now, in the code block below, we will use k-NN to classify the same data, but before that, we need to split the data into training and test sets, and visualize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a128e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========== MAIN ==========\n",
    "# Generate three classes\n",
    "X, y = make_xor_data()\n",
    "\n",
    "# split data to training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Plot the training data\n",
    "plt.figure( figsize=(5, 5), dpi=200)\n",
    "\n",
    "# Different Marker for Scatter plot\n",
    "markers = ['o', 's', '*', 'x', '^', 'v', '<', '>']  # Different markers for different classes\n",
    "\n",
    "# Plot each class dynamically\n",
    "for i, class_label in enumerate(np.unique(y_train)):\n",
    "    plt.scatter(\n",
    "        X_train[y_train == class_label][:, 0], \n",
    "        X_train[y_train == class_label][:, 1],\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=markers[i % len(markers)],\n",
    "        s=70, edgecolor='k', alpha=0.7,\n",
    "        label=f'Class {class_label}'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Feature x1')\n",
    "plt.ylabel('Feature x2')\n",
    "plt.title(\"Training Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8344029",
   "metadata": {},
   "source": [
    "Now, we will use k-NN to classify the data. The code is simple, and since I've used sklearn, in principle is similar to the SklearnKNNRegressor class. The major difference is that here we are using `KNeighborsClassifier` instead of `KNeighborsRegressor`. Study the code first, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 1\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=k))\n",
    "        ])\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "h = 0.1  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min()-0.5, X[:, 0].max()+0.5\n",
    "y_min, y_max = X[:, 1].min()-0.5, X[:, 1].max()+0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict on meshgrid\n",
    "y_pred = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "y_pred = y_pred.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure()\n",
    "\n",
    "# Plot each class dynamically\n",
    "for i, class_label in enumerate(np.unique(y_train)):\n",
    "    plt.scatter(\n",
    "        X_train[y_train == class_label][:, 0], \n",
    "        X_train[y_train == class_label][:, 1],\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=markers[i % len(markers)],\n",
    "        s=70, edgecolor='k', alpha=0.7,\n",
    "        label=f'Class {class_label}'\n",
    "    )\n",
    "\n",
    "plt.contourf(xx, yy, y_pred, alpha=0.4, cmap='Set1')\n",
    "\n",
    "plt.xlabel('Feature x1')\n",
    "plt.ylabel('Feature x2')\n",
    "plt.title(f\"KNN Decision Boundary (k={k})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93237b2f",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "- Analyze the decision boundary plot. How does the k-NN classifier separate the different classes in the feature space? Discuss how the choice of `k` influences the shape and complexity of the decision boundary. You can try different `k` values to see how the decision boundary changes.\n",
    "\n",
    "- Find the best `k` value for the k-NN classifier. How did you find it? Explain in a short paragraph.\n",
    "\n",
    "- For the best `k` value, make the confusion matrix for the test data. How well does the model perform on the test data? Explain your observations.\n",
    "\n",
    "- Instead of the xor dataset, you can choose something else (e.g., `make_two_half_moons` and `make_concentric_circles`) and run the k-NN classification code again, and see how well k-NN can classify such data. Remember those data generating functions have some parameters that you can change to make the classification task easier or harder ü§ì\n",
    "\n",
    "‚ö†Ô∏è Practical note: The default distance metric in sklearn's KNeighborsClassifier and KNeighborsRegressor is 'minkowski' distance. You can change it to other metrics, such as Manhattan, Euclidean, or others. For that, you need to set the `metric` parameter when initializing the k-NN model, e.g., `KNeighborsClassifier(n_neighbors=k, metric='euclidean')`.\n",
    "\n",
    "### ‚ö° Mandatory submission\n",
    "- Instead of the xor function, use `make_three_classes_modified` function to generate data. Visualize the data and re-run the k-NN classification code. Find the best `k` value and make the confusion matrix for the test data. What is the best `k` value and how did you find it? There is no need to submit your code.\n",
    "\n",
    "- What does the confusion matrix tell you about the model's performance? Calculate the accuracy and precision from the confusion matrix and interpret these metrics in the context of the classification task.\n",
    "\n",
    "- For a classification task, explain what may happen if we choose an even value for `k` (e.g., 2, 4, 6, etc.) instead of an odd value. Discuss the potential implications on the model's predictions and performance. Does your explanation applicable to regression tasks as well? Write your answer in a short paragraph.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14294ce2",
   "metadata": {},
   "source": [
    "## Beauty of KDE\n",
    "In the lecture notes, you have learned about Kernel Density Estimation (KDE) as a non-parametric way to estimate the probability density function of a random variable. During out previous lab session, I also emphasized on the importance of KDE in classification tasks. KDE can be used for classification tasks by estimating the class-conditional densities and applying Bayes' theorem to classify new data points.\n",
    "\n",
    "Let's make a quick implementation of this idea. However, moving towards the beauties of KDE deserves a dedicated lab session, which is beyond the scale and scope of this course. For now, we only limit ourselves to a simple implementation of the idea.\n",
    "\n",
    "For that, let's first generate some data using the `make_three_classes_modified` function.\n",
    "\n",
    "Then, for each class, we will fit a KDE model to estimate the density of that class. Finally, we will classify new data points based on the estimated densities. The code is simple and self-explanatory. Study it and run it. If you do not understand it, first discuss it with your neighbor classmates, and if you still do not understand it, ask me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82714a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Generate three classes using the modified function\n",
    "X, y = make_three_classes_modified()\n",
    "\n",
    "# split data to training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "k = 5 # number of nearest neighbors\n",
    "\n",
    "# Train a KNN classifier with scaling\n",
    "knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=k))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "h = 0.1  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict on meshgrid\n",
    "y_pred = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "y_pred = y_pred.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, y_pred, alpha=0.4, cmap='Set1')\n",
    "\n",
    "# For each class, fit a KDE and overlay the density contours\n",
    "for i, class_label in enumerate(np.unique(y_train)):\n",
    "    X_class = X_train[y_train == class_label]\n",
    "\n",
    "    # Plot class points\n",
    "    plt.scatter(\n",
    "        X_class[:, 0], X_class[:, 1],\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=markers[i % len(markers)],\n",
    "        s=70, edgecolor='k', alpha=0.5,\n",
    "        label=f'Class {class_label}'\n",
    "    )\n",
    "\n",
    "    # Fit KDE for this class\n",
    "    # Do you remember the bandwidth parameter from the lecture notes?\n",
    "    kde = KernelDensity(bandwidth=0.3, kernel='gaussian')\n",
    "    kde.fit(X_class)\n",
    "\n",
    "    # Evaluate density on meshgrid\n",
    "    # kde.score_samples returns the log of the probability density\n",
    "    # then we exponentiate it to get the actual density values\n",
    "    log_dens = kde.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    dens = np.exp(log_dens).reshape(xx.shape)\n",
    "\n",
    "    # Overlay density contours\n",
    "    plt.contour(xx, yy, dens, colors=[colors[i % len(colors)]], \n",
    "                linewidths=1.0, alpha=0.9)\n",
    "\n",
    "plt.xlabel('Feature x1')\n",
    "plt.ylabel('Feature x2')\n",
    "plt.title(f\"KNN Decision Boundary with KDE (k={k})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73485a",
   "metadata": {},
   "source": [
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
