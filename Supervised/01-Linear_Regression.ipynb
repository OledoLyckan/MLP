{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2024-04-08\n",
    "\n",
    "**Last update:** 2025-08-17\n",
    "\n",
    "**MIT License** ‚Äî Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Ume√• University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "**GitHub Copilot** was a valuable assistant in preparing Jupyter Notebooks for this course. It helped me generate code comments and organize the code more clearly and consistently throughout the course materials.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* Exercise submission is not required; these tasks are designed to help you practice, explore the concepts, and learn by doing.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget üôÇ\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you begin\n",
    "\n",
    "### ‚ö†Ô∏è Note 1:\n",
    "As you begin your programming journey in this course, it is important to recognize that there are many different ways to write codes. Many programming languages offer multiple functions or methods to achieve the same task, allowing you to choose the most suitable approach for your needs. Additionaly, a variety of libraries and frameworks exist, each with unique strengths (and limitations) that can enhance your coding efficiency.\n",
    "\n",
    "### ‚ö†Ô∏è Note 2:\n",
    "Individual coding styles also play an important role in how programmers approach their tasks. Each programmer develops a unique style influenced by personal preference and experience, which may prioritize conciseness or clarity. In team environments, the story is a lot more different, requires adopting coding standards that helps maintain consistency and readability within the team. Remember that the examples provided in this course are just one of many ways to solve programming challenges.\n",
    "\n",
    "### ‚ö†Ô∏è Note 3:\n",
    "What truly matters in programming are the accuracy and efficiency of your code as well as its readability and maintainability. A well-written code should produce correct results while being optimized for performance, making it easier for others (or yourself in the future) to understand, modify, and build. Providing a balance between these elements is essential for delivering high-quality software.\n",
    "\n",
    "### ‚ö†Ô∏è Note 4:\n",
    "I have tested all of these codes using Python >=3.12 in VS Code, and they should run without any errors. If you faced an error, please try to identify the root-cause and fix it on your own first. If the issue persists (e.g., you spent >=30 minutes on it and did not go away), do not hesitate to ask me for help.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Your Tasks:\n",
    "\n",
    "* During all lab sessions, you are not required to write code from scratch; instead, your main responsibility is to carefully read through the provided instructions, examine and understand the sample codes I have prepared, and connect them to the concepts being introduced in the class.\n",
    "\n",
    "* You should read the text provided above the code lines, run the given code, observe how it behaves, and analyze the output in relation to the theoretical background discussed in class.\n",
    "\n",
    "* However, only running the code is not enough; you should also read through the codes line by line to understand how the code is written.\n",
    "\n",
    "* The goal is for you to **actively engage** with the material by experimenting with the code, asking yourself how the code is written, why it produces certain results, and drawing conclusions that will strengthen your understanding of both the practical and conceptual aspects of the topic.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Overview\n",
    "\n",
    "Regression is a supervised technique for estimating the relationships between a continuous dependent variable (output or target) and one or more independent variables (features or regressors).\n",
    "\n",
    "Due to its simplicity, it has been widly used to solve linear problems.\n",
    "\n",
    "- **Goal:** Find the best-fitting straight line through the data points.\n",
    "\n",
    "- **Equation:**  \n",
    "    $y = mx + b$\n",
    "  where:\n",
    "  - $y$ is the predicted output (target),\n",
    "  - $x$ is the input feature,\n",
    "  - $m$ is the line slope (or $w_1$),\n",
    "  - $b$ is the y-intercept (or $w_0$).\n",
    "\n",
    "- **How does it work?** The model minimizes the difference between the actual data points and the predicted values on the line (this difference is called the error or residual).\n",
    "\n",
    "Linear regression can also be extended to multiple features, which is called **Multiple Linear Regression**. More advanced topics are covered in the upcoming notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Let's begin simple. Very simple! \n",
    "\n",
    "In this code, we simulate uniform linear motion of an object with added noise to model measurement errors. Two datasets with different sample sizes are generated using the same equation of motion: $y=v_0 t+y_0$. For each dataset, the code computes the sum of squared residuals (SSR) and the root mean squared error (RMSE) to quantify the deviation between the true trajectory and the noisy observations. This comparison shows how sample size affects error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Compute the true y values for the motion of a projectile\n",
    "# y = v0*t + y0\n",
    "# and return both the true and noisy data\n",
    "#\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# NOTE: The commenting format you see below (in the function) is an standard Python docstring format\n",
    "#       used for properly documenting the code. I highly encourage you to use this format in your\n",
    "#       own code, but for simplicity, I keep the comments (expect for this one and a few more) brief\n",
    "#       in our lab notebooks.\n",
    "#\n",
    "#       As I said earlier in the class, this course is not \"just\" about Machine Learning. I try\n",
    "#       to incorporate concepts from physics, programming, data analysis and engineering as well.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "#\n",
    "def uniform_motion(v0, y0, time_range=(0, 5), num_samples=100, noise_level=5, seed=42):\n",
    "    \"\"\" \n",
    "    Simulate uniform motion with added Gaussian noise.\n",
    "    \n",
    "    v0: initial velocity (or the true slope of the line)\n",
    "    y0: initial position (or the y-intercept of the line)\n",
    "    time_range: the range of time over which we simulate the motion of the projectile in units of seconds\n",
    "    num_samples: the number of data samples to be generated\n",
    "    noise_level: the standard deviation of the Gaussian noise to be added to the data to simulate measurement noise\n",
    "    seed: the random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)  # Set random seed for reproducibility\n",
    "\n",
    "    # time for the uniform motion in units of seconds\n",
    "    t = np.linspace(time_range[0], time_range[1], num_samples)\n",
    "\n",
    "    # The equation of motion is y = v0*t + y0\n",
    "    y_true  = v0*t + y0 # true trajectory\n",
    "    y_noisy = y_true + np.random.normal(0, noise_level, num_samples) # Add noise to the trajectory\n",
    "    return t, y_true, y_noisy\n",
    "\n",
    "# ======== MAIN ========\n",
    "# Parameters for the uniform motion\n",
    "v0 = 2.0   # initial velocity (or the true slope of the line)\n",
    "y0 = 1.0   # initial position (or the y-intercept of the line)\n",
    "\n",
    "# Defining sample sizes for two datasets\n",
    "n_samples1, n_samples2 = 10, 100  # Two datasets of different sizes\n",
    "\n",
    "# Generate data for experiment 1\n",
    "t1, y1_true, y1_noisy = uniform_motion(v0, y0, num_samples=n_samples1)\n",
    "\n",
    "# Generate data for experiment 2\n",
    "t2, y2_true, y2_noisy = uniform_motion(v0, y0, num_samples=n_samples2)\n",
    "\n",
    "# Calculating the Sum of Squared Residuals (SSR)\n",
    "ssr1 = np.sum((y1_true - y1_noisy) ** 2)\n",
    "ssr2 = np.sum((y2_true - y2_noisy) ** 2)\n",
    "\n",
    "# Calculating Root Mean Squared Error (RMSE)\n",
    "rmse1 = root_mean_squared_error(y1_true, y1_noisy)\n",
    "rmse2 = root_mean_squared_error(y2_true, y2_noisy)\n",
    "\n",
    "print(f\"\\nDataset 1 ({n_samples1} samples):\")\n",
    "print(f\" - Sum of Squared Residuals (SSR): {ssr1:.2f}\")\n",
    "print(f\" - Root Mean Squared Error (RMSE): {rmse1:.2f}\")\n",
    "print(f\"\\nDataset 2 ({n_samples2} samples):\")\n",
    "print(f\" - Sum of Squared Residuals (SSR): {ssr2:.2f}\")\n",
    "print(f\" - Root Mean Squared Error (RMSE): {rmse2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to simulate the uniform motion of an object in one dimension and visualize the motion over time with and without added noise. Uniform motion is characterized by a constant velocity $v$ and an initial position $y_0$‚Äã.\n",
    "\n",
    "We generate 100 time values uniformly distributed between 0 and 5 seconds, representing moments in time during which the object's position is recorded. Additionally, random noise is introduced to simulate measurement errors or environmental variability. The level of noise is controlled by \"noise_level\", which uses a Gaussian function. \n",
    "\n",
    "The positions of the object at each time instance are calculated using the equation $y = v_0t + y_0$‚Äã, where $y$ represents the position, $v$ is the constant velocity, and $t$ is time. The calculated positions are then perturbed by the generated noise to create a realistic measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters for uniform motion\n",
    "v0 = +2.1  # constant velocity\n",
    "y0 = -1.2  # initial position y0\n",
    "n  = 100   # Number of smaples\n",
    "noise_level = 3.0   # noise level\n",
    "\n",
    "# Generate data\n",
    "t, y_true, y_noisy = uniform_motion(v0, y0, num_samples=n, noise_level=noise_level)\n",
    "\n",
    "# Plot the uniform motion\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "\n",
    "# Scatter plot of noisy data\n",
    "plt.scatter(t, y_noisy, color=\"k\", s=70, alpha=0.3, label=f\"Noisy Data\")\n",
    "\n",
    "# Plot the true trajectory\n",
    "plt.plot(t, y_true, color=\"royalblue\", linewidth=2, label=f\"True y = f(t) = {v0:+}t {y0:+}\")\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Uniform Motion in One Dimension\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position (m)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we do not have knowledge about the true function governing the motion of our object. This means we don't know $f(t) = 2.1t - 1.2$. Instead, we make an educated guess for the parameters: the slope $v_0$ (denoted as $w_1$ in the lecture notes) and the intercept $y_0$ (or $w_0$). In the code below, we define guessed values $v0_{pred}$ and $y0_{pred}$, construct a hypothetical function $h(t) = v0_{pred} t + y0_{pred}$, and evaluate how well this guess fits the data by calculating regression metrics (SSR, MSE, and RMSE).\n",
    "\n",
    "In the next code section, we only calculate the metrics and in the code section comes after, we visualize our predicted values using our $h(t)$. \n",
    "\n",
    "**Note:** $h(t)$ is $h(x)$ in the lecture slides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothetical predicted parameters\n",
    "v0_pred = +3.0  # My guess for the slope\n",
    "y0_pred = -3.0  # My guess for the y-intercept\n",
    "\n",
    "# Predicted y using the guessed values. \n",
    "# This is a possible hypothesis, h(t), but should not necessarily be the most accurate one.\n",
    "# This h(t) is member of the hypothesis space H, we discussed in the class.\n",
    "y_pred = v0_pred * t + y0_pred\n",
    "\n",
    "# Compute metrics for the noisy data\n",
    "ssr_pred  = np.sum ((y_pred - y_noisy)**2)              # Sum of Squared Residuals\n",
    "mse_pred  = np.mean((y_pred - y_noisy)**2)              # Mean Squared Error\n",
    "rmse_pred = root_mean_squared_error(y_pred, y_noisy)    # Root Mean Squared Error\n",
    "\n",
    "print(f\"\\nModel prediction using v0_pred={v0_pred}, y0_pred={y0_pred}:\")\n",
    "print(f\" - Sum of Squared Residuals (SSR): {ssr_pred:.2f}\")\n",
    "print(f\" - Mean Squared Error (MSE)      : {mse_pred:.2f}\")\n",
    "print(f\" - Root Mean Squared Error (RMSE): {rmse_pred:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# NOTE:\n",
    "#       Those of you who are not familiar or have little experience with Python and its libraries,\n",
    "#       you should carefully read the documentations.\n",
    "# =========================================================\n",
    "#\n",
    "# Plot the uniform motion\n",
    "#   figsize: the size of the figure in inches (width, height)\n",
    "#   dpi: the resolution of the figure in dots per inch\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "\n",
    "# Scatter plot of noisy data\n",
    "#   color: the color of the points\n",
    "#   s: the size of the points\n",
    "#   alpha: the transparency level of the points\n",
    "#   label: the label for the points to be shown in the legend\n",
    "plt.scatter(t, y_noisy, color=\"k\", s=70, alpha=0.3, label=f\"Noisy Data\")\n",
    "\n",
    "# Plot the true trajectory\n",
    "#   color: the color of the line\n",
    "#   linewidth: the width of the line\n",
    "#   label: the label for the line to be shown in the legend\n",
    "plt.plot(t, y_true, color=\"royalblue\", linewidth=2, label=f\"True y = f(t) = {v0:+}t {y0:+}\")\n",
    "\n",
    "# Scatter plot of predicted data\n",
    "plt.scatter(t, y_pred, marker=\"s\", color=\"forestgreen\", s=15, alpha=0.5, label=f\"y_pred = h(t) = {v0_pred:+}t {y0_pred:+}\")\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Uniform Motion in One Dimension\")     # Title of the plot\n",
    "plt.xlabel(\"Time (s)\")                           # X-axis label\n",
    "plt.ylabel(\"Position (m)\")                       # Y-axis label\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)        # Add gridlines with dashed lines and low opacity (0.5)\n",
    "plt.legend()                                     # Add legend\n",
    "plt.show()                                       # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚õ∑Ô∏è Exercise\n",
    "\n",
    "I mentioned earlier that you are not required to write much code during the lab sessions. That is true! However, for beginners in Python, regular practice is important. That is why (very) occasional code-writing tasks are included. They are meant to help you gradually work toward **mastering** Python.\n",
    "\n",
    "Write a Python script that performs a brute-force search to find the best-fitting parameters $v_0$ and $y_0$ for a linear model $h(t) = v_0 t + y_0$, based on our generated data. Without assuming knowledge of the true function, define a range of candidate values using `np.linspace` for both $v_0$ and $y_0$, compute the predicted values $h(t)$ for each pair, and evaluate the model using a suitable regression metric (MSE or RMSE). The goal is to find the combination of $v_0$ and $y_0$ that give the minimume error with respect to the noisy observations.\n",
    "\n",
    "E.g.,\n",
    "```python\n",
    "v0_pred = np.linspace(-3, 3, 13)\n",
    "y0_pred = np.linspace(-3, 3, 13)\n",
    "```\n",
    "\n",
    "Do the rest yourself. \n",
    "\n",
    "**Do not use AI tools (e.g., Chat GPT) to produce codes for you, if you truly want to learn.**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Position: Analytical Approach\n",
    "Now, we minimize the Squared Error to analytically solve for the slope and intercept of our simple linear function.\n",
    "Our linear function is in the form of $y=w_0 + w_1x$. We have 'n' data points from an observation, where\n",
    "$(x_1, x_2, ..., x_n)$ are mapped to $(y_1, y_2, ..., y_n)$. By calculating the square to the residuals $\\sum(y_i - (w_0 + w_1x_i))^2$ and minimizing it, we will find slope ($w_1$) and constant ($w_0$).\n",
    "\n",
    "$$\n",
    "w_1 = \\frac{\\sum(xy) - \\bar{y} \\cdot \\sum x}{\\sum x^2 - \\bar{x} \\cdot \\sum x}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "w_0 = \\frac{\\bar{y} \\cdot \\sum x^2 - \\bar{t} \\cdot \\sum(xy)}{\\sum x^2 - \\bar{x} \\cdot \\sum x}\n",
    "$$\n",
    "\n",
    "The code below performs linear regression to fit a line to our dataset using the analytical approach. We need to calculate every elements expressed in the analytical solution. So, we do that one by one in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of t\n",
    "mean_t = np.mean(t)\n",
    "\n",
    "# Calculate the mean of y_noisy\n",
    "mean_y = np.mean(y_noisy)\n",
    "\n",
    "# Calculate the sum of all elements in t\n",
    "sum_t = np.sum(t)\n",
    "\n",
    "# Calculate the sum of all elements in y_noisy\n",
    "sum_y = np.sum(y_noisy)\n",
    "\n",
    "# Calculate the sum of squares of all elements in t\n",
    "sum_t_square = np.sum(t**2)\n",
    "\n",
    "# Calculate the sum of squares of all elements in y_noisy\n",
    "sum_y_square = np.sum(y_noisy**2)\n",
    "\n",
    "# Calculate the dot product of t and y_noisy\n",
    "dot_t_y = np.dot(t.T, y_noisy)\n",
    "\n",
    "# Calculate the slope (w1) of the linear regression line using the analytic method\n",
    "# Using the formula: w1 = (Œ£(xy) - mean(y) * Œ£(x)) / (Œ£(x^2) - mean(x) * Œ£(x))\n",
    "# The expression above is mathematically the same as the one you have in your lecture notes.\n",
    "analytic_w1 = (dot_t_y - mean_y * sum_t) / (sum_t_square - mean_t * sum_t)\n",
    "\n",
    "# Convert analytic_w1 to a scalar\n",
    "analytic_w1 = analytic_w1.item()\n",
    "\n",
    "# Calculate the y-intercept (w0) of the linear regression line using the analytic method\n",
    "# Using the formula: w0 = (mean(y) * Œ£(x^2) - mean(x) * Œ£(xy)) / (Œ£(x^2) - mean(x) * Œ£(x))\n",
    "# The expression above is mathematically the same as the one you have in your lecture notes.\n",
    "analytic_w0 = (mean_y * sum_t_square - mean_t * dot_t_y) / (sum_t_square - mean_t * sum_t)\n",
    "\n",
    "# Convert analytic_w0 to a scalar\n",
    "analytic_w0 = analytic_w0.item()\n",
    "\n",
    "# Print the calculated slope and y-intercept\n",
    "print(\"Slope     (v0) or (w1): {:.2f}\".format(analytic_w1) )\n",
    "print(\"Intercept (y0) or (w0): {:.2f}\".format(analytic_w0) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found the slope and intercept, we can calculate the predicted y values ($y^*$ in the lecture notes) using the analytic solution, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted y values using the analytic solution\n",
    "y_analytic = analytic_w1 * t + analytic_w0\n",
    "\n",
    "# Plot the uniform motion\n",
    "plt.figure(dpi=200, figsize=(6, 4))\n",
    "\n",
    "# Scatter plot of noisy data\n",
    "plt.scatter(t, y_noisy, color=\"k\", s=70, alpha=0.3, label=f\"Noisy Data\")\n",
    "\n",
    "# Plot the true trajectory\n",
    "plt.plot(t, y_true, color=\"royalblue\", linewidth=2, label=f\"True y = f(t) = {v0:+}t {y0:+}\")\n",
    "\n",
    "# Scatter plot of analytic data\n",
    "plt.scatter(t, y_analytic, marker=\"s\", color=\"forestgreen\", s=15, alpha=0.5, label=f\"y_analytic = h(t) = {analytic_w1:+.2f}t {analytic_w0:+.2f}\")\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Uniform Motion in One Dimension\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position (m)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "- Calculate the RMSE for the analytical solution and compare it with the smallest RMSE you got from the  **exercise** section (the brute-force search). Which one is the smallest? Explain your observations.\n",
    "\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- The green squares shown in the figure represent the best-fit hypothetical function $h(t)$. However, this predicted solution does not perfectly match the actual true function $f(t)$, shown by the blue solid line. Explain the reasons for these discrepancies and discuss why the predicted model cannot fully recover the true function for the equation of motion.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Position: ML Approach using Scikit Learn\n",
    "\n",
    "We have not covered `Scikit-Learn` in detail yet and you may not be familiar with it at this point. However, this library forms the foundation of our course and will be used frequently (actually more than frequent), as it provides implementations for nearly all standard ML methods. As a first step, take some time to explore the Scikit-Learn website (https://scikit-learn.org) to get an overview of its capabilities and structure.\n",
    "\n",
    "In the code section below, we introduce a basic ML approach to model the relationship between time $t$ and the noisy position measurements $y_{noisy}$ using Scikit-Learn. Although this example is simplified and not a proper implementation, it gives you an initial understanding of how to use ML models from this great library. In a complete workflow, the data would typically be split into training and test (or validation) sets to evaluate the model's performance properly. Since we haven't discussed data splitting yet, we will skip that step here for **simplicity**.\n",
    "\n",
    "1- We begin by creating a linear regression model using the `LinearRegression()` function from Scikit-Learn. This is our **hop**, where we initialize the ML algorithm we want to use. \n",
    "\n",
    "2- Next comes the **step**: we train the model on the available data $(t, y_{\\text{noisy}})$ using `model.fit()`, allowing the algorithm to learn the best-fitting parameters from the data. \n",
    "\n",
    "3- Finally, we make the **jump** by using the trained model to generate predictions $y_{\\text{pred}}$ with `model.predict()`. \n",
    "\n",
    "These three stages (*hop, step, and jump*) form the essential workflow of applying a ML method in practice. The code below introduces you how Scikit-Learn models are used to learn from data and make predictions.\n",
    "\n",
    "**üéâWelcome to applying ML methods!üéâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries from scikit-learn \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##############################################\n",
    "# This cell generates data for uniform motion\n",
    "# It is redundant as we generated the data earlier, but \n",
    "# we do it again, because I want you to change them later.\n",
    "\n",
    "# Parameters for uniform motion\n",
    "v0 = +2.1  # constant velocity\n",
    "y0 = -1.2  # initial position y0\n",
    "n  = 100   # Number of smaples\n",
    "noise_level = 3.0   # noise level\n",
    "\n",
    "# Generate data\n",
    "t, y_true, y_noisy = uniform_motion(v0, y0, num_samples=n, noise_level=noise_level)\n",
    "##############################################\n",
    "\n",
    "# Step 1) [** hop **] \n",
    "# Create a linear regression model\n",
    "model = LinearRegression()  # Linear regression modlel from scikit-learn\n",
    "\n",
    "# Step 2) [** step **] \n",
    "# Train (or Fit) the model\n",
    "model.fit(t.reshape(-1, 1), y_noisy)\n",
    "\n",
    "# Step 3) [** jump **] \n",
    "# Make predictions using scikit learn\n",
    "y_sk_pred = model.predict(t.reshape(-1, 1))  # SciKit Predicted y values\n",
    "\n",
    "# Print the coefficients calculated by scikit-learn\n",
    "print(\"Predicted Slope     (v0) or (w1): {:.2f}\".format(model.coef_[0]) )\n",
    "print(\"Predicted Intercept (y0) or (w0): {:.2f}\".format(model.intercept_) )\n",
    "\n",
    "# Calculate root mean squared error\n",
    "rmse = root_mean_squared_error(y_noisy, y_sk_pred)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Plot the uniform motion\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "\n",
    "# Scatter plot of noisy data\n",
    "plt.scatter(t, y_noisy, color=\"k\", s=70, alpha=0.3, label=f\"Noisy Data\")\n",
    "\n",
    "# Plot the true trajectory\n",
    "plt.plot(t, y_true, color=\"royalblue\", linewidth=2, label=f\"True y = f(t) = {v0:+}t {y0:+}\")\n",
    "\n",
    "# Scatter plot of sklearn predicted data\n",
    "plt.scatter(t, y_sk_pred, marker=\".\", color=\"red\", s=10, alpha=0.7, \n",
    "            label=f'y_sk_pred = {model.coef_[0]:+.2f}t {model.intercept_:+.2f}')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Uniform Motion in One Dimension\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position (m)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "Compare the results you got from the ML model in Scikit Learn with those you got from the analytical approach. You see the results are the same, if not identical. Why is that? Isn't so that Scikit-Learn is supposed to provide highly accurate, \"state-of-the-art\" ML models? The reason for this agreement is because the relationship between $t$ and $y$ is perfectly linear, and `LinearRegression` is designed to model exactly that. Therefore, both the analytic approach and the ML model are solving the same simple problem (essentially in the same way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- Earlier, the noise level we applied to generate the data was relatively high. Reduce the noise level to 0.2 and rerun the code above. Compare the updated values of $w_0$‚Äã and $w_1$‚Äã with those obtained at the higher noise level. What changes do you observe, and what conclusions can you draw?\n",
    "\n",
    "- Restore the noise level to its original value (3). Instead of reducing noise, increase the number of samples (or samples size) once by a factor of 10 and once by a factor of 100. Re-run the code and recalculate $w_0$‚Äã and $w_1$‚Äã. What changes do you observe, and what conclusions do you draw?\n",
    "\n",
    "- ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Jupyter Notebook is a powerful and convenient tool. There is no doubt! **But** it requires careful attention to how cells are executed. For example, since in the previous task, I asked you to chang the number of samples to 10,000 and re-ran the code cell, the variables `t`, `y_true`, and `y_noisy` were updated with the new values. As a result, if you now scroll up and re-run the cell where we applied the analytical approach (without making any further changes), it will operate on the **latest** values of `t`, `y_true`, and `y_noisy`. Go ahead and try it, and re-run the analytical section and compare the results to those obtained from the SK-Learn model. Do you notice any differences?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we want to make sure to reset the data to its original configuration, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Parameters for uniform motion\n",
    "v0 = +2.1  # constant velocity\n",
    "y0 = -1.2  # initial position y0\n",
    "n  = 100   # Number of smaples\n",
    "noise_level = 3.0   # noise level\n",
    "\n",
    "# Generate data\n",
    "t, y_true, y_noisy = uniform_motion(v0, y0, num_samples=n, noise_level=noise_level)\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a grird search to compute and visualize the cost function for our simple linear regression model. This is a more systematic version of the brute-force parameter search you did in the earlier exercise.\n",
    "\n",
    "Below, we compute the MSE for each combination of $(w_0, w_1)$ over a predefined grid. We define a range for $w_0$ and $w_1$ values, and for every pair of values, we use the current data $t$ and $y_{noisy}$ to evaluate the cost and store the resulting scalar values in a cost matrix, $J_{vals}$. After evaluating all combinations, we identify the minimum of the cost function along with the corresponding values of $(w_0, w_1)$, and visualize the full cost \"landscape\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# NOTE: The code below is not efficient AT ALL!\n",
    "# One should vectorize the operations (for loops).\n",
    "# However, for simplicity, we use loops here.\n",
    "# This is intensional for those not familiar with vectorization.\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Cost function J(w) using the sum of squared errors\n",
    "def cost_function(w0, w1, x, y):\n",
    "    n = len(y)  # number of data points\n",
    "    predictions = w1*x + w0\n",
    "    return (1/n) * np.sum((y - predictions)**2)\n",
    "\n",
    "# Generate a dense range of w0 and w1 values\n",
    "# We assume w0=[-10, 10], w1=[-10, 10]\n",
    "w0_range = np.linspace(-10, 10, 201)\n",
    "w1_range = np.linspace(-10, 10, 201)\n",
    "\n",
    "# Create meshgrid for computing and plotting\n",
    "w0, w1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "J_vals = np.zeros(w0.shape) # pre-allocate memory for cost function values\n",
    "\n",
    "# For-loops should be avoided in Python, but are used here for simplicity\n",
    "# Calculate the cost function for each combination of w0 and w1\n",
    "for i in range(w0.shape[0]):\n",
    "    for j in range(w0.shape[1]):\n",
    "        J_vals[i, j] = cost_function(w0[i, j], w1[i, j], t, y_noisy)\n",
    "\n",
    "# Find the minimum of J(w)\n",
    "J_min = J_vals.min()    # find the minimum value of J(w)\n",
    "min_index_i, min_index_j = np.where(J_vals == J_min) # find the indices of the minimum value\n",
    "\n",
    "w0_min = w0[min_index_i[0], min_index_j[0]]      # find w0 where J(w) is minimum\n",
    "w1_min = w1[min_index_i[0], min_index_j[0]]      # find w1 where J(w) is minimum\n",
    "\n",
    "print(f\"The minimum cost is J(w) = {J_min:.2f}\")\n",
    "print(f\"The optimal parameters are w0 = {w0_min:.2f}, w1 = {w1_min:.2f}\")\n",
    "print(f\"The best predicted line is h(t) = {w1_min:+.2f}t{w0_min:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the solution space in 3D. The 2 horizontal axis show $w_0$ and $w_1$, and the vertical axis shows the associated cost function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function as a 3D surface\n",
    "fig = plt.figure(figsize=(6, 6), dpi=200)\n",
    "\n",
    "# Create 3D axis subplot.\n",
    "#   Since we defined a subplot, and assigned it to ax, we can use ax to plot.\n",
    "#   Therefore, in the following code, we use \"ax.blablaa\" and not \"plt.blablaa\"\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Surface plot\n",
    "#   x: w0 (Intercept)\n",
    "#   y: w1 (Slope)\n",
    "#   z: J(w)\n",
    "#   cmap: colormap (e.g., \"magma\"). \n",
    "#   For more color options, see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "#\n",
    "ax.plot_surface(w0, w1, np.log10(J_vals), cmap=\"magma\")\n",
    "\n",
    "# Mark the minimum point on the surface\n",
    "ax.scatter(w0_min, w1_min, np.log10(J_min), \n",
    "           color=\"green\",   # Minimum point color\n",
    "           s=100,           # Marker size\n",
    "           label=f\"Min J(w) at (w1, w0)=({w1_min:.2f}, {w0_min:.2f})\")\n",
    "\n",
    "ax.set_xlabel(\"w0 (Intercept)\")\n",
    "ax.set_ylabel(\"w1 (Slope)\")\n",
    "ax.set_zlabel(\"J(w)\")\n",
    "ax.set_title(\"3D view of the cost function\")\n",
    "plt.legend()\n",
    "\n",
    "# Rotate the figure for better viewing angle\n",
    "ax.view_init(elev=20, azim=-30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D visualizations are cool, but sometimes not easy to digest. Let's show a 2D presentation of the cost function in the code section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot J_vals using imshow in 2D (a heatmap)\n",
    "plt.figure(figsize=(5, 4),dpi=200)\n",
    "\n",
    "# Show a cyan circle at the minimum point. It should have been green to be \n",
    "# consistent with the previous 3D code, but for better visualization, I changed its color.\n",
    "plt.scatter(w0_min, w1_min, s=50, color=\"cyan\", edgecolor=\"black\", label=\"Minimum J(w)\")\n",
    "\n",
    "# Show the cost function as a heatmap\n",
    "# We have imshow in MATLAB as well. It is quite similar to the one in Python.\n",
    "#   extent: indicates the extent of the axes [xmin, xmax, ymin, ymax]\n",
    "#   origin: determines the [0,0] point of the image (default is \"upper\")\n",
    "#   cmap: colormap (e.g., \"magma\"). For more color options, see: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "im = plt.imshow(np.log10(J_vals), \n",
    "           extent=[w0_range.min(), w0_range.max(), w1_range.min(), w1_range.max()], \n",
    "           origin=\"lower\", cmap=\"magma\")\n",
    "plt.colorbar(im, label=r\"$log_{10}$(J(w))\") # Add colorbar\n",
    "\n",
    "# Add contour lines for the log10(J(w))\n",
    "#    levels: number of contour levels\n",
    "plt.contour(w0, w1, np.log10(J_vals), levels=20, linewidths=0.5, colors=\"w\", alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"w0 (Intercept)\")\n",
    "plt.ylabel(\"w1 (Slope)\")\n",
    "plt.title(\"Heatmap of the Cost Function J(w)\")\n",
    "plt.grid(True, linestyle=\"--\", color=\"w\", alpha=0.4)\n",
    "plt.axis(\"equal\")   # Equal aspect ratio for the figure\n",
    "plt.tight_layout()  # Ensure everything fits into the figure without clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Do you understand what is shown in the figures above? Can you explain your observations?\n",
    "\n",
    "- What does the cyan circle show? \n",
    "\n",
    "- As you can see, the cost function is a convex function. What does it tell us and why is it important?\n",
    "\n",
    "- Can we get our predicted ($w_0$, $w_1$) closer to the actual ($w_0$, $w_1$)? Think about it and discuss with your classmates.\n",
    " \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚õ∑Ô∏è Exercise\n",
    "\n",
    "I'm not providing step-by-step \n",
    "\n",
    "Use the dataset generated from a physical model of force:\n",
    "\n",
    "$$\n",
    "F = ma + \\mu mg\n",
    "$$\n",
    "\n",
    "The dataset is located in the \"datasets\" folder and its name is \"force_data.csv\".\n",
    "\n",
    "* Use Pandas to load the dataset (also see \"Python_Jumpstart/05-Data_Analysis.ipynb\"):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../datasets/force_data.csv\", comment=\"#\")\n",
    "```\n",
    "\n",
    "* Explore the data through visualizing the relationships between the input features and the target variable `F`. Note that the dataset contains of multiple features. To list all features (i.e. column names) in a dataset using pandas, use:\n",
    "\n",
    "```python\n",
    "df.columns\n",
    "```\n",
    "\n",
    "* Assume you are not a physics student and you do not know anything about the relation between different features. This dataset is given to you and you assume the target (`F`) depends linearly on all input features as:\n",
    "\n",
    "$$\n",
    "F = w_1 m + w_2 a + w_3 \\mu + w_0\n",
    "$$\n",
    "\n",
    "Use the **closed-form solution** for linear regression to solve for the optimal weights $w_0, w_1, w_2, w_3$. We discussed the general form in the class:\n",
    "\n",
    "$$\n",
    "\\mathbf{w^*} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "to solve for the weights.\n",
    "\n",
    "You need to construct matrix $X$ with a column of ones (for $w_0$) and columns for `mass`, `acceleration`, and `mu`. The $y$ matrix (vector) is the target, `F`. Once you computed the weights, use them to predict $F$ and compare your predictions to the actual values using metrics such as MSE or $R^2$ score. To calculate the inverse, you can use the `np.linalg.inv()` function from NumPy, or any smarter way you come up with. Also review the notebooks in \"Python_Jumpstart\", particularly \"01-Numpy.ipynb\".\n",
    "\n",
    "* This exercise is fun to work on, but it also highlights an important lesson: when applying an ML model, you should always be aware of the data and the true relationships between features. In reality, the applied force is governed by the physical law $F = ma + \\mu mg$, not by the simplified liear model we used earlier.\n",
    "\n",
    "* Let's have more fun (or challenge): Imagine I now tell you that the dataset you worked on was generated in a simulator, mimicking an object moving on the surface of a solar system body other than Earth. Based on the physics in the data, can you figure out which solar system object it was?\n",
    "\n",
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
