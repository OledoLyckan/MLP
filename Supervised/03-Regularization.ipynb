{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2024-04-19\n",
    "\n",
    "**Last update:** 2025-09-10\n",
    "\n",
    "**MIT License** ‚Äî Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Ume√• University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* Exercise submission is not required; these tasks are designed to help you practice, explore the concepts, and learn by doing.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget üôÇ\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Purpose and Learning Outcomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build upon your previous lab, with a focus on avoiding overfitting by selecting better model parameters using regularization techniques:\n",
    "    * L1 (Lasso)\n",
    "    * L2 (Ridge)\n",
    "    * Elastic Net\n",
    "\n",
    "At the end, you will also learn how the \"learning curves\" are used for diagnosing underfitting and overfitting. We did not discuss it in the class, so you will learn it in here.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    Creating a list of colors based on the \"tab10\" colormap.\n",
    "    I want to use the color set in the \"tab10\" colormap for my plotting.\n",
    "\"\"\"\n",
    "cmap = plt.colormaps[\"tab10\"]\n",
    "colors = [cmap(i) for i in range(21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from the previous lab\n",
    "\n",
    "The following code sections should look familiar to you as they are the same functions and setups used in your previous lab (Polynomials).\n",
    "\n",
    "Previously, you explored how increasing model complexity (e.g., using high degree polynomials) can lead to overfitting. In this lab, we will start by regenerating the data, visualizing it, and observing the behavior of high-order polynomial models once again. These are requyired for the later steps.\n",
    "\n",
    "Compared to the previous lab, I've made a new `fit_and_plot` function to fit and visualize data for different models. This newly developed model is a modification of `plot_multiple_fits` used in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to generate data\n",
    "def generate_new_data(a=+1, b=-5, c=+3, x_range=(-2, 2), \n",
    "                      num_points=100, noise_level=3.0, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    x = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    y_true = a * x**5 + b * x**3 + c * x\n",
    "    y_noisy = y_true + np.random.normal(0, noise_level, num_points)\n",
    "    return x, y_true, y_noisy\n",
    "\n",
    "# Function to plot data\n",
    "def plot_data(t, y_true, y_noisy, x_fit=None, y_fit=None, poly_degree=None, title=\"Projectile Motion\"):\n",
    "    plt.figure(figsize=(6, 4), dpi=200)\n",
    "    plt.scatter(t, y_noisy, color=colors[1], s=40, edgecolors=\"k\", alpha=0.6, label=f\"Noisy data\")\n",
    "    if (y_true is not None):\n",
    "        plt.scatter(t, y_true, color=colors[0], s=5, label=\"True y\")\n",
    "\n",
    "    if (x_fit is not None and \n",
    "        y_fit is not None):\n",
    "        if (poly_degree is not None):\n",
    "            label_text = f\"Polynomial degree {poly_degree}\"\n",
    "        else:\n",
    "            label_text = \"Regression fit\"\n",
    "        plt.scatter(x_fit, y_fit, color=colors[2], s=2, label=label_text)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    plt.xlabel(\"Time (s)\", fontsize=14)\n",
    "    plt.ylabel(\"Displacement (m)\", fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", color=\"grey\", linewidth=0.5, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# ========== MAIN ==========\n",
    "# Parameters for a non-linear model\n",
    "n = 50    # number of points\n",
    "a = +1    # 1st coefficient\n",
    "b = -7    # 2nd coefficient\n",
    "c = +3    # 3rd coefficient\n",
    "x_range = (-2.5, 2.5)\n",
    "noise_level = 7.0  # Noise lebel\n",
    "\n",
    "# Generate data and add some noise\n",
    "x, y_true, y_noisy = generate_new_data(a, b, c, x_range, n, noise_level)\n",
    "\n",
    "# Randomly split data (both true and noisy) into training and validation/test sets\n",
    "x_train, x_test, y_train_noisy, y_test_noisy, y_train_true, y_test_true = train_test_split(\n",
    "    x, y_noisy, y_true, test_size=0.3, random_state=42)\n",
    "\n",
    "# Plot only the training set\n",
    "plot_data(x_train, y_train_true, y_train_noisy, title=\"Training Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code section, we explore how the model fits the data when using different polynomial degrees. Focus on the overfitted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit (train) polynomial regression model using sklearn pipeline\n",
    "def fit_polynomial_sklearn(x, y, degree=2, normalize=True):\n",
    "    if(normalize):\n",
    "        # Create a pipeline with normalization\n",
    "        model = make_pipeline(StandardScaler(), \n",
    "                              PolynomialFeatures(degree, include_bias=False), \n",
    "                              LinearRegression())\n",
    "    else:\n",
    "        # Create a pipeline without normalization\n",
    "        model = make_pipeline(PolynomialFeatures(degree, include_bias=False), \n",
    "                              LinearRegression())\n",
    "\n",
    "    # We need to use x.reshape, as required by sklearn\n",
    "    model.fit(x.reshape(-1, 1), y)\n",
    "    return model\n",
    "\n",
    "# Fits multiple models and plots their results\n",
    "def fit_and_plot(t, y_true, y_noisy, models, labels=None):\n",
    "    plt.figure(figsize=(6, 4), dpi=200)\n",
    "    plt.scatter(t, y_noisy, color=colors[1], s=40, edgecolors=\"k\", alpha=0.6, label=\"Noisy data\")\n",
    "    plt.scatter(t, y_true , color=colors[0], s=5 , label=\"True y\")\n",
    "\n",
    "    # Generate dense grid for smooth predictions\n",
    "    t_vals = np.linspace(min(t), max(t), 2*t.size)\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        y_vals = model.predict(t_vals[:, np.newaxis])\n",
    "        label = labels[idx] if labels else f\"Model {idx+1}\"\n",
    "        plt.plot(t_vals, y_vals, color=colors[idx+1], linewidth=2, label=label)\n",
    "\n",
    "    plt.xlabel(\"x\", fontsize=14)\n",
    "    plt.ylabel(\"y\", fontsize=14)\n",
    "    plt.title(\"Polynomial Regression w. Regularization\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", color=\"grey\", alpha=0.6)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========= MAIN ==========\n",
    "# Fit polynomial models of different degrees to the training data\n",
    "degrees = [6, 12, 21]  # Selected polynomial degrees\n",
    "models  = []\n",
    "labels  = []\n",
    "\n",
    "for d in degrees:\n",
    "    model = fit_polynomial_sklearn(x_train, y_train_noisy, degree=d, normalize=True)\n",
    "    models.append(model)\n",
    "    labels.append(f\"Poly deg {d}\")\n",
    "\n",
    "# Plot data and fits\n",
    "fit_and_plot(x_train, y_train_true, y_train_noisy, models, labels=labels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "- Examine the plots generated above. How does changing the polynomial degree affect the model's ability to fit the data? What behavior do you observe for low vs. high-degree polynomials?\n",
    "\n",
    "- Go through both functions implemented above line by line. Make sure you understand the purpose and logic of every part of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Regularization\n",
    "\n",
    "Regularization is a fundamental ML technique used to prevent overfitting, especially when working with flexible models like high-degree polynomials. By adding a penalty term to the model's cost function, regularization prevents the model for fitting noise and less important features in training data. In this lab, you will work on Ridge (L2), Lasso (L1), and Elastic Net methods, and see their influences in model generalization. Before moving forward, review your lecture notes and make sure you understand how Ridge, Lasso, and Elastic Net work.\n",
    "\n",
    "The code section below fits polynomial regression models to noisy data, both with and without regularization. To streamline the process, I've developed two functions. The first one, `polynomial_regularization`, builds a modeling pipeline using sklearn. It first normalizes the input features with `StandardScaler`, then generates polynomial features of a chosen degree, and finally applies a regression method, either plain linear regression, or any of the chosen regularizors. The regularization parameter is `reg_lambda`.\n",
    "\n",
    "Once the models are trained, `fit_and_plot` is called to visualize the results. These functions allow you to explore how regularization affects polynomial regression in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Function to fit polynomial regression with regularization\n",
    "# Here I also show you how to write comments for your functions to be \n",
    "# more informative and easily converted to documentation tools, like Sphinx. \n",
    "# I strongly recommend you to follow this style in your future works.\n",
    "def polynomial_regularization(t, y, regularization, reg_lambda, poly_degree):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    - t (array): Time values.\n",
    "    - y (array): measured values (e.g., projectile trajectory).\n",
    "    - regularization (list): List of models to fit (e.g., Ridge, Lasso, etc.).\n",
    "    - reg_lambda (scalar, float32): Regularization parameter. \n",
    "    - poly_degree (scalar, int): Degree of the polynomial to fit.\n",
    "    \"\"\"\n",
    "    # Create a pipeline with normalization and polynomial regression\n",
    "    model = make_pipeline( StandardScaler(),                 # Normalize features\n",
    "                           PolynomialFeatures(poly_degree, include_bias=False),  # Create polynomial features\n",
    "                           regularization(alpha=reg_lambda, max_iter=5000)  # Apply regularization\n",
    "                        )\n",
    "\n",
    "    # Reshape t to be a column vector\n",
    "    # As said earlier, reshape is necessary because sklearn expects input features to \n",
    "    # be 2D (n_samples, n_features)\n",
    "    # If t is a 1D array, reshape it to a 2D array with one column. \n",
    "    # Instead of reshape, one can use np.newaxis.\n",
    "    model.fit( t.reshape(-1,1), y ) # Fit the model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è When you run the code block below, you may get warnings. Ignore them and move on for now. They are associated parameters in regularization models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= MAIN ==========\n",
    "# Define degrees of polynomials to fit\n",
    "poly_degree = 21 # polynomail degree\n",
    "\n",
    "ridge_lambda = 1.0  # Ridge regularization factor\n",
    "lasso_lambda = 1.0  # Lasso regularization factor\n",
    "elnet_lambda = 1.0  # ElasticNet regularization factor\n",
    "\n",
    "models  = [fit_polynomial_sklearn   (x_train, y_train_noisy, poly_degree),\n",
    "           polynomial_regularization(x_train, y_train_noisy, Ridge, \n",
    "                                     reg_lambda=ridge_lambda, poly_degree=poly_degree),\n",
    "           polynomial_regularization(x_train, y_train_noisy, Lasso, \n",
    "                                     reg_lambda=lasso_lambda, poly_degree=poly_degree),\n",
    "           polynomial_regularization(x_train, y_train_noisy, ElasticNet, \n",
    "                                     reg_lambda=elnet_lambda, poly_degree=poly_degree)]\n",
    "\n",
    "labels  = [f\"Polynom (d={poly_degree})\", \n",
    "           rf\"Ridge (d={poly_degree}, $\\lambda={ridge_lambda}$)\", \n",
    "           rf\"Lasso (d={poly_degree}, $\\lambda={lasso_lambda}$)\", \n",
    "           rf\"ElasticNet (d={poly_degree}, $\\lambda={elnet_lambda}$)\"]\n",
    "\n",
    "# Plot data and fits\n",
    "fit_and_plot(x_train, y_train_true, y_train_noisy, models, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model coefficients\n",
    "for i, model in enumerate(models):\n",
    "    # Get the regression model (the second step in the pipeline)\n",
    "    mymodel   = model.steps[-1][1]  # e.g., LinearRegression, Ridge, etc.\n",
    "    coefs     = mymodel.coef_\n",
    "    intercept = mymodel.intercept_\n",
    "\n",
    "    print(f\"Model {i+1}:\")\n",
    "    print(f\"  Intercept: {intercept:.4f}\")\n",
    "    print(f\"  Coefficients: {coefs}\")\n",
    "    print(35*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Study the figure above and make sure you understand the impact of different regularization techniques on polynomial regression.\n",
    "- What are the differences between Ridge, Lasso, and ElasticNet regression? See your lecture notes.\n",
    "- Did regularization help reduce overfitting in your model? How can you verify that, when visually not inspected? Often, for multi-dimensional data, it is not easy to visualize.\n",
    "- Examine the model coefficients printed in the previous code block. Compare how the coefficients differ for different models. Do the results align with your expectations, especially regarding the impact of regularization?\n",
    "- Why is it important to use a `pipeline` in our code developments?\n",
    "\n",
    "***\n",
    "### üí° Reflect and Run\n",
    "- Change the `poly_degree` to 5, 13, and 19 to see how it affects the model fits.\n",
    "\n",
    "- We have a few more `hyperparameters` to tune: i.e., Ridge_lambda, Lasso_lambda, ElNet_lambda. Experiment with different values for these parameters to see how they impact the model performance.\n",
    "\n",
    "Side Note: You know that you do not necessarily need to use the data I've used for training. You can create your own dataset that follows any function. For example, generate data for a damping oscillator: \n",
    "\n",
    "${y = A \\cdot \\exp{(-ct)} \\cos(\\omega t + \\phi)}$, \n",
    "where `A`, `c`, $\\omega$, and $\\phi$ are parameters you can set.\n",
    "\n",
    "***\n",
    "# ‚õ∑Ô∏è Exercise\n",
    "\n",
    "Focus on L1 and L2 norm regularizations: Lasso and Ridge Regression, respectively. Develop a code that performs a complete evaluation to determine the optimal value of ${\\lambda}$ (the regularization hyper-parameter) for both models. Use all suitable metrics and techniques you learned so far (e.g., cross-validation, cost function, ...) and justify your choice of ${\\lambda}$ based on your findings.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve\n",
    "\n",
    "We move to a different topic which was left from the previous lab, i.e., **learning curve**. \n",
    "\n",
    "A learning curve visually illustrates how a model's performance evolves as the size of the training dataset increases. It shows the connection between the model's effectiveness (e.g., metrics like accuracy, loss, or RMSE) and the amount of training data used. This analysis again helps us to identify underfitting and overfitting conditions for our model.\n",
    "\n",
    "In the code below, we calculate the learning curve for a model by evaluating its performance across varying training set sizes. The code uses the `learning_curve` function from sklearn to compute training and validation scores through cross validation. The scores, measured as negative RMSE (how the learning_curve returns outputs), are converted to positive RMSE values. In general, we analyze how the model's performance evolves as more data is used for training, providing insight into underfitting or overfitting behavior of the model. We test it on the projectile motion problem.\n",
    "\n",
    "Before you run the code, read about the learning curve: \n",
    "https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)\n",
    "\n",
    "and study:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(t, y_noisy, \n",
    "                        model, \n",
    "                        cv=5, y_lim=[-1, 100]):\n",
    "    # Reshape the time data and create a feature matrix\n",
    "    t = t.reshape(-1, 1)  # Reshape for sklearn\n",
    "\n",
    "    # Generate learning curves\n",
    "    # Learning_curve is a sklearn function that computes training and test scores for different training set sizes\n",
    "    # Read about it here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=model,    # model\n",
    "        X=t,                # Time feature\n",
    "        y=y_noisy,          # Noisy target variable\n",
    "        train_sizes=np.linspace(0.01, 1.0, 30),  # Training set sizes from 1% to 100%\n",
    "        cv=cv,              # Cross-validation splitting strategy.\n",
    "        scoring='neg_root_mean_squared_error', # Negative RMSE for scoring in regression\n",
    "        shuffle=True )\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the training scores\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    test_scores_mean  = -np.mean(test_scores , axis=1)\n",
    "    train_scores_std  =  np.std (train_scores, axis=1)\n",
    "    test_scores_std   =  np.std (test_scores , axis=1)\n",
    "\n",
    "    # Plotting the learning curve\n",
    "    plt.figure(figsize=(6, 4), dpi=200)\n",
    "       \n",
    "    plt.plot(train_sizes, train_scores_mean, \"--\", marker='o', color=colors[0], label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, test_scores_mean , \"-\" , marker='s', color=colors[1], label=\"Validation RMSE\")\n",
    "\n",
    "    # Plot the standard deviation as a shaded region\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n",
    "                                  train_scores_mean + train_scores_std, \n",
    "                                  color=colors[0], alpha=0.2)\n",
    "    plt.fill_between(train_sizes, test_scores_mean  - test_scores_std , \n",
    "                                  test_scores_mean  + test_scores_std , \n",
    "                                  color=colors[1], alpha=0.2)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    plt.xlabel(\"Training Set Size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "    plt.ylim(y_lim)\n",
    "    plt.title(\"Learning Curve\", fontsize=16)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True, linestyle=\"--\", color=\"grey\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# ========== MAIN ==========\n",
    "# Parameters for a non-linear model\n",
    "a = +1    # 1st coefficient\n",
    "b = -7    # 2nd coefficient\n",
    "c = +3    # 3rd coefficient\n",
    "x_range = (-2.5, 2.5)\n",
    "noise_level = 7.0  # Noise lebel\n",
    "\n",
    "num_samples = 50    # number of points\n",
    "poly_degree = 5     # polynomail degree\n",
    "\n",
    "# Generate data and add some noise\n",
    "x, y_true, y_noisy = generate_new_data(a, b, c, x_range, num_samples, noise_level)\n",
    "\n",
    "# Develop a pipeline to use polynomial model of degree N\n",
    "model = make_pipeline(StandardScaler(), \n",
    "                      PolynomialFeatures(poly_degree, include_bias=False), \n",
    "                      LinearRegression())\n",
    "\n",
    "# Plot learning curve, using linear regression model\n",
    "plot_learning_curve(x, y_noisy, model=model, cv=5, y_lim=[-1, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è Important:\n",
    "\n",
    "Let's interpret the Learning Curve above together:\n",
    "\n",
    "In general, if both training and validation errors are high, the model is too simple to capture the true patterns in the data, thus underfitting. On the other hand, if the training error is low but the validation error remains high, the model is too complex and is fitting noise in the training data, thus overfitting.\n",
    "\n",
    "A model that generalizes well will have both training and validation errors low and close to each other. The point where the two curves begin to converge typically indicates a good balance between bias and variance.\n",
    "\n",
    "In our plot, the gap between training and validation RMSE suggests that the model still exhibits some variance (i.e., overfitting). Increasing the size of the training data helps reduce this gap, indicating improved generalization. However, if the gap persists and the validation error plateaus suggests that the model also suffers from some bias which means there is a limit to how much the error can be improved by simply adding more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- How large is the generalization error in the experiment above, and what does that suggest about your model's performance?\n",
    "\n",
    "- Now, explore how the model behaves under different conditions by running the learning curve experiment for the following combinations of number of samples and polynomial degree:\n",
    "\n",
    "    | number of samples |  Polynomail degree |\n",
    "    |-------------------|--------------------|\n",
    "    |   50              |        2           |\n",
    "    |   50              |        5           |\n",
    "    |   50              |        7           |\n",
    "    |   500             |        2           |\n",
    "    |   500             |        5           |\n",
    "    |   500             |        10          |\n",
    "    |   50000           |        2           |\n",
    "    |   50000           |        5           |\n",
    "    |   50000           |        10          |\n",
    "\n",
    "For each setting run the experiment and adjust the `y_lim` parameter in your `plot_learning_curve` function if needed. It is a good idea to place each run in a separate code block so you can easily compare the plots. After running the experiments, analyze the results:\n",
    "\n",
    "- How large is the generalization error in each case?\n",
    "\n",
    "- How deos it change as you increase the number of training samples or the model complexity (here, polynomial degree)?\n",
    "\n",
    "- Do you see any sign for underfitting or overfitting, or does the model achieve a good fit?\n",
    "\n",
    "***\n",
    "\n",
    "# ‚õ∑Ô∏è Exercise (compulsory)\n",
    "\n",
    "Take a pen and paper and based on your observations, sketch three different learning curves, one for each of the following scenarios:\n",
    "- A model that underfits the data\n",
    "- A model that overfits the data\n",
    "- A model that generalizes well (good fit)\n",
    "\n",
    "Make your sketch for a generic model.\n",
    "\n",
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
