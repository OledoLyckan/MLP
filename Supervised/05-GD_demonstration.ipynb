{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2024-11-xx\n",
    "\n",
    "**Last update:** 2025-09-11\n",
    "\n",
    "**MIT License** ‚Äî Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Ume√• University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* Exercise submission is not required; these tasks are designed to help you practice, explore the concepts, and learn by doing.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget üôÇ\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Purpose and Learning Outcomes:\n",
    "\n",
    "This section builds on what you learned about Gradient Descent earlier. Here, I have developed longer code examples that use animations to show how GD works. You will see how the algorithm moves toward a solution, both in simple cases and in problems that involve local minima.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "from notebook_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, I've implemented the Gradient Descent algorithm to optimize parameters of a function by iteratively minimizing its cost. The code, while simple in principle, might be not so easy for Python's beginners. However, I have included it here, because I am using it to visually demonstrate the GD for you. Since I know that some of you have no or little experience with Python, I will explain the code step by step. If you do not want to learn the code itself, you can skip and move to running it and playing with it. \n",
    "\n",
    "### Step by step:\n",
    "- **Goal:** To develope a full visualization of GD optimization for a nonlinear hypothesis model.\n",
    "\n",
    "1. First, the data is generated using a known function with two parameters, $h(x)=w_0 / ((x - w_1)^2 + 1)$. This model is based on a Cauchy-like function, and noise is added to simulate real world examplrs.\n",
    "\n",
    "2. A cost function is defined using MSE between the model predictions and noisy data. The script computes a grid of cost values over a wide range of possible parameters to visualize the loss surface.\n",
    "\n",
    "3. The gradient of the cost function is calculated with respect to both parameters. These gradients are then used to update the parameters iteratively in the gradeint descent loop. At each step, the updated parameter values are recorded.\n",
    "\n",
    "4. During the optimization process, a live plot is updated to show the path of the parameters over the contour map of the cost function. Below the contour plot, a dynamic gauge shows the current cost value, scaled relative to the initial cost. \n",
    "\n",
    "5. Finally, the script prints both the true parameters used to generate the data and the optimized parameters found by gradient descent.\n",
    "\n",
    "### In short:\n",
    "First, we initialize parameters and repeatedly update them by moving in the direction of the negative gradient of the cost function with respect to each parameter, scaled by a learning rate. This iterative process continues for a number of iteratitions. The goal is to find the parameters that minimize the cost function. In this example, our hypothesis function is $h(x)=w_0 / ((x - w_1)^2 + 1)$. In this example, we have theoretically calculated the gradient of the cost function.\n",
    "\n",
    "#### NOTE: \n",
    "- When you run the code, see the animated figure.\n",
    "- It was not necessary to write the code in a class. However, since i am going to have many similar functions in the next code sections, and to avoid choosing odd names for them, I decided to encapsulate codes into classes. \n",
    "- If you have hard-time understanding the code, I'd be happy to help :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_right  # For the cost guage\n",
    "from matplotlib.patches import Wedge\n",
    "\n",
    "# Gradient Descent Visualizer class\n",
    "class GDVisualizer:\n",
    "    # Initialize the class.\n",
    "    def __init__(self, w_true, w_init, x_range=(0, 5), num_points=100, noise_level=0.1, learning_rate=0.05, epochs=100):\n",
    "        self.w_true        = w_true             # True parameters used to generate the data\n",
    "        self.w_init        = w_init             # Initial guess for the parameters\n",
    "        self.num_points    = num_points         # Number of data points to generate\n",
    "        self.x_range       = x_range            # Range of x values for data generation\n",
    "        self.noise_level   = noise_level        # Noise level for data generation\n",
    "        self.learning_rate = learning_rate      # Learning rate for gradient descent\n",
    "        self.epochs        = epochs             # Number of iterations for optimization\n",
    "\n",
    "        # Generate data\n",
    "        self.x_data, self.y_true, self.y_noisy = self.generate_data()\n",
    "\n",
    "        # Generate the cost function on a grid for a wide range of w0 and w1\n",
    "        self.w0_range = np.linspace(0.5, 2.5, 100)\n",
    "        self.w1_range = np.linspace(1.5, 3.5, 100)\n",
    "        self.w0_vals, self.w1_vals, self.cost_grid = self.compute_cost_grid()\n",
    "\n",
    "        # Initialize cost\n",
    "        self.cost_init = self.cost_function(self.w_init, self.x_data, self.y_noisy)\n",
    "\n",
    "    # Hypothesis function h(w,x) in the form of a cauchy probability distribution function\n",
    "    def h(self, w, x):\n",
    "        return w[0] / ((x - w[1])**2 + 1)\n",
    "\n",
    "    # Generate noisy data based on the hypothesis function h(w, x)\n",
    "    def generate_data(self, seed=42):\n",
    "        np.random.seed(seed)  # For reproducibility\n",
    "        x_data  = np.linspace(self.x_range[0], self.x_range[1], self.num_points)\n",
    "        y_true  = self.h(self.w_true, x_data)\n",
    "        y_noisy = y_true + np.random.normal(0, self.noise_level, size=self.num_points)\n",
    "        return x_data, y_true, y_noisy\n",
    "\n",
    "    # MSE cost function.\n",
    "    def cost_function(self, w, x, y):\n",
    "        predictions = self.h(w, x)\n",
    "        return np.mean((predictions - y)**2)\n",
    "\n",
    "    # Compute gradients of the cost function with respect to w values\n",
    "    def gradients(self, w, x, y):\n",
    "        predictions = self.h(w, x)\n",
    "        error = y - predictions\n",
    "        grad_w0 = -2 * np.mean(error / ((x - w[1])**2 + 1))   # Theoretical dh/w0\n",
    "        grad_w1 = -2 * np.mean(error * w[0] * 2 * (x - w[1]) / (((x - w[1])**2 + 1)**2))  # Theoretical dh/w1\n",
    "        return np.array([grad_w0, grad_w1])\n",
    "\n",
    "    # Generate a cost grid over a range of w values for visualization.\n",
    "    # This is used to generate the contours in the visualization function.\n",
    "    def compute_cost_grid(self):\n",
    "        w0_vals, w1_vals = np.meshgrid(self.w0_range, self.w1_range)\n",
    "        cost_grid = np.zeros_like(w0_vals)\n",
    "        for i in range(w0_vals.shape[0]):\n",
    "            for j in range(w0_vals.shape[1]):\n",
    "                cost_grid[i, j] = self.cost_function([w0_vals[i, j], w1_vals[i, j]], self.x_data, self.y_noisy)\n",
    "        return w0_vals, w1_vals, cost_grid\n",
    "\n",
    "    # Perform gradient descent with real-time plotting.\n",
    "    def gradient_descent(self):\n",
    "        w = np.array(self.w_init, dtype=float)\n",
    "        trajectory = [w.copy()]   # Store the data points for the trajectory of the descent\n",
    "\n",
    "        # this is the main loop for gradient descent\n",
    "        for epoch in range(self.epochs):\n",
    "            grad = self.gradients(w, self.x_data, self.y_noisy)\n",
    "            w -= self.learning_rate * grad\n",
    "            trajectory.append(w.copy())\n",
    "            \n",
    "            cost_current = self.cost_function(w, self.x_data, self.y_noisy)\n",
    "            self.plot_realtime(trajectory, cost_current)\n",
    "\n",
    "        return w\n",
    "\n",
    "    # Update the plot with the current trajectory of the Gradient Descent and plot cost guage.\n",
    "    def plot_realtime(self, trajectory, cost_current):\n",
    "        clear_output(wait=True)   # Clear the output for real-time plotting\n",
    "        plt.figure(figsize=(5, 5), dpi=150)\n",
    "\n",
    "        # Cost grid and trajectory of the Gradient Descent\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        contour = ax1.contourf(self.w0_vals, self.w1_vals, np.log10(self.cost_grid), levels=50, cmap=\"magma\")\n",
    "        ax1.contour(self.w0_vals, self.w1_vals, np.log10(self.cost_grid), levels=10, linewidths=1.0, colors=\"white\", alpha=0.6)\n",
    "        \n",
    "        ## All lines below are visualization touchups\n",
    "        # Set the color bar and its ranges\n",
    "        cbar = plt.colorbar(contour, ax=ax1)\n",
    "        cbar.set_label('Cost')\n",
    "        min_tick = int(np.floor(np.min(np.log10(self.cost_grid)))) + 1\n",
    "        max_tick = int(np.ceil(np.max(np.log10(self.cost_grid))))\n",
    "        ticks = range(min_tick, max_tick + 1)\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.set_ticklabels([f'$10^{{{tick}}}$' for tick in ticks])\n",
    "\n",
    "        # Plot the trajectory.\n",
    "        ax1.plot([pt[0] for pt in trajectory], [pt[1] for pt in trajectory], marker='.', markersize=2, linewidth=1.0, color='cyan')\n",
    "        \n",
    "        # Mark the true values for w.\n",
    "        ax1.scatter(self.w_true[0], self.w_true[1], marker='*', color='#2ecc71', edgecolor='#00bcd4', label='True Ws')\n",
    "        \n",
    "        ax1.set_title('Gradient Descent')\n",
    "        ax1.set_xlabel('$w_0$')\n",
    "        ax1.set_ylabel('$w_1$')\n",
    "        ax1.set_xlim([0.5, 2.5])\n",
    "        ax1.set_ylim([1.5, 3])\n",
    "        ax1.legend()\n",
    "\n",
    "        # Show the cost guage \n",
    "        ax2 = plt.subplot(2, 1, 2)\n",
    "        ax2.set_aspect('equal')\n",
    "        ax2.set_xlim([-1.5, 1.5])\n",
    "        ax2.set_ylim([-1.5, 1.5])\n",
    "        ax2.axis('off')\n",
    "\n",
    "        gauge_max = self.cost_init\n",
    "        normalized_cost = cost_current / gauge_max\n",
    "\n",
    "        # Use the Wedge widget.\n",
    "        wedge_background = Wedge((0, 0), 1.0, 0, 180, facecolor=\"lightgray\", edgecolor=\"black\")\n",
    "        ax2.add_patch(wedge_background)\n",
    "\n",
    "        angle = 180 * normalized_cost\n",
    "        thresholds = [0, 10, 30, 90]\n",
    "        colors = [\"forestgreen\", \"yellow\", \"orange\", \"red\"]\n",
    "\n",
    "        color_index = bisect_right(thresholds, angle) - 1\n",
    "        needle_color = colors[max(0, min(color_index, len(colors) - 1))]\n",
    "        wedge_needle = Wedge((0, 0), 1.0, angle - 2, angle + 2, facecolor=needle_color, edgecolor=needle_color)\n",
    "        ax2.add_patch(wedge_needle)\n",
    "\n",
    "        ax2.text(0.0, -0.3, f\"Cost: {cost_current:.4f}\", ha='center')\n",
    "        ax2.text(+1.1, +0.1, f\"0\", ha='center')\n",
    "        ax2.text(-1.3, +0.1, f\"{gauge_max:.2f}\", ha='center')\n",
    "        ax2.text(0.0, +1.2, \"Cost Gauge\", ha='center', fontsize=14, fontweight='bold')\n",
    "        #plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    w_true = [1.60, 2.25]  # True parameters\n",
    "    w_init = [0.75, 1.75]  # Initial guess\n",
    "    epochs = 100           # Number of iterations\n",
    "    learning_rate = 0.1    # Learning rate\n",
    "\n",
    "    # Instanse of the GDVisualizer and initalize the class parameters\n",
    "    gd_visualizer = GDVisualizer(w_true, w_init, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "    # Run gradient descent and visualize\n",
    "    w_optimized = gd_visualizer.gradient_descent()\n",
    "\n",
    "    print(f\"True parameters: {gd_visualizer.w_true}\")\n",
    "    print(f\"Optimized parameters: {w_optimized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- In lines 20-21, I've defined a range for the w-space to explore. How do we know the range for `w0` and `w1`?\n",
    "\n",
    "- Add a third subplot that displays the cost function's history over the epochs.\n",
    "\n",
    "- Next, modify the learning rate: first set it to 1.0, then to 3.0. For each case, run the full training and observe how both the cost function and the parameter updates evolve. Does the model still converge? Does it overshoot? Explain what you observe. \n",
    "\n",
    "- In the example above, $h(x) = w_0/((x - w_1)^2 + 1)$. Change the code such that it handles $h(x) = w_0/((x - w_1)^2 + w_2)$. If you have not time for actually doing it, think about the required changes in the `gradients` function. Only changing the `h(self, w, x)` in the class is not sufficient. Why?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD for nonlinear models with multiple minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of the examples above, we used a convex function with one minimum for the cost functions. The code below implements a GD method to optimize parameters of a model with multiple minima, using a differenthypothesis function $h(w, x) = \\sin(w_0 x + w_1)$. The function $h(w, x)$ is nonlinear and has multiple minima, which means that the optimization process may converge to different local minima depending on the initial parameter values.\n",
    "\n",
    "The class I've written below is fundamentally similar to the `GDVisualizer` you worked on and studied earlier in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Visualizer class\n",
    "class GDOscillator:\n",
    "    # Initialize the GradientDescentVisualizer class.\n",
    "    def __init__(self, w_true, w_init, x_range=(0.0, np.pi), num_points=100, noise_level=0.1, learning_rate=0.05, epochs=100):\n",
    "        self.w_true        = w_true\t\t# True parameters used to generate the data\n",
    "        self.w_init        = w_init             # Initial guess for the parameters\n",
    "        self.num_points    = num_points\t\t# Number of data points to generate\n",
    "        self.x_range       = x_range\t\t# Range of x values for data generation\n",
    "        self.noise_level   = noise_level\t# Noise level for data generation\n",
    "        self.learning_rate = learning_rate\t# Learning rate for gradient descent\n",
    "        self.epochs        = epochs\t\t# Number of iterations for optimization\n",
    "\n",
    "        # Generate data\n",
    "        self.x_data, self.y_true, self.y_noisy = self.generate_data()\n",
    "\n",
    "        # Generate the cost function on a grid for a wide range of w0 and w1\n",
    "        self.w0_range = np.linspace(-np.pi/2,    np.pi/2, 100)\n",
    "        self.w1_range = np.linspace(-1.5*np.pi  , 2.*np.pi  , 100)\n",
    "        self.w0_vals, self.w1_vals, self.cost_grid = self.compute_cost_grid()\n",
    "\n",
    "        # Initialize cost\n",
    "        self.cost_init = self.cost_function(self.w_init, self.x_data, self.y_noisy)\n",
    "\n",
    "    # Hypothesis function h(w, x) = sin(w[0] * x + w[1])\n",
    "    def h(self, w, x):\n",
    "        return np.sin(w[0] * x + w[1])\n",
    "\n",
    "    # Generate noisy data based on the hypothesis function h(w, x)\n",
    "    def generate_data(self, seed=42):\n",
    "        np.random.seed(seed)  # For reproducibility\n",
    "        x_data = np.linspace(self.x_range[0], self.x_range[1], self.num_points)\n",
    "        y_true = self.h(self.w_true, x_data)\n",
    "        y_noisy = y_true + np.random.normal(0, self.noise_level, size=self.num_points)\n",
    "        return x_data, y_true, y_noisy\n",
    "\n",
    "    # MSE cost function.\n",
    "    def cost_function(self, w, x, y):\n",
    "        predictions = self.h(w, x)\n",
    "        return np.mean((predictions - y)**2)\n",
    "\n",
    "    # Compute gradients of the cost function with respect to w values\n",
    "    def gradients(self, w, x, y):\n",
    "        predictions = self.h(w, x)\n",
    "        error = y - predictions\n",
    "        grad_w0 = -2 * np.mean(error * np.cos(w[0] * x + w[1]) * x)  # Derivative with respect to w0\n",
    "        grad_w1 = -2 * np.mean(error * np.cos(w[0] * x + w[1]))      # Derivative with respect to w1\n",
    "        return np.array([grad_w0, grad_w1])\n",
    "\n",
    "    # Generate a cost grid over a range of w values for visualization.\n",
    "    # This is used to generate the contours in the visualization function.\n",
    "    def compute_cost_grid(self):\n",
    "        w0_vals, w1_vals = np.meshgrid(self.w0_range, self.w1_range)\n",
    "        cost_grid = np.zeros_like(w0_vals)\n",
    "        for i in range(w0_vals.shape[0]):\n",
    "            for j in range(w0_vals.shape[1]):\n",
    "                cost_grid[i, j] = self.cost_function([w0_vals[i, j], w1_vals[i, j]], self.x_data, self.y_noisy)\n",
    "        return w0_vals, w1_vals, cost_grid\n",
    "\n",
    "    # Perform gradient descent with real-time plotting.\n",
    "    def gradient_descent(self):\n",
    "        w = np.array(self.w_init, dtype=float)\n",
    "        trajectory = [w.copy()]   # Store the data points for the trajectory of the descent\n",
    "\n",
    "        # this is the main loop for gradient descent\n",
    "        for epoch in range(self.epochs):\n",
    "            grad = self.gradients(w, self.x_data, self.y_noisy)\n",
    "            w -= self.learning_rate * grad\n",
    "            trajectory.append(w.copy())\n",
    "            \n",
    "            cost_current = self.cost_function(w, self.x_data, self.y_noisy)\n",
    "            self.plot_realtime(trajectory, cost_current)\n",
    "\n",
    "        return w\n",
    "\n",
    "    # Update the plot with the current trajectory of the Gradient Descent and plot cost guage.\n",
    "    def plot_realtime(self, trajectory, cost_current):\n",
    "        clear_output(wait=True)   # Clear the output for real-time plotting\n",
    "        plt.figure(figsize=(5, 5), dpi=150)\n",
    "\n",
    "        # Cost grid and trajectory of the Gradient Descent\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        contour = ax1.contourf(self.w0_vals, self.w1_vals, np.log10(self.cost_grid), levels=50, cmap=\"magma\")\n",
    "        ax1.contour(self.w0_vals, self.w1_vals, \n",
    "                    np.log10(self.cost_grid), levels=10, \n",
    "                    linewidths=1.0, colors=\"white\", alpha=0.6)\n",
    "        \n",
    "        # Set the color bar and its ranges\n",
    "        cbar = plt.colorbar(contour, ax=ax1)\n",
    "        contour.set_clim(-2, 1)\n",
    "        cbar.set_label('Cost')\n",
    "        min_tick = int(np.floor(np.min(np.log10(self.cost_grid))))+1\n",
    "        max_tick = int(np.ceil(np.max(np.log10(self.cost_grid))))-1\n",
    "        ticks = range(min_tick, max_tick + 1)\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.set_ticklabels([f'$10^{{{tick}}}$' for tick in ticks])\n",
    "\n",
    "        # Plot the trajectory.\n",
    "        ax1.plot(self.w_init[0], self.w_init[1], 's', markersize=4, color='w')\n",
    "        ax1.plot([pt[0] for pt in trajectory], [pt[1] for pt in trajectory], marker='.', markersize=2, linewidth=1.0, color='cyan')\n",
    "        \n",
    "        # Mark the true values for w.\n",
    "        ax1.scatter(self.w_true[0], self.w_true[1], marker='*', color='#2ecc71', edgecolor='#00bcd4', label='True Ws')\n",
    "        \n",
    "        ax1.set_title('Gradient Descent')\n",
    "        ax1.set_xlabel('$w_0$')\n",
    "        ax1.set_ylabel('$w_1$')\n",
    "        #ax1.set_xlim([0.5, 2.5])\n",
    "        #ax1.set_ylim([1.5, 3])\n",
    "        ax1.legend()\n",
    "\n",
    "        # Show the cost guage \n",
    "        ax2 = plt.subplot(2, 1, 2)\n",
    "        ax2.set_aspect('equal')\n",
    "        ax2.set_xlim([-1.5, 1.5])\n",
    "        ax2.set_ylim([-1.5, 1.5])\n",
    "        ax2.axis('off')\n",
    "\n",
    "        gauge_max = self.cost_init\n",
    "        normalized_cost = cost_current / gauge_max\n",
    "\n",
    "        # Use the Wedge widget.\n",
    "        wedge_background = Wedge((0, 0), 1.0, 0, 180, facecolor=\"lightgray\", edgecolor=\"black\")\n",
    "        ax2.add_patch(wedge_background)\n",
    "\n",
    "        angle = 180 * normalized_cost\n",
    "        thresholds = [0, 10, 30, 90]\n",
    "        colors = [\"forestgreen\", \"yellow\", \"orange\", \"red\"]\n",
    "\n",
    "        color_index = bisect_right(thresholds, angle) - 1\n",
    "        needle_color = colors[max(0, min(color_index, len(colors) - 1))]\n",
    "        wedge_needle = Wedge((0, 0), 1.0, angle - 2, angle + 2, facecolor=needle_color, edgecolor=needle_color)\n",
    "        ax2.add_patch(wedge_needle)\n",
    "\n",
    "        ax2.text(0.0, -0.3, f\"Cost: {cost_current:.4f}\", ha='center')\n",
    "        ax2.text(+1.1, +0.1, f\"0\", ha='center')\n",
    "        ax2.text(-1.3, +0.1, f\"{gauge_max:.2f}\", ha='center')\n",
    "        ax2.text(0.0, +1.2, \"Cost Gauge\", ha='center', fontsize=14, fontweight='bold')\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    w_true = [1.0, 0.5]\n",
    "    w_init = [0., 0.] # [0.5, 2.0] \n",
    "    epochs = 100\n",
    "    learning_rate = 0.2\n",
    "\n",
    "    # Instanse of the GDOscillator and initalize the class parameters\n",
    "    gd_visualizer = GDOscillator(w_true, w_init, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "    # Run gradient descent and visualize\n",
    "    w_optimized = gd_visualizer.gradient_descent()\n",
    "\n",
    "    print(f\"True parameters: {gd_visualizer.w_true}\")\n",
    "    print(f\"Optimized parameters: {w_optimized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- Increase the number of epochs and observe the effect on convergence.\n",
    "\n",
    "- Experiment with different learning rates and observe their effect on convergence speed and stability. For example, set `epochs = 200` and\n",
    "`learning_rate = 0.5` and re-run the code. What do you observe? Why?\n",
    "\n",
    "- Change the learning rate to 0.2, and set `w_init = [0.5, 2.0]`. Re-run the code and explain your observations.\n",
    "\n",
    "- Change the initial weights to different values, re-run the code, explain what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
