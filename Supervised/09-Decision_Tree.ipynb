{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2313fd7",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2025-08-01\n",
    "\n",
    "**Last update:** 2025-09-30\n",
    "\n",
    "**MIT License** ‚Äî Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Ume√• University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be279b0d",
   "metadata": {},
   "source": [
    "üì¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget üôÇ\n",
    "\n",
    "* Your answers for the \"‚ö° Mandatory\" sections of each lab <span style=\"color:red\"><strong>must be submitted before the start of the next lab session</strong></span>.\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a786f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual loading of our utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "from notebook_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073169fc",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A decision tree is a supervised, non-parametric ML algorithm used for both classification and regression problems that models decisions and their possible consequences in a tree-like structure. It splits the data into subsets based on the value of input features, creating branches that lead to decision nodes and leaf nodes, which represent the final output or decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2acf7",
   "metadata": {},
   "source": [
    "In this section, we will begin by loading and exploring the dataset gathered from antenna measurements to understand its structure and key characteristics. This analysis will include examining the relationships between various input features, such as frequency, and the target variable, antenna gain. By identifying patterns and trends within the data, we aim to build a decision tree model that can accurately predict antenna gain based on these input features. The decision tree will split the data into subsets based on feature values, creating a hierarchy of decision rules to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44898337",
   "metadata": {},
   "source": [
    "***\n",
    "### Data Analysis\n",
    "\n",
    "**‚ö†Ô∏è READ CAREFULLY** \n",
    "\n",
    "Before moving to the decision tree model, I intend to provide a comprehensive analysis of the data. Thorough data analysis is a critical step in all ML tasks, as it helps you understand the structure, quality, and patterns within the dataset. Without proper data exploration and preparation, it is unlikely that you will develop an accurate or effective model.\n",
    "\n",
    "Data analysis involves assessing the distribution of variables, identifying relationships between features, detecting outliers or anomalies, and addressing issues such as missing values or incorrect data types. These analyses guide decisions about feature selection, engineering, and preprocessing, which are essential for creating models that generalize well to unseen data.\n",
    "\n",
    "Neglecting or performing data analysis incorrectly can lead to significant problems, such as introducing bias, overfitting, or failing to capture the true patterns in the data. Machine learning models are fundamentally dependent on the quality of the input data, and any inaccuracies or inconsistencies in the data can directly impact the model's performance and predictions.\n",
    "\n",
    "Please note and remember that data analysis is not **just** a preliminary step but a **foundational process** that drives the success of the entire ML pipeline. By thoroughly analyzing the data, you ensure that the decision tree (in this notebook or any other model) is built on a solid foundation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd60d58",
   "metadata": {},
   "source": [
    "Let's read/load the data and explore it using pandas functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file for antenna data\n",
    "df = pd.read_csv(\"../datasets/antenna_data.csv\", \n",
    "                 comment=\"#\", skip_blank_lines=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# preview the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0301e9f",
   "metadata": {},
   "source": [
    "As you already see in the CSV file, not much information is given about the data. For example, we do not know the units. When this is the case, it is your first duty to ask the data provider to give you more information on the dataset. For here, I can tell you that the frequency is in Hz, antenna gain is in dB, and direction is in radian. No more information exists on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb66c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34be49",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "Study the last three reports you got above. What do they tell you?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412714d",
   "metadata": {},
   "source": [
    "We aim to generate summary statistics for the object-type columns (categorical or string data) in the `train_df` DataFrame using pandas. This step is essential for quickly understanding the distribution and characteristics of categorical or text-based data within the DataFrame. For example, upon examining the \"Polarization\" column, we discover that it contains text data. It has 373 data points, consisting of 2 unique categories. The most frequent category is \"Ciculat,\" appearing 187 times. This analysis provides valuable insights into the structure and distribution of the column's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include=['O']) # Encode categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78958499",
   "metadata": {},
   "source": [
    "### Univariate analysis\n",
    "Univariate analysis is the examination and analysis of a single variable within a dataset. Its primary goal is to understand the patterns, distribution, and characteristics of the variable by summarizing its central tendency (e.g., mean, median, mode), dispersion (e.g., variance, standard deviation, range), and shape (e.g., skewness). This type of analysis helps identify key patterns such as the presence of outliers, the spread of data, or the overall data distribution. \n",
    "\n",
    "I've designed the function below (inspired from a Kaggle source code) to perform univariate analysis of a continuous variable by visually combining a `boxplot` and a `histogram` into a single figure. The boxplot highlights the data's range, quartiles, and potential outliers, while the histogram displays the frequency distribution of the variable, optionally overlaying a Kernel Density Estimate (KDE) curve to illustrate the smooth probability density. Additionally, the function calculates and displays key statistical metrics such as mean, median, standard deviation, and skewness directly on the plot, making it a powerful tool for summarizing and interpreting the behavior of a single continuous variable.\n",
    "\n",
    "Alternatively to `boxplot`, you can use `violinplot`. I've included it in the function below, but commented it. You need to comment the boxplot and uncomment the violinplot to get it to work. But first, use the original setting with the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5868f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot a combined boxplot and histogram for univariate analysis of a continuous variable from Pandas.\n",
    "# bins is the number of bins for histogram\n",
    "def dist_box(data, bins=30):\n",
    "    if not isinstance(data, pd.Series):\n",
    "        raise ValueError(\"Input must be a pandas Series\")\n",
    "\n",
    "    # basic stats\n",
    "    name   = data.name.upper()\n",
    "    mean   = data.mean()\n",
    "    median = data.median()\n",
    "    std    = data.std()\n",
    "    skew   = data.skew()\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax_box, ax_dis) = plt.subplots(nrows=2, sharex=True,\n",
    "                                         gridspec_kw={\"height_ratios\": (0.2, 0.8)},\n",
    "                                         figsize=(10, 6))\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig.suptitle(f\"UNIVARIATE ANALYSIS: {name}\", fontsize=18, fontweight='bold')\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(x=data, showmeans = True, orient = 'h', color = 'violet', ax = ax_box)\n",
    "    #sns.violinplot(x= data, orient= 'h', color = 'violet', ax = ax_box)\n",
    "    \n",
    "    ax_box.set(xlabel = \"\" )\n",
    "    ax_box.grid(\"True\")\n",
    "    ax_box.set_title(\"Boxplot (Spread & Outliers)\", fontsize=12)\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data, bins=bins, ax=ax_dis, color='skyblue', edgecolor='black', kde=True)\n",
    "    \n",
    "    ax_dis.axvline(mean  , color='g', linestyle='-', linewidth=2, label=f'Mean: {mean:.2f}')\n",
    "    ax_dis.axvline(median, color='r', linestyle='--' , linewidth=2, label=f'Median: {median:.2f}')\n",
    "    ax_dis.legend(loc=\"upper left\")\n",
    "    ax_dis.grid(\"True\")\n",
    "    ax_dis.set_title(\"Histogram\", fontsize=12)\n",
    "\n",
    "    # Add text box with stats\n",
    "    stats_text = f\"\"\"Count: {data.count()}\n",
    "            Std Dev: {std:.2f}\n",
    "            Skewness: {skew:.2f}\"\"\"\n",
    "    props = dict(boxstyle =\"round\", facecolor= \"white\", alpha = 0.7)\n",
    "    ax_dis.text(0.15, 0.8, stats_text, transform = ax_dis.transAxes,\n",
    "                verticalalignment='top', horizontalalignment='right', bbox = props, fontsize =10)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all quantitative columns for checking the spread\n",
    "list_col = [\"Gain\", \"Frequency\", \"Direction\", \"Impedance\", \"SNR\"]\n",
    "\n",
    "# Loop through each column and apply the dist_box function\n",
    "for col in list_col:\n",
    "    if col in train_df.columns:\n",
    "        dist_box(train_df[col])\n",
    "    else:\n",
    "        print(f\"Column {col} does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4263bd",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Study the plotted data above and provide a detailed explanation of your observations. Focus on identifying trends, patterns, relationships, and any notable features within the data. Highlight variations, distributions, correlations, or anomalies, and explain their significance in the context of the analysis.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e2a47",
   "metadata": {},
   "source": [
    "As noted in the description of the training data, the dataset contains `NaN` values. Additionally, the `Frequency` and `Gain` have large ranges and need to be transformed for use in our ML model. For them, only scaling is not sufficient, and more tricks required to be applied.\n",
    "\n",
    "The code below preprocesses the dataset by addressing missing values, transforming numerical features, encoding categorical variables, and splitting the data into training and validation sets to prepare it for modeling.\n",
    "\n",
    "‚ö†Ô∏è Remember, you need to apply the same preprocessing steps to both training and validation sets. However, for steps involving fitting (like imputation or scaling), you should fit on the training set and then apply the same transformation to the validation set to avoid data leakage.\n",
    "\n",
    "***\n",
    "Stop working on the notebook and read the following page (Sections 11.1 and 11.2) on common pitfalls in using scikit-learn:\n",
    "\n",
    "https://scikit-learn.org/stable/common_pitfalls.html\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a42a0",
   "metadata": {},
   "source": [
    "Step 1) Replace infinite values with NaN in the entire DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN in the entire DataFrame\n",
    "for d in [train_df, val_df]:\n",
    "    d.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa327cd5",
   "metadata": {},
   "source": [
    "Step 2) Convert Frequency and Gain to logarithmic scale in both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Frequency and Gain to logarithmic scale\n",
    "for d in [train_df, val_df]:\n",
    "    d[\"Frequency\"] = np.log10(d[\"Frequency\"])\n",
    "    d[\"Gain\"]      = np.log10(d[\"Gain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7cebc",
   "metadata": {},
   "source": [
    "Step 4) Apply label encoding to categorical column on both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a40ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Apply label encoding to categorical column\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"Polarization\"] = label_encoder.fit_transform(train_df[\"Polarization\"])\n",
    "val_df[\"Polarization\"]   = label_encoder.transform(val_df[\"Polarization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1006905",
   "metadata": {},
   "source": [
    "Step 5) \n",
    "* Create imputer strategy with 'mean' to fill NaN values with the mean of each column.\n",
    "* Fit and transform the DataFrame using the imputer.\n",
    "\n",
    "Note: If you want to use other strategies, check the [SimpleImputer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Apply imputer (fit on training, transform both)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "train_df[:] = imputer.fit_transform(train_df)\n",
    "val_df[:]   = imputer.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd17f9e",
   "metadata": {},
   "source": [
    "Here, we provide data visualization for exploratory data analysis (EDA) using **Seaborn**. You are aware that Seaborn is a powerful Python visualization library built on top of Matplotlib. The main focus of our code sections is to reveal relationships and patterns in the `train_df` using two common types of plots: correlation heatmaps and pair plots. We/you are not allowed to visualize the `val_df` DataFrame, as it is reserved for validation purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(train_df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffde39e",
   "metadata": {},
   "source": [
    "Note: You can limit the heatmap plot by selecting a hanful columns of data. The code below does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective=[\"Frequency\", \"Gain\", \"Direction\"]  # Selective columns for heatmap\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(train_df[selective].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e914cdc",
   "metadata": {},
   "source": [
    "***\n",
    "‚ö†Ô∏è The tables shown above are not the confusion matrices. They represent a heatmap of correlation between different features in the training set.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_df, hue=\"Polarization\", diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe890b8d",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "Study all the plotted data above and provide a detailed explanation of your observations. Focus on identifying trends, patterns, relationships, and any notable features within the data. Highlight key information, such as variations, distributions, correlations, or anomalies, and explain their significance in the context of the analysis.\n",
    "\n",
    "**NOTE:** Since our dataset has few samples with limited features, you may not necessarily find strong correlation or patterns in the plotted data. However, remember to provide such analysis, or even more indepth as you will see later (or have seen earlier), in all of your data analysis in your practical project.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a265c",
   "metadata": {},
   "source": [
    "In this section, we examine the relationship between antenna Gain and Frequency. Our goal is to develop a ML Regression model that predicts antenna Gain based on Frequency. As a first step in this approach, we perform data visualization to explore the structure and patterns within our training dataset for Frequency and Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fb85d",
   "metadata": {},
   "source": [
    "Run the code section below first, and then read this note. Remember not to skip reading it!\n",
    "\n",
    "In the data visualization code below, and in earlier visualizations from previous notebooks, you might have noticed that I often use `alpha` values <1.0 (e.g., 0.7 or 0.5) when plotting. Have you ever wondered why?\n",
    "\n",
    "Let me break it down for you. This is actually a really useful trick for data anaysis to better understanding your data:\n",
    "\n",
    "1. **Make overlapping points easier to see**: When lots of data points overlap or are really close to each other, using full opacity (`alpha = 1.0`) can make them blend together or even hide some points entirely. Lowering the alpha lets you see these overlaps more clearly, making it easier to spot patterns like clusters or dense areas in your data.\n",
    "\n",
    "2. **Show data density**: transparnecy naturally darkens areas where points overlap, giving you a quick visual hint about where the data is concentracted. It's kind of like creating a density plot or heatmap, but without doing extra work.\n",
    "\n",
    "3. **Balance class visibility in multi-class data**: If you are plotting data with multiple classes (like with a `hue` in Seaborn), transparency helps to make sure all classes are visible, even if one class has way more data points than the others.\n",
    "\n",
    "4. **Keep plots readable when they are crowded**: When you work with a complex plots (like Seaborn pairplots), things can get messy if there are too many data points. Lowering the alpha reduces visual clutter, making it easier to interpret the plots without overwhelming your eyes.\n",
    "\n",
    "You can examine and verify all aforementioned points by setting alpha to 1.0 and re-run the plotting code below and compare with the original code with alpha=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_df[\"Frequency\"], train_df[\"Gain\"], \"o\", \n",
    "           markersize = 8, color=\"royalblue\", \n",
    "           markeredgecolor=\"black\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(r\"Frequency: $\\log_{10}(f)$ [Hz]\")\n",
    "plt.ylabel(r\"Gain: $\\log_{10}(G)$ [dB]\")\n",
    "plt.title(\"Antenna Gain\")\n",
    "plt.grid(\"True\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2cb13",
   "metadata": {},
   "source": [
    "***\n",
    "Did you notice the replaced NaNs with mean values in the dataset? As you see, they are concentrated at a certain frequency range ($2.2<log_{10}(f)<5$). This may cause bias in your model, and you need to be careful about it. due to its special distribution, it would have been better not to include NaNs in our dataset, and drop those rows instead of replacing them with mean values. You could use `dropna()` function for this purpose. For this lab, we continue with the imputed dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6efa56",
   "metadata": {},
   "source": [
    "The code section below computes the **SSR** and **MSE** for all possible split points in the training data, sorted by Frequency. We have seen this earlier in the lecture notes. For each frequency index, we split the data to the left and right of that point (for the left and right nodes), calculate the error for each node, by measuring how much the data in each node deviates from its local mean. This forms the basis for evaluating potential split points in our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b601a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort training data by Frequency. \n",
    "# This is important for the SSR and MSE calculations, as we explained in the lecture notes.\n",
    "train_df = train_df.sort_values(by = \"Frequency\")\n",
    "\n",
    "# Extract Frequency and Gain values\n",
    "f = train_df[\"Frequency\"].values\n",
    "g = train_df[\"Gain\"].values\n",
    "\n",
    "# Store SSR and MSE history\n",
    "ssr_hist_right = []\n",
    "ssr_hist_left  = []\n",
    "mse_hist_right = []\n",
    "mse_hist_left  = []\n",
    "\n",
    "# Loop to calculate averages, SSR, and MSE\n",
    "for i in range(len(f)):\n",
    "    # Calculate averages\n",
    "    avg_right = np.mean(g[i + 1:]) if i + 1 < len(f) else 0  # Handle empty slice for last index\n",
    "    avg_left = np.mean(g[0:i + 1])\n",
    "\n",
    "    # Calculate SSR\n",
    "    ssr_right = sum((g[i + 1:] - avg_right) ** 2)\n",
    "    ssr_left = sum((g[0:i + 1] - avg_left ) ** 2)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse_right = np.mean((g[i + 1:] - avg_right) ** 2) if i + 1 < len(f) else 0  # Handle empty slice\n",
    "    mse_left = np.mean((g[0:i + 1] - avg_left ) ** 2)\n",
    "    \n",
    "    # Append SSR and MSE to history\n",
    "    ssr_hist_right.append(ssr_right)\n",
    "    ssr_hist_left .append(ssr_left )\n",
    "    mse_hist_right.append(mse_right)\n",
    "    mse_hist_left .append(mse_left )\n",
    "\n",
    "# Plot SSR history\n",
    "plt.figure()\n",
    "plt.plot(range(len(ssr_hist_right)), ssr_hist_right, color=\"royalblue\", label=\"SSR Right Node\")\n",
    "plt.plot(range(len(ssr_hist_left )), ssr_hist_left , color=\"tomato\"   , label=\"SSR Left Node\" )\n",
    "plt.xlabel(\"Frequency index\")\n",
    "plt.ylabel(\"SSR\")\n",
    "plt.legend()\n",
    "plt.grid(\"True\")\n",
    "plt.title(\"SSR History\")\n",
    "\n",
    "# Plot MSE history\n",
    "plt.figure()\n",
    "plt.plot(range(len(mse_hist_right)), mse_hist_right, color=\"green\" , label=\"MSE Right Node\")\n",
    "plt.plot(range(len(mse_hist_left)) , mse_hist_left , color=\"orange\", label=\"MSE Left Node\" )\n",
    "plt.xlabel(\"Frequency index\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.grid(\"True\")\n",
    "plt.title(\"MSE History\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42bf9a",
   "metadata": {},
   "source": [
    "Before moving on to the Decision Tree, lets apply what we learned earlier on regression. In the section below, we train and compare **polynomial regression** models of varying degrees (including a linear baseline) to predict antenna Gain based on Frequency. We use the scikit-learn `pipeline` to streamline preprocessing (including polynomial feature expansion and standardization) and model fitting. The trained models are evaluated by plotting their predictions and reporting the R2-score on a validation set, helping assess model complexity and detect **underfitting** or **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19930002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train = train_df[[\"Frequency\"]].values\n",
    "y_train = train_df[\"Gain\"].values\n",
    "\n",
    "X_val = val_df[[\"Frequency\"]].values\n",
    "y_val = val_df[\"Gain\"].values\n",
    "\n",
    "# Prepare prediction X values\n",
    "x_line = np.linspace(train_df[\"Frequency\"].min(), train_df[\"Frequency\"].max(), 100).reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_train, y_train, \"o\", \n",
    "           markersize = 8, color=\"royalblue\", \n",
    "           markeredgecolor=\"black\", alpha=0.3,\n",
    "           label=\"Training\")\n",
    "\n",
    "# -------------------------------\n",
    "# Linear regression for reference\n",
    "linear_pipeline = Pipeline([\n",
    "    (\"scaler\"   , StandardScaler()), # Standardize the features\n",
    "    (\"regressor\", LinearRegression()) ])\n",
    "\n",
    "linear_pipeline.fit(X_train, y_train)\n",
    "y_linear = linear_pipeline.predict(x_line)\n",
    "plt.plot(x_line, y_linear, \"-.\", linewidth=2, color=\"royalblue\", label=\"Linear\")\n",
    "\n",
    "# -------------------------------\n",
    "# Polynomial Fits\n",
    "degrees = [3, 5, 7, 9]\n",
    "\n",
    "for degree, color in zip(degrees, colors):\n",
    "    poly_pipeline = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=degree)),\n",
    "        (\"scaler\", StandardScaler()), # Standardize the features\n",
    "        (\"regressor\", LinearRegression()) # Train linear regression using polynomial features\n",
    "    ])\n",
    "    \n",
    "    poly_pipeline.fit(X_train, y_train)\n",
    "    y_poly = poly_pipeline.predict(x_line)\n",
    "    plt.plot(x_line, y_poly, linewidth=2, color=color, label=f\"Polynom d={degree}\")\n",
    "    \n",
    "    y_val_pred = poly_pipeline.predict(X_val)\n",
    "    score = r2_score(y_val, y_val_pred)\n",
    "    print(f\"Degree {degree}: R2-score on validation: {score:.4f}\")\n",
    "\n",
    "plt.xlabel(r\"Frequency: $\\log_{10}(f)$ [Hz]\")\n",
    "plt.ylabel(r\"Gain: $\\log_{10}(G)$ [dB]\")\n",
    "plt.legend()\n",
    "plt.grid(\"True\")\n",
    "plt.title(\"Antenna Gain and Polynomial Regression Fits\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9284d",
   "metadata": {},
   "source": [
    "The code section below trains a Decision Tree regresor to predict a target varioble based on input features and evaluates its performance using metrics like MSE, SSR. These metrics help measure how accurately the model predicts and how well it explains the variance in the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbe64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error , r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "# As you see, the concept for developing a Decision Tree Regressor is similar to the polynomial regression, \n",
    "# but it uses a tree structure to make decisions based on feature values.\n",
    "dt_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Standardize the features\n",
    "    ('dt_regressor', DecisionTreeRegressor(random_state=42))  # Train Decision Tree Regressor\n",
    "])\n",
    "\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = dt_pipeline.predict(X_val)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error (y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "ssr = np.sum((y_val - y_pred) ** 2)\n",
    "r2  = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Decision Tree regression metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"SSR: {ssr:.4f}\")\n",
    "print(f\"R2-score: {r2:.4f}\")\n",
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(dt_pipeline.named_steps['dt_regressor'], feature_names=[\"Frequency\"],\n",
    "          filled   = True,\n",
    "          rounded  = True,\n",
    "          max_depth= 3,  # You can remove or increase this if the tree is shallow.\n",
    "          fontsize = 10 )\n",
    "plt.title(\"Decision Tree (Frequency vs. Gain)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c5d37",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- Study the decision tree and make sure you understand how the tree is made.\n",
    "\n",
    "- Compare model performance and results from `DecisionTreeRegressor` with those obtained earlier from the Linear Regression and Polynomial models. Which model performs better and why?\n",
    "\n",
    "- What are the advantages and disadvantages of using a Decision Tree Regressor compared to other regression models like Linear Regression or Polynomial models?\n",
    "\n",
    "- Study `DecisionTreeRegressor` in SciKit-Learn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "In case the link did not work, search for `DecisionTreeRegressor` in SciKit-Learn.\n",
    "\n",
    "- Read about `min_samples_split` and `min_samples_leaf` parameters in DecisionTreeRegressor.\n",
    "\n",
    "- Modify the `DecisionTreeRegressor` by changing the `max_depth` parameter. Train the model with different depths (e.g., 3, 5, 10) and observe how the evaluation metrics (MSE, MAE, SSR, R2-score) change.\n",
    "\n",
    "- Use the `.feature_importances_ attribute` of the `DecisionTreeRegressor` to rank the importance of features in the dataset. Visualize this using a bar chart.\n",
    "\n",
    "***\n",
    "‚ö†Ô∏è For Decision Trees, you need to be careful about overfitting. You can control the depth of the tree using the `max_depth` parameter. A deeper tree can capture more complex patterns but may also lead to overfitting, where the model performs well on training data but poorly on unseen data. In contrast, a shallower tree may underfit the data, failing to capture important patterns. Finding the right balance is crucial for optimal model performance.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b77ff2",
   "metadata": {},
   "source": [
    "In the previous code, we trained the `DecisionTreeRegressor` with default parameters, which may not provide the best model performance. Therefore, we need to imporve our model by incorporating **hyperparameter tuning** and **cross-validation**. Therefore, in the code section below,  we use `RandomizedSearchCV` to optimize the model by searching for the best combination of hyperparameters (`max_depth`, `min_samples_split`, and `min_samples_leaf`) within a specified range. Additionally, we perform **cross-validation** to ensure the model generalizes well to unseen data, reducing the risk of overfitting. This makes the second code more robust and capable of producing a well-optimized decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Build pipeline\n",
    "new_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Standardize the features\n",
    "    ('new_dt_regressor', DecisionTreeRegressor(random_state=42))  # Train Decision Tree Regressor\n",
    "])\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_distributions = {\n",
    "    'new_dt_regressor__max_depth'        : randint(2, 10),\n",
    "    'new_dt_regressor__min_samples_split': randint(2, 10),\n",
    "    'new_dt_regressor__min_samples_leaf' : randint(1, 10)\n",
    "}\n",
    "\n",
    "# Randomized search with cross-validation\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=new_pipeline,   # Use the new pipeline\n",
    "    param_distributions=param_distributions, # Hyperparameter search space\n",
    "    n_iter       = 30,    # Number of iterations for random search\n",
    "    cv           = 5,     # Cross-validation folds\n",
    "    scoring      = 'r2',  # Scoring metric for evaluation\n",
    "    n_jobs       = -1,    # Use all available CPU cores\n",
    "    random_state = 42,    # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Fit search\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "ssr = np.sum((y_val - y_pred) ** 2)\n",
    "r2  = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", search.best_params_)\n",
    "print(\"Decision Tree regression metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"SSR: {ssr:.4f}\")\n",
    "print(f\"R2-score: {r2:.4f}\")\n",
    "\n",
    "# Cross-validation with best model\n",
    "cv_scores = cross_val_score(best_model, X_val, y_val, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-Validated R2-Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validated R2: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(best_model.named_steps['new_dt_regressor'], feature_names=[\"Frequency\"],\n",
    "          filled   = True,\n",
    "          rounded  = True,\n",
    "          max_depth= 3,  # You can remove or increase this if the tree is shallow\n",
    "          fontsize = 10 )\n",
    "plt.title(\"Best Decision Tree Model (Frequency vs. Gain)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9689db",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- Compare performance metrics (MSE, MAE, R2-score) of the tuned model with those obtained earlier from the model with default parameters. Which one performs better and why?\n",
    "\n",
    "- Change the range of values in the `param_distributions`. For example, increase the maximum value of `max_depth` to 20; and/or Modify `min_samples_split` and `min_samples_leaf` to have a broader range (e.g., randint(2, 20)). Rerun the code and observe how the best parameters and model performance change.\n",
    "\n",
    "- Increase the `n_iter` parameter in `RandomizedSearchCV` from 30 to \\{50 or 100\\}. Does the performance improve with more iterations? Why or why not?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bf1d8",
   "metadata": {},
   "source": [
    "Up to this point, we focused on developing a Frequency-Gain model, which primarily relied on a single feature (`Frequency`) to predict the target variable (`Gain`). However, our dataset contains additional features (i.e., multi-dimensional), making it more complex for prediction. To better leverage this complexity, we updated the code to include all available features, except for the target variable (`Gain`), in the feature sets (`X_train` and `X_val`). By excluding `Gain` from the input features, we ensure proper training practices and avoid data leakage, where the target variable could improperly influence predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire features for X_train and X_val\n",
    "X_train = train_df.drop(columns=[\"Gain\"]).values  # Exclude \"Gain\" from training features\n",
    "y_train = train_df[\"Gain\"].values  # Target variable\n",
    "\n",
    "X_val = val_df.drop(columns=[\"Gain\"]).values  # Exclude \"Gain\" from validation features\n",
    "y_val = val_df[\"Gain\"].values  # Target variable\n",
    "\n",
    "# Build pipeline\n",
    "new_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize the features\n",
    "    ('full_dt_regressor', DecisionTreeRegressor(random_state=42))  # Train Decision Tree Regressor\n",
    "])\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_distributions = {\n",
    "    'full_dt_regressor__max_depth'        : randint(2, 10),\n",
    "    'full_dt_regressor__min_samples_split': randint(2, 10),\n",
    "    'full_dt_regressor__min_samples_leaf' : randint(1, 10)\n",
    "}\n",
    "\n",
    "# Randomized search with cross-validation\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=new_pipeline,   # Use the new pipeline\n",
    "    param_distributions=param_distributions, # Hyperparameter search space\n",
    "    n_iter       = 30,    # Number of iterations for random search\n",
    "    cv           = 5,     # Cross-validation folds\n",
    "    scoring      = 'r2',  # Scoring metric for evaluation\n",
    "    n_jobs       = -1,    # Use all available CPU cores\n",
    "    random_state = 42,    # Random state for reproducibility\n",
    ")\n",
    "\n",
    "# Fit search\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "ssr = np.sum((y_val - y_pred) ** 2)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", search.best_params_)\n",
    "print(\"Decision Tree regression metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"SSR: {ssr:.4f}\")\n",
    "print(f\"R2-score: {r2:.4f}\")\n",
    "\n",
    "# Cross-validation with best model\n",
    "cv_scores = cross_val_score(best_model, X_val, y_val, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-Validated R2-Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validated R2: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(best_model.named_steps['full_dt_regressor'], feature_names=train_df.columns,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          max_depth=3,  # You can remove or increase this if the tree is shallow\n",
    "          fontsize=10)\n",
    "plt.title(\"Best Decision Tree Model (All Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5556df9",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚ö° Mandatory submission\n",
    "\n",
    "Construct the \"Rod Breaking\" example from the lecture notes, build the decision tree, and find out if the rode breaks for the given test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6a293",
   "metadata": {},
   "source": [
    "***\n",
    "   END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
