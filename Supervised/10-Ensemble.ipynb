{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8aa507",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2025-08-10\n",
    "\n",
    "**Last update:** 2025-10-02\n",
    "\n",
    "**MIT License** â€” Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, UmeÃ¥ University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b114e",
   "metadata": {},
   "source": [
    "ðŸ“¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget ðŸ™‚\n",
    "\n",
    "* Your answers for the \"âš¡ Mandatory\" sections of each lab <span style=\"color:red\"><strong>must be submitted before the start of the next lab session</strong></span>.\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00ea61",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Purpose and Learning Outcomes:\n",
    "\n",
    "- This lab aims to introduce you to ensemble learning techniques. You will learn how to implement and evaluate various ensemble models using Scikit-Learn.\n",
    "\n",
    "- Our intention in this notebook is to use various ensemble models to solve a multi-feature (multi-dimensional) classification problem.\n",
    "\n",
    "- You learn how to cross-validate and hyperparameter tune your models to improve their performance.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "from notebook_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525523a",
   "metadata": {},
   "source": [
    "***\n",
    "# Ensemble Learning\n",
    "\n",
    "Ensemble learning is a powerful ML technique that enhances the performance of predictive models by combining the predictions of multiple base learners (weak learners). Instead of relying on just one strong model, we use a group of \"base/weak learners\" and blend their answers. This usually gives us more accurate and reliable results because the strengths of one model can help make up for the weaknesses of another.\n",
    "\n",
    "In this section, we will go over the main ideas behind ensemble learning that we talked about in class, plus introduce a few new Ada-based models we did not cover or discuss. By the end, you will have an understanding of how these methods work and how they can be used in real-world ML projects. Some of these models, as I explained, are very popular in training ML models.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3284e6",
   "metadata": {},
   "source": [
    "- In the code section below, we have a function that generates a dataset for classification, simulating a real-world scenario with multiple features. All generated features are made as `informative`, meaning they directly contribute to the class labels. The non-informative features (also called \"noise\" or \"redundant\" features) do not contribute to the class labels. Here, we want all features to be informative, therefore, `n_informative` is set to the total number of features. You can read about it in the SciKit-Learn documentation on `make_classification()` function.\n",
    "\n",
    "- Another new aspect of the developed function below is error handling in the function using `assert`. The `assert` statements in the function act as guardrails, ensuring that the input parameters meet necessary preconditions for the function to operate correctly. It is a good practice to use `assert` in your codes and to evaluate the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a classification dataset\n",
    "def make_multiclasses_classification(n_samples=1000, \n",
    "                                     n_features=5  , \n",
    "                                     n_classes=3   , \n",
    "                                     noise_std=0.3 ):\n",
    "    assert n_samples  >= 100, \"Number of samples must be at least 100.\"\n",
    "    assert n_features >= 2, \"Number of features must be at least 2.\"\n",
    "    assert n_classes  >= 2 and n_classes <= 5, \"Number of classes must be between 2 and 5.\"\n",
    "\n",
    "    X, y = make_classification(n_samples=n_samples,\n",
    "                               n_features=n_features,\n",
    "                               n_informative=n_features,  # all features are informative\n",
    "                               n_redundant=0,\n",
    "                               n_classes=n_classes,\n",
    "                               n_clusters_per_class=1,\n",
    "                               class_sep=1.5,                               \n",
    "                               random_state=42)\n",
    "    \n",
    "    # Optional: Add Gaussian noise (like in make_blobs version)\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_std, size=X.shape)\n",
    "    X_noisy = X + noise\n",
    "\n",
    "    return X_noisy, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cc129",
   "metadata": {},
   "source": [
    "Here, we generate data using our `make_multiclasses_classification` function, and visualize the training dataset. We generate a grid of scatter plots (i.e., a **pairwise scatter plot matrix**) for the training data, where each subplot shows the relationship between two different features. The diagonal subplots display histograms of individual features. In the previous notebook on Decision Trees, we used Seaborn's `pairplot` to create pairwise plots from a Pandas DataFrame. In this notebook, I demonstrate an alternative approach to visualizing pairwise data without relying on the Pandas framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========== MAIN ==========\n",
    "n_samples  = 2000  # Number of samples\n",
    "n_features = 5     # Number of features\n",
    "n_classes  = 3     # Number of classes\n",
    "noise_std  = 0.1   # Standard deviation of Gaussian noise\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_multiclasses_classification(n_samples, n_features, n_classes, noise_std)\n",
    "\n",
    "# Separate into training and test sets.\n",
    "# I've used 70% of the data for training and 30% for testing.\n",
    "# Often, a 80-20 or 70-30 split is used in practice.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"Dataset shape: X_train={X_train.shape}, y_train={y_train.shape}, X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# ===== plot data =====\n",
    "fig=plt.figure( figsize=(15, 15) )\n",
    "plt_num = 1  # Initialize plot number\n",
    "\n",
    "for i in range(n_features):\n",
    "    for j in range(n_features):\n",
    "        ax = fig.add_subplot(n_features, n_features, plt_num)\n",
    "        if(i == j):\n",
    "            ax.hist(X_train[:, i], bins=25, color='gray')\n",
    "        else:\n",
    "            ax.scatter(X_train[:, j], X_train[:, i], c=np.array(colors)[y_train], s=30, alpha=0.3)\n",
    "                \n",
    "        if(i == n_features-1):\n",
    "            ax.set_xlabel(f'$x_{{{j}}}$', fontsize=22)\n",
    "        \n",
    "        if(j==0):\n",
    "            ax.set_ylabel(f'$x_{{{i}}}$', fontsize=22)\n",
    "\n",
    "        ax.grid(True)\n",
    "        plt_num +=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3b133",
   "metadata": {},
   "source": [
    "***\n",
    "We want to use Ensemble models to solve this multi-feature classification problem. Before proceeding with this notebook, make sure you have installed the following Python packages, as they are necessary for running the next code sections:\n",
    "\n",
    "- `xgboost`\n",
    "- `lightgbm`\n",
    "- `catboost`\n",
    "\n",
    "You can install these packages using one of the following methods:\n",
    "\n",
    "- If you use `pip`:\n",
    "\n",
    "```bash\n",
    "pip install xgboost lightgbm catboost\n",
    "```\n",
    "\n",
    "- If you use `conda`:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge xgboost lightgbm catboost\n",
    "```\n",
    "\n",
    "In worst case that you could not install the packages, you can comment out the code sections that use these packages. Do not spend a lot of time trying to install them. If you know your environment, it should not take more than a minute to install them.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5b191",
   "metadata": {},
   "source": [
    "Below, I've developed a comprehensive class named `ClassifierComparison` to evaluate and compare various non-parametric classification models. I assume after almost 5 weeks of using Python, you are familiar with such type of code and you should know how to read and interpret it. However, if you have any questions about the code, please ask me.\n",
    "\n",
    "The `fit_models` function trains a series of classifiers (Decision Tree, Random Forest, Bagging, AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the training data, measures their training and prediction time (computation run-time), and calculates key performance metrics (accuracy, precision, recall, and F1-score). \n",
    "\n",
    "The `print_summary` function displays the model performance metrics. It once prints the metrics sorted by accuracy and once sorted by training time for easy comparison. Then we visualize the confusion matrix for each trained model and visualize the decision boundaries learned by each model for a specified pair of features. For plotting the decision boundary, we need to select 2 feature because we show them on a 2D plane, providing a graphical representation of how each classifier separates different classes in the feature space. Please note that our data is multi-dimensional.\n",
    "\n",
    "Carefully study the `ClassifierComparison` class in the code section below. The code is lengthy, but I have added comments to help you understand each part. If you have any questions about the code, please ask me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2298a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, \n",
    "                             confusion_matrix, ConfusionMatrixDisplay)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, BaggingClassifier, \n",
    "                              AdaBoostClassifier, GradientBoostingClassifier)\n",
    "from xgboost import XGBClassifier        # Requires installation of the package ; Not a native function in sklearn\n",
    "from lightgbm import LGBMClassifier      # Requires installation of the package; Not a native function in sklearn\n",
    "from catboost import CatBoostClassifier  # Requires installation of the package; Not a native function in sklearn\n",
    "\n",
    "# This class is designed to compare various classifiers on a given dataset.\n",
    "class ClassifierComparison:\n",
    "    def __init__(self, X, y, test_size=0.3, use_bootstrap=True, random_state=42):\n",
    "        # Split data stratified by labels\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler       = StandardScaler()\n",
    "        self.X_train = scaler.fit_transform(X_train)\n",
    "        self.X_test  = scaler.transform(X_test)\n",
    "\n",
    "        # Take local copies for later use\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "        self.use_bootstrap = use_bootstrap\n",
    "        self.models     = {}\n",
    "        self.results    = {}\n",
    "        self.results_df = None\n",
    "\n",
    "    def fit_models(self):\n",
    "        # Define the classifiers to be compared\n",
    "        self.models = {\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, bootstrap=self.use_bootstrap, random_state=42),\n",
    "            'Bagging' : BaggingClassifier(n_estimators=100, bootstrap=self.use_bootstrap, random_state=42),\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=100, algorithm='SAMME', random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "            'XGBoost' : XGBClassifier(n_estimators=100, eval_metric='mlogloss', random_state=42),\n",
    "            'LightGBM': LGBMClassifier(n_estimators=100, random_state=42),\n",
    "            'CatBoost': CatBoostClassifier(verbose=0, iterations=100, random_state=42)\n",
    "        }\n",
    "\n",
    "        results_list = [] # List to store results for DataFrame\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            # tic-toc the training and prediction time\n",
    "            start_time_train = time.time()          # start the timer for training\n",
    "            model.fit(self.X_train, self.y_train)   # train the model\n",
    "            end_time_train = time.time()            # stop the timer for training\n",
    "            \n",
    "            start_time_pred = end_time_train        # start the timer for prediction\n",
    "            y_pred = model.predict(self.X_test)     # make predictions\n",
    "            end_time_pred = time.time()             # stop the timer for prediction\n",
    "\n",
    "            # Calculate metrics and run time\n",
    "            metrics = {\n",
    "                'Model': name,\n",
    "                'Accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'Precision': precision_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "                'Recall': recall_score(self.y_test, y_pred, average='weighted'),\n",
    "                'F1 Score': f1_score(self.y_test, y_pred, average='weighted'),\n",
    "                'Training Time (s)': (end_time_train - start_time_train),\n",
    "                'Prediction Time (s)': (end_time_pred - start_time_pred),\n",
    "                'Total Time (s)': (end_time_pred - start_time_train)\n",
    "            }\n",
    "\n",
    "            self.results[name] = {\n",
    "                'model': model,\n",
    "                'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "            }\n",
    "\n",
    "            results_list.append(metrics)\n",
    "\n",
    "        # Create a DataFrame from the results list\n",
    "        self.results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"\\n\")\n",
    "        print(\"------ Results Sorted by Accuracy ------\")\n",
    "        print(self.results_df.sort_values(by='Accuracy', ascending=False).to_string(index=False))\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"------ Results Sorted by Total Time ------\")\n",
    "        print(self.results_df.sort_values(by='Total Time (s)', ascending=True).to_string(index=False))\n",
    "\n",
    "    def plot_confusion_matrices(self):\n",
    "        for name, result in self.results.items():\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=result['confusion_matrix'])\n",
    "            disp.plot()\n",
    "            plt.title(f\"Confusion Matrix: {name}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Plot decision boundaries for 2 features of different classifiers.\n",
    "    # The default feature indices are (0, 1), but you can specify any two features.\n",
    "    def plot_decision_boundaries(self, feature_indices=(0, 1)):\n",
    "        i, j = feature_indices\n",
    "        if self.X_train.shape[1] < 2:\n",
    "            print(\"Decision boundary plot requires at least 2 features.\")\n",
    "            return\n",
    "\n",
    "        h = 0.01\n",
    "        x_min, x_max = self.X_train[:, i].min() - 0.5, self.X_train[:, i].max() + 0.5\n",
    "        y_min, y_max = self.X_train[:, j].min() - 0.5, self.X_train[:, j].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "\n",
    "        X_vis = np.zeros((xx.size, self.X_train.shape[1]))\n",
    "        X_vis[:, i] = xx.ravel()\n",
    "        X_vis[:, j] = yy.ravel()\n",
    "\n",
    "        for name, result in self.results.items():\n",
    "            model = result['model']\n",
    "            # Predict the decision boundary. \n",
    "            # We use try-except to handle any potential errors during prediction.\n",
    "            # Google the \"try-except\" in Python and learn it.\n",
    "            try:\n",
    "                prd = model.predict(X_vis)\n",
    "                prd = prd.reshape(xx.shape)\n",
    "                \n",
    "                # Create a colormap for the regions\n",
    "                region_cmap = ListedColormap(colors[:len(np.unique(self.y_train))])\n",
    "                point_colors = np.array(colors)[self.y_train]\n",
    "\n",
    "                plt.figure()\n",
    "                plt.contourf(xx, yy, prd, alpha=0.3, cmap=region_cmap)\n",
    "                plt.scatter(self.X_train[:, i], self.X_train[:, j],\n",
    "                            c=point_colors, edgecolor='k', s=40, alpha=0.8)\n",
    "                plt.xlabel(\"$x_{i}$\", fontsize=14)\n",
    "                plt.ylabel(\"$x_{j}$\", fontsize=14)\n",
    "                plt.title(f\"Decision Boundary: {name} (Features x{i} vs x{j})\")\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: failed to predict decision boundary. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1df6cf",
   "metadata": {},
   "source": [
    "Now we create an instance of the `ClassifierComparison` class using training and test data. The we fit multiple classification models to the training data, evaluate their performance on the test data, display confusion matrices for each model, and plot their decision boundaries using the first two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN =====\n",
    "n_samples  = 2000  # Number of samples\n",
    "n_features = 5     # Number of features\n",
    "n_classes  = 3     # Number of classes\n",
    "noise_std  = 0.1   # Standard deviation of Gaussian noise\n",
    "\n",
    "# Generate a multiclass classification dataset\n",
    "X, y = make_multiclasses_classification(n_samples, n_features, n_classes, noise_std)\n",
    "\n",
    "# Create an instance of ClassifierComparison and fit the models\n",
    "clf = ClassifierComparison(X, y)\n",
    "clf.fit_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b18c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of results\n",
    "clf.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56d309",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### âœ… Check your understanding\n",
    "\n",
    "- Carefully study the report above and investigate different models accuracy and run-time. Which model is the fastest? Which model is the slowest? Which model has the highest accuracy? Is there any trade-off between accuracy and run-time?\n",
    "\n",
    "âš ï¸ Do not generalize your answers. Your answers are specific to this dataset and the parameters used in the models.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dca5d",
   "metadata": {},
   "source": [
    "You can also plot the confusion matrix for each model using the `plot_confusion_matrices` but I've commented it out as it does not add much value since we already have the accuracy, precision, recall, and F1-score metrics in the summary table. However, if you want to see the confusion matrices, you can uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each model\n",
    "# clf.plot_confusion_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2700046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries obtained from each model for the first two features\n",
    "clf.plot_decision_boundaries((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad037f56",
   "metadata": {},
   "source": [
    "***\n",
    "### âœ… Check your understanding\n",
    "\n",
    "- Increase the number of input features from 5 to 50 and re-run the classifier comparison. What changes do you observe in model performance and computation time? Which classifier handles the increased feature space most effectively? I suggest to write a spearate code block for this, because you want to compare the results with the previous results.\n",
    "\n",
    "- In our developed class, we used `StandardScaler()` for feature scaling. Read SciKit-Learn page on `sklearn.preprocessing` class. There you see several ther feature scaling methods (e.g., MinMaxScaler). Read about them and think on which scalar is more suitable for your data. You should choose the one that suits your data the best. You can also try different scalers and see how they affect the model performance. This is indeed a difficult task.\n",
    "\n",
    "***\n",
    "\n",
    "### âš ï¸âš ï¸ IMPORTANT âš ï¸âš ï¸\n",
    "Generally speaking, you see how many different parameters and choices you have in building a machine learning model. This is why machine learning is more of an art than a science. There is no one-size-fits-all solution. You need to experiment and find what works best for your specific problem and dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127c202",
   "metadata": {},
   "source": [
    "## Room for Improvement?\n",
    "\n",
    "Are we happy with the results we obtained above? Can we do better?\n",
    "\n",
    "Of course we can! Before that, lets recap the techniques we can use to improve our model's performance: Cross-Validation and Hyperparameter Tuning.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "In case you forgot, cross validation is a fundamental technique for evaluating and improving the generalizability of ML models. It plays a key role in ensuring that your model performs well not just on the training data, but also on new data. For classification problems using `StratifiedKFold` is highly recommended, especially when the dataset has imbalanced classes. See your earlier lecture notes or check out [this link](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "### Hyperparameter Tuning: Optimizing model performance\n",
    "\n",
    "Hyperparameters are settings that define how a model operates, such as the maximum depth of a decision tree, the number of estimators in an ensemble method, or the learning rate for gradient boosting. Choosing the right combination of hyperparameters can significantly impact a model's accuracy, stability, and overall effectiveness.\n",
    "\n",
    "Rather than relying on default settings or trial-and-error, hyperparameter tuning systematically searches for the optimal configuration. One popular method for this task is `GridSearch`, which combines the power of exhaustive search and cross-validation. `GridSearch` works by defining a \"grid\" of hyperparameter values you want to test. For each combination in the grid, the model is trained and validated using cross-validation (e.g., StratifiedKFold). By evaluating performance across all combinations, `GridSearchCV` identifies the hyperparameter settings that provide the best results based on the chosen metric, such as accuracy, precision, or mean squared error.\n",
    "\n",
    "The combination of cross-validation and hyperparameter tuning is a cornerstone of **modern machine learning workflows (pipelines)**. Together, these techniques make sure that your model is not only well evaluated but also fine tuned for optimal performance. By investing time in these steps, you can build models that generalize well to new data, avoiding pitfalls like overfitting or underfitting and achieving better results across the board.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### âš ï¸âš ï¸ REMINDER âš ï¸âš ï¸\n",
    "Generally speaking, you see how many different parameters and choices you have in building a machine learning model. This is why machine learning is more of an art than a science. There is no one-size-fits-all solution. You need to experiment and find what works best for your specific problem and dataset.\n",
    "\n",
    "\n",
    "âš ï¸ NOTE:\n",
    "**You need to perform cross-validation and hyperparameter tuning to improve your model's performance for your FINAL, practical project. Write it down in your to-do list, and remember it, not only for your project, but for any future machine learning tasks you work on.**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835122b",
   "metadata": {},
   "source": [
    "\n",
    "In the code section below, I have developed the modified version of `ClassifierComparison` class, named `ClassifierComparisonOpt`, as optimizer. This class integrates both cross-validation and hyperparameter tuning to give a more accurate and fair comparison across popular classifiers like Decision Trees, Random Forests, XGBoost, and others. I intentionally opt out LightGBM and CatBoost to reduce the execution time, and I recommend not to include them in this lab. \n",
    "\n",
    "Our new class automatically scales features, runs grid search over predefined hyperparameter ranges using k-fold cross-validation, and evaluates the best model on a hold-out test set. This ensures that each model is tuned and validated properly before comparing their accuracy, precision, F1 score, confusion matrices, and decision boundaries.\n",
    "\n",
    "First, focus on `get_models_with_params` function developed in `ClassifierComparisonOpt` class. The function defines a dictionary of popular classifiers paired with their respective hyperparameter grids (perhaps not all hyperparameters in that model, but rather the most important ones). Each entry consists of a model instance and a dictionary specifying the range of values to explore for key hyperparameters like `max_depth`, `n_estimators`, `learning_rate`, or `min_samples_split`. This setup allows the class to systematically apply grid search using `GridSearchCV` to each model, verify that every algorithm is tuned over a meaningful range of settings for fair and optimized comparison.\n",
    "\n",
    "Then, the `fit_models` function performs hyperparameter tuning using `GridSearchCV` with cross-validation for each classifier, fits the best model to the training data, evaluates it on the test set, and records performance metrics, and the best parameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7395307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# This is an optimized version of the ClassifierComparison class that includes \n",
    "# additional features such as cross-validation, hyperparameter tuning, and more detailed metrics. \n",
    "# It is designed to handle larger datasets and provide a more comprehensive analysis of classifier performance.\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "# Feel free to use it in your projects, with some modifications e.g., with classifiers and hyperparameters.\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "class ClassifierComparisonOpt:\n",
    "    def __init__(self, X, y, test_size=0.3, use_bootstrap=True, random_state=42, cv_folds=5):\n",
    "        # Split data stratified by labels\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        self.X_train = scaler.fit_transform(X_train)\n",
    "        self.X_test  = scaler.transform(X_test)\n",
    "\n",
    "        self.y_train       = y_train\n",
    "        self.y_test        = y_test\n",
    "        self.use_bootstrap = use_bootstrap\n",
    "        self.cv_folds      = cv_folds\n",
    "        self.models        = {}\n",
    "        self.results       = {}\n",
    "        self.results_df    = None\n",
    "\n",
    "    # Define the classifiers and their hyperparameters\n",
    "    def get_models_with_params(self):\n",
    "        return {\n",
    "            'Decision Tree': (DecisionTreeClassifier(random_state=42), {\n",
    "                'max_depth': [None, 3, 5, 10, 20],\n",
    "                'min_samples_split': [2, 5, 7, 10]\n",
    "            }),\n",
    "            'Random Forest': (RandomForestClassifier(bootstrap=self.use_bootstrap, random_state=42), {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 3, 5, 10, 20],\n",
    "                'min_samples_split': [2, 5, 7, 10]\n",
    "            }),\n",
    "            'Bagging': (BaggingClassifier(bootstrap=self.use_bootstrap, random_state=42), {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_samples': [0.5, 1.0],\n",
    "                'oob_score': [True, False],\n",
    "            }),\n",
    "            # SAMME: Stagewise Additive Modeling using a Multi-class Exponential loss\n",
    "            'AdaBoost': (AdaBoostClassifier(algorithm='SAMME', random_state=42), {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.01, 0.1, 0.2, 0.5, 1.0]\n",
    "            }),\n",
    "            'Gradient Boosting': (GradientBoostingClassifier(random_state=42), {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "                'max_depth': [3, 5, 10]\n",
    "            }),\n",
    "            'XGBoost': (XGBClassifier(eval_metric='mlogloss', random_state=42), {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "                'max_depth': [3, 5, 10]\n",
    "            }),\n",
    "            #'LightGBM': (LGBMClassifier(random_state=42), {\n",
    "            #    'n_estimators': [50, 100, 200],\n",
    "            #    'learning_rate': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "            #    'max_depth': [-1, 5, 10]\n",
    "            #}),\n",
    "            #'CatBoost': (CatBoostClassifier(verbose=0, random_state=42), {\n",
    "            #    'iterations': [50, 100, 200],\n",
    "            #    'learning_rate': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "            #    'depth': [4, 6, 10]\n",
    "            #})\n",
    "        }\n",
    "\n",
    "    def fit_models(self):\n",
    "        results_list = []\n",
    "        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)\n",
    "        models_with_params = self.get_models_with_params()\n",
    "\n",
    "        for name, (model, param_grid) in models_with_params.items():\n",
    "            print(f\"Tuning {name} ...\")\n",
    "            \n",
    "            grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "            \n",
    "            start_train = time.time()\n",
    "            grid_search.fit(self.X_train, self.y_train)\n",
    "            end_train = time.time()\n",
    "\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(self.X_test)\n",
    "            end_pred = time.time()\n",
    "\n",
    "            self.models[name] = {\n",
    "                'model': best_model,\n",
    "                'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "            }\n",
    "\n",
    "            metrics = {\n",
    "                'Model': name,\n",
    "                'Accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'Precision': precision_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "                'Recall': recall_score(self.y_test, y_pred, average='weighted'),\n",
    "                'F1 Score': f1_score(self.y_test, y_pred, average='weighted'),\n",
    "                'Best Params': grid_search.best_params_,\n",
    "                'Training Time (s)': (end_train - start_train),\n",
    "                'Prediction Time (s)': (end_pred - end_train),\n",
    "                'Total Time (s)': (end_pred - start_train)\n",
    "            }\n",
    "\n",
    "            results_list.append(metrics)\n",
    "\n",
    "        self.results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"\\n------ Results Sorted by Accuracy ------\")\n",
    "        print(self.results_df.sort_values(by='Accuracy', ascending=False).to_string(index=False))\n",
    "\n",
    "        print(\"\\n------ Results Sorted by Total Time ------\")\n",
    "        print(self.results_df.sort_values(by='Total Time (s)', ascending=True).to_string(index=False))\n",
    "\n",
    "    # Show feature importance for models that support it          \n",
    "    def show_feature_importance(self):\n",
    "        importance = {}\n",
    "\n",
    "        for name, result in self.models.items():\n",
    "            model = result['model']\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance[name] = model.feature_importances_\n",
    "            elif hasattr(model, 'coef_'):\n",
    "                coef = model.coef_\n",
    "                if coef.ndim == 1:\n",
    "                    importance[name] = np.abs(coef)\n",
    "                else:\n",
    "                    importance[name] = np.mean(np.abs(coef), axis=0)\n",
    "            else:\n",
    "                print(f\"Feature importance not available for model {name}\")\n",
    "\n",
    "        for name, imp in importance.items():\n",
    "            sorted_idx = np.argsort(imp)[::-1]\n",
    "            plt.figure()\n",
    "            plt.bar(range(len(imp)), imp[sorted_idx], align='center')\n",
    "            plt.xticks(range(len(imp)), sorted_idx)\n",
    "            plt.title(f\"Feature importance for {name}\")\n",
    "            plt.xlabel(\"Feature index\")\n",
    "            plt.ylabel(\"Importance score\")\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6206a",
   "metadata": {},
   "source": [
    "The code section below may take a while to run, depending on your machine's performance and the number of hyperparameter combinations being tested. Please be patient while it executes. On my office desktop, it takes ~27 seconds to run, and indeed the XGBoost model is the most time-consuming among listed models. You cans also exclude it from the list of models to speed up the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9897485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN =====\n",
    "# optimized classifiers and find their hyperparameters\n",
    "clf_opt = ClassifierComparisonOpt(X, y)\n",
    "clf_opt.fit_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of results\n",
    "clf_opt.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature importance for each model\n",
    "clf_opt.show_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f38b08",
   "metadata": {},
   "source": [
    "***\n",
    "### âš¡ Mandatory submission\n",
    "\n",
    "- Study the results obtained from the optimized classifier comparison. Compare these results with those from the initial comparison without cross-validation and hyperparameter tuning. Which models showed the most improvement? How did the computation time change? Shortly discuss your observations.\n",
    "\n",
    "- What do the feature importance plots tell you? You can read about feature importance in SciKit-Learn [here](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f9e08",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "\n",
    "Parallelization in ML allows multiple computations to run simultaneously, significantly speeding up training and evaluation, especially when working with many classifiers or large datasets. Some classifiers like `Random Forest`, `Bagging`, and `Gradient Boosting` natively support parallel training by setting the `n_jobs` parameter. If `n_jobs=k` then computations are partitioned into `k` parallel jobs, and run on k CPU cores of the machine. If `n_jobs=-1` then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using k jobs will unfortunately not be k times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets). When tuning hyperparameters across multiple models, `GridSearchCV` also supports parallelization with `n_jobs=-1`, allowing it to train and evaluate multiple parameter combinations at once. You can see the `n_jobs` settings in the `ClassifierComparisonOpt` class we developed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620a4d0",
   "metadata": {},
   "source": [
    "***\n",
    "# â›·ï¸ Exercise\n",
    "\n",
    "In this exercise, you should work on the **Breast Cancer** dataset from from `sklearn.datasets.load_breast_cancer` to build, train, and evaluate a classifier. Your goal is to classify breast cancer cases as **malignant** (cancer) or **benign** (no cancer) using the provided features.\n",
    "\n",
    "Carefully analyze the dataset, train your model on the training data, and evaluate its performance using appropriate metrics.\n",
    "\n",
    "Here are some hints and todos:\n",
    "- How to load the dataset:\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "```\n",
    "\n",
    "- Explore the dataset:\n",
    "    * Check the shape of X and y to understand the dimensions.\n",
    "    * Display the feature names and target classes to familiarize yourself with the data.\n",
    "    * Ensure there are no missing values or anomalies in the data.\n",
    "\n",
    "- Split, Scale, (and Impute the data if needed) similar to what we did in `ClassifierComparisonOpt`.\n",
    "\n",
    "- Use different classifiers and experiment with hyperparameters to improve the model's performance. Use what you learned earlier from the `ClassifierComparisonOpt` class.\n",
    "\n",
    "- Finally, evaluate the model using the following metrics:\n",
    "    * Confusion Matrix\n",
    "    * Classification Report\n",
    "    * Accuracy Score\n",
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
