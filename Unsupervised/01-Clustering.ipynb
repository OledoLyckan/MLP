{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57294c7f",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2025-08-16\n",
    "\n",
    "**Last update:** 2025-10-06\n",
    "\n",
    "**MIT License** ‚Äî Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Ume√• University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573ea48",
   "metadata": {},
   "source": [
    "üì¢ <span style=\"color:red\"><strong> Note for Students:</strong></span>\n",
    "\n",
    "* Before working on the labs, review your lecture notes.\n",
    "\n",
    "* Please read all sections, code blocks, and comments **carefully** to fully understand the material. Throughout the labs, my instructions are provided to you in written form, guiding you through the materials step-by-step.\n",
    "\n",
    "* All concepts covered in this lab are part of the course and may be included in the final exam.\n",
    "\n",
    "* I strongly encourage you to work in pairs and discuss your findings, observations, and reasoning with each other.\n",
    "\n",
    "* If something is unclear, don't hesitate to ask.\n",
    "\n",
    "* I have done my best to make the lab files as bug-free (and error-free) as possible, but remember: *there is no such thing as bug-free code.* If you observed any bugs, errors, typos, or other issues, I would greatly appreciate it if you report them to me by email. Verbal notifications are not work, as I will likely forget üôÇ\n",
    "\n",
    "* Your answers for the \"‚ö° Mandatory\" sections of each lab <span style=\"color:red\"><strong>must be submitted before the start of the next lab session</strong></span>.\n",
    "\n",
    "ENJOY WORKING ON THIS LAB.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f6b2e",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Purpose and Learning Outcomes:\n",
    "\n",
    "- Understand the concept of clustering.\n",
    "- Apply the K-Means clustering algorithm.\n",
    "- Evaluate clustering performance using metrics like inertia and silhouette score.\n",
    "- Understand the DBSCAN clustering algorithm.\n",
    "- Apply Gaussian Mixture Models (GMM) for clustering.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "from notebook_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8439f1",
   "metadata": {},
   "source": [
    "# Clustering and Unsupervised Learning\n",
    "\n",
    "Clustering is a key technique in unsupervised ML used to group similar data points without predefined labels. Its goal is to discover natural groupings in data, where items in the same cluster are more similar than those in different clusters. This makes clustering especially useful for Exploratory Data Analysis (EDA) to identify patterns, reveal hidden structures, and marks outliers. Rather than relying on a single method, clustering includes various algorithms, each with its own definition of similarity and approach to forming clusters. Here, we practice on a few of the most common models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3f4ed",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "K-means is a popular clustering algorithm used to group data into $K$ number of clusters. It works by iteratively assigning data points to the nearest cluster centroid and then updating the centroids based on the assigned points. Below, I illustrate how K-means iteratively updates centroids and cluster assignments using 3 initial centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c779fd",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../figures/kmeans_animation.gif\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba90c19",
   "metadata": {},
   "source": [
    "Let's load a dataset with multiple features that we aim to group into K clusters. Since we don't know what the data looks like or how many clusters it may contain, we start by exploring the dataset. We use `Pandas` to load and inspect the data before applying any clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame.\n",
    "df = pd.read_csv(\"../datasets/clustering_data.csv\", comment=\"#\").iloc[:, :-1] # We drop it, y, as not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152b7f1",
   "metadata": {},
   "source": [
    "Since the dataset is small, we can jump straight to visualizing it. For large datasets, this is not as straightforward or trivial. I intentionally set `alpha=0.5` to introduce transparency in the scatter plot, which helps you visually assess the density of overlapping data points. Using transparency is a simple yet powerful technique to enhance clarity and insight in dense visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2417821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(X):\n",
    "    assert X.shape[1] == 2, \"X must have exactly two features for scatter plot.\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker=\"o\",\n",
    "        c='grey', s=50, edgecolors=\"k\", linewidth=0.6, alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49824eda",
   "metadata": {},
   "source": [
    "We visualize x1 vs. x3 features. Our dataset contains 4 features: x1, x2, x3, and x4.\n",
    "Here, we only focus on x1 vs. x3. You can of course try different features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16344f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x1\", \"x3\"]].values  # Select two features for 2D scatter plot\n",
    "plot_scatter(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5088d",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- How many clusters do you see in the data?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777850d",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "There are different methods to detect outliers. Earlier, you tried Box Plots. Here, we want to try another method, called the Z-score. \n",
    "\n",
    "The Z-score (also known as a standard score in statistics) measures how far a data point is from the mean of a dataset in terms of standard deviations. It helps determine whether a specific value is above or below the average and by how much. We calculate the Z-score of a data point X as $Z = (X - \\mu)/\\sigma$, where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "A Z-score = 0 shows that the data point is exactly at the mean. A positive (negative) Z-score indicates that the data point is above (below) the mean. Z-scores are useful to identify outliers in a dataset. Typically, points with Z-scores beyond a threshold of ¬±3 are considered unusual and can be marked as outliers. \n",
    "\n",
    "In the code below we apply this method to our dataset and identify any potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc247132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score is part of the scipy library.\n",
    "from scipy import stats\n",
    "\n",
    "# Finding outliers using Z-score\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    z_scores = np.abs( stats.zscore(data))\n",
    "    outliers = (z_scores > threshold)\n",
    "    print(\"Number of outliers using Z-score > 3:\")\n",
    "    print(pd.DataFrame(outliers, columns=data.columns).sum())\n",
    "    \n",
    "detect_outliers_zscore(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b4d6e",
   "metadata": {},
   "source": [
    "***\n",
    "See the figure below. Do you see the Z-score > 3 and why it is insignificant and considered as outlier?\n",
    "<p align=\"center\">\n",
    "  <img src=\"../figures/Gaussian.png\" width=\"600\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdda3e",
   "metadata": {},
   "source": [
    "The code below applies the K-Means clustering to our dataset `X`, and making groups specified by the number of clusters (`n_clusters`). It uses the `fit_predict` method to both fit the model and assign each data point to the nearest cluster based on Euclidean distance. Once training is complete, the final cluster centroids and data labels (the cluster assignment) are used to analyze the clustering result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# plot KMeans clusters\n",
    "def plot_kmeans_clusters(X, n_clusters=3):\n",
    "    # Fit KMeans\n",
    "    #kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)  # Get the labels (cluster assignments) for each point\n",
    "\n",
    "    centroids = kmeans.cluster_centers_  # Get the centroids of the clusters    \n",
    "    print(\"Centroids:\\n\", np.round(centroids, 2))\n",
    "\n",
    "    plt.figure()   \n",
    "    plt.scatter(X[:, 0], X[:, 1], marker=\"o\",\n",
    "        c=[colors[i] for i in labels], s=50, edgecolors=\"k\", linewidth=0.6, alpha=0.5)\n",
    "\n",
    "    # plot centroids\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"X\",\n",
    "        c=\"red\", s=120, edgecolors=\"k\", linewidth=1.2, alpha=0.7, label=\"Centroids\")\n",
    "\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(f\"K-Means Clustering with k = {n_clusters}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad8e5d",
   "metadata": {},
   "source": [
    "First we try it with 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a70630",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(X, n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc65fb5",
   "metadata": {},
   "source": [
    "Then, with 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(X, n_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1541bb",
   "metadata": {},
   "source": [
    "You can change the method for the initial assignment of the centroids in the KMeans algorithm using the `init` parameter in Scikit-Learn's KMeans class. The two supported methods are `'k-means++'` and `'random'`. You can do this by:\n",
    "```python\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976ee6a",
   "metadata": {},
   "source": [
    "## Choosing the number of clusters\n",
    "\n",
    "The weakness of KMeans clustering is that we do not know how many clusters we need by just running the model. We need to test ranges of values and make a decision on the best value of K. We typically make a decision using the Elbow method to determine the optimal number of clusters where we aim neither **overfit** with too many clusters nor **underfit** with too few.\n",
    "\n",
    "Yes, the concepts of `overfitting` and `underfitting` also exist in unsupervised learning!\n",
    "\n",
    "In the code below we calculate the inertia (Within Cluster Sum of Squared Distances) for different values of K (from 1 to 10). We then plot the inertia against the number of clusters to visualize the \"elbow\" point, which guids us in selecting the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal number of clusters\n",
    "WCSS = []  # Within-Cluster Sum of Squares for each k. WCSS is also known as inertia.\n",
    "\n",
    "k_vals = range(1, 10)  # Test k from 1 to 9\n",
    "\n",
    "for k in k_vals:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    WCSS.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow\n",
    "plt.figure()\n",
    "plt.plot(k_vals, WCSS, \"--\", marker=\"o\", color=\"RoyalBlue\", linewidth=1.0, alpha=1.0)\n",
    "\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (or Inertia)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cf0cc",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Examine the inertia graph to determine the optimal value for K.\n",
    "- To enhance the clarity of the results, modify the figure based on the lecture notes to improve the WCSS  presentation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d981ad",
   "metadata": {},
   "source": [
    "## Evaluation Methods for KMeans Clustering\n",
    "\n",
    "In the code section below, we have used four main metrics to evaluate KMeans clustering: **WCSS**, **Silhouette Score**, **Davies-Bouldin Index**, and **Calinski-Harabasz Index**.\n",
    "\n",
    "1. **WCSS (inertia)**: This metric, as we used it earlier, measures the total variance within each cluster. It quantifies how tightly the data points in each cluster are packed. Lower WCSS values indicate better clustering, as they suggest that the clusters are more distinct.\n",
    "\n",
    "2. **Silhouette Score**: This score assesses how similar a data point is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a high score indicates that points are well clustered. A score close to 1 suggests that the points are far away from the neighboring clusters, and a score close to 0 indicates overlapping clusters (see lecture notes).\n",
    "\n",
    "3. **Davies-Bouldin Index**: We did not discuss this index in the class. It evaluates the average similarity ratio of each cluster with its most similar one. A lower Davies-Bouldin index indicates better clustering, as it suggests that clusters are well-separated and distinct from each other.\n",
    "\n",
    "4. **Calinski-Harabasz Index**: Also known as the \"Variance Ratio Criterion\", measures the ratio of the sum of between-cluster dispersion to within-cluster dispersion. A higher score indicates better defined clusters.\n",
    "\n",
    "By analyzing these metrics across different values of k, you can determine the most appropriate number of clusters for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Evaluate KMeans clustering using various metrics\n",
    "def evaluate_kmeans_clustering(X, k_range=range(2, 15)):\n",
    "    wcss        = []  # Saving Within-Cluster Sum of Squares (WCSS) for each k\n",
    "    silhouettes = []  # Saving Silhouette score for each k\n",
    "    db_scores   = []  # Saving Davies-Bouldin Index for each k\n",
    "    ch_scores   = []  # Saving Calinski-Harabasz Index for each k\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        silhouettes.append(silhouette_score(X, labels))\n",
    "        db_scores.append(davies_bouldin_score(X, labels))\n",
    "        ch_scores.append(calinski_harabasz_score(X, labels))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.subplot(221)\n",
    "\n",
    "    ax.plot(k_range, wcss, \"--\", marker=\"o\", color=\"RoyalBlue\", linewidth=1.0, alpha=0.7)\n",
    "    ax.set_xlabel(\"Number of Clusters (k)\")\n",
    "    ax.set_ylabel(\"WCSS\")\n",
    "    ax.set_title(\"Elbow Method\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = plt.subplot(222)\n",
    "    ax.plot(k_range, silhouettes, \"--\", marker=\"o\", color=\"tomato\", linewidth=1.0, alpha=0.7)\n",
    "    ax.set_xlabel(\"Number of Clusters (k)\")\n",
    "    ax.set_ylabel(\"Silhouette score\")\n",
    "    ax.set_title(\"Silhouette Score\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = plt.subplot(223)\n",
    "    ax.plot(k_range, db_scores, \"--\", marker=\"o\", color=\"forestgreen\", linewidth=1.0, alpha=0.7)\n",
    "    ax.set_xlabel(\"Number of Clusters (k)\")\n",
    "    ax.set_ylabel(\"DB Index (lower is Better)\")\n",
    "    ax.set_title(\"Davies‚ÄìBouldin Index\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = plt.subplot(224)\n",
    "    ax.plot(k_range, ch_scores, \"--\", marker=\"o\", color=\"purple\", linewidth=1.0, alpha=0.7)\n",
    "    ax.set_xlabel(\"Number of Clusters (k)\")\n",
    "    ax.set_ylabel(\"CH Score (higher is Better)\")\n",
    "    ax.set_title(\"Calinski-Harabasz Index\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "evaluate_kmeans_clustering(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd3b09",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Look at the scores and indexes you calculated. Which K do you choose and why?\n",
    "\n",
    "- Does the Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index agree with the Elbow method in determining the optimal number of clusters? Why or why not?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d3764",
   "metadata": {},
   "source": [
    "## Standardization of Features\n",
    "\n",
    "Oops! Did you notice that the features were not standardaized? :(\n",
    "\n",
    "It is important to scale the input features before you run KMeans, or the clusters may be very stretched and k-means will perform poorly. Scaling the features does not guarantee that all the clusters will be\n",
    "nice and spherical, but it generally helps k-means <span style=\"color:cyan\">[A. G√©ron]</span>\n",
    "\n",
    "**REMEMBER:** While standardizing is crucial, it should not be done blindly. Always consider the context of your data and the specific characteristics of the features. For example, if certain features represent categorical data encoded as numerical values, standardization may not be appropriate. Always assess the impact of standardization on your specific dataset and analysis.\n",
    "\n",
    "- When algorithms sensitive to scale or features have different units, we need to Standardize.\n",
    "\n",
    "- For tree-based models, and for binary features, we do not need to Standardize.\n",
    "\n",
    "Now we standardize our data. Here, we only do it for x1 and x3, because earlier we set \n",
    "```python\n",
    "X = df[[\"x1\", \"x3\"]].values \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe8b7f",
   "metadata": {},
   "source": [
    "Let's run our KMeans with 5 clusters after standardization. Compare the results with those you got earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(X_scaled, n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f99d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KMeans clustering on scaled data\n",
    "evaluate_kmeans_clustering(X_scaled, k_range=range(2, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81fe8a",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- Look at the scores and indexes you calculated from standardized data. Which K do you choose now? Compare it with your earlier choice.\n",
    "\n",
    "- Test your choice. In the CSV file, there is an extra column called **\"y\"** that we did not load before. Look at your data file. This column contains the true cluster labels. Since this is an unsupervised learning task, we did not need the labels earlier, so we masked it. But now, you can load the **\"y\"** values to check how well your KMeans results match the real labels. Use this to validate your chosen K.\n",
    "\n",
    "‚ö†Ô∏è **NOTE:** In unsupervised learning, we do not have access to the true labels. However, since this is a lab exercise, we can use the true labels to validate our clustering results.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55867ef",
   "metadata": {},
   "source": [
    "## Silhouette analysis\n",
    "\n",
    "Let's perform a silhouette analysis to evaluate the quality of clustering for our standardized dataset across a specified range of cluster numbers. The silhouette score is a metric that measures how similar each data point is to its own cluster compared to other clusters, with values ranging from -1 to 1; a higher score indicates better-defined clusters. Our code calculates the average silhouette score for each k, helping us to identifying the optimal number of clusters. We highlight the average index on each subplot with a dashed red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# Silhouette analysis to visualize the silhouette scores for different clusters\n",
    "def silhouette_analysis(X, k_range=range(2, 11)):\n",
    "    assert X.shape[1] == 2, \"X must have exactly 2 columns.\"\n",
    "\n",
    "    silhouette_avgs = [] # List to store average silhouette scores for each k\n",
    "\n",
    "    # Subplot layout for 2 columns\n",
    "    n_cols = 2\n",
    "    n_plots = len(k_range)\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 3 * n_rows), dpi=300)\n",
    "    axes = axes.flatten()  # Flatten to 1D list for easy indexing\n",
    "\n",
    "    for idx, k in enumerate(k_range):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        silhouette_vals = silhouette_samples(X, cluster_labels)\n",
    "        silhouette_avgs.append(silhouette_avg)\n",
    "\n",
    "        ax.set_xlim([-0.2, 1])\n",
    "        ax.set_ylim([0, len(X) + (k + 1) * 10])\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(k):\n",
    "            ith_vals = silhouette_vals[cluster_labels == i]\n",
    "            ith_vals.sort()\n",
    "\n",
    "            size_cluster_i = ith_vals.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = colors[i % len(colors)]\n",
    "            ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                             0, ith_vals,\n",
    "                             facecolor=color, edgecolor='k', alpha=0.5)\n",
    "\n",
    "            ax.text(0.0, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "        ax.text(-0.12, 0.5 * y_lower, \"Clusters\", rotation=90, va=\"center\", weight=\"bold\")\n",
    "        ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=3)\n",
    "\n",
    "        ax.set_xlabel(\"Silhouette score values\")\n",
    "        ax.set_ylabel(\"# of samples\")\n",
    "        ax.set_title(f\"k = {k}\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "silhouette_analysis(X_scaled, k_range=range(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55ab52",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- What does each subplot show? Make sure you fully understand them.\n",
    "\n",
    "- What do the negative scores show (e.g. see subplots for k=3 or k=7)?\n",
    "\n",
    "- For each value of k, you can compare the highlighted average score (red dashed line) with those you got earlier from `evaluate_kmeans_clustering` function (the previous code section).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c4ff1",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9169d83",
   "metadata": {},
   "source": [
    "Instead of KMeans, here we try DBSCAN (Density-Based Spatial Clustering of Applications with Noise) for clustering our scaled data, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d33d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def plot_dbscan_clusters(X, epsilon=0.4, min_samples=10):\n",
    "    # Standardize to make sure all features are on the same scale\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # DBSCAN\n",
    "    db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(X_scaled)\n",
    "    labels = db.labels_  # Get the labels (cluster assignments) for each point\n",
    "\n",
    "    # Cluster stats\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) # Number of clusters excluding noise\n",
    "    n_noise_ = list(labels).count(-1) # Number of noise points (outliers)\n",
    "\n",
    "    print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "    print(f\"Estimated number of noise points: {n_noise_}\")\n",
    "\n",
    "    # Core point mask\n",
    "    core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    unique_labels = set(labels)\n",
    "\n",
    "    for k in unique_labels:\n",
    "        col = 'black' if k == -1 else colors[k % len(colors)]\n",
    "        class_mask = labels == k\n",
    "\n",
    "        # Core points\n",
    "        xy_core = X_scaled[class_mask & core_samples_mask]\n",
    "        plt.scatter(xy_core[:, 0], xy_core[:, 1], marker=\"o\",\n",
    "            c=[col], s=80, edgecolors=\"k\", linewidth=0.6, alpha=0.5,\n",
    "            label=f\"Cluster {k}\" if k != -1 else \"Noise\")\n",
    "\n",
    "        # Border points\n",
    "        xy_border = X_scaled[class_mask & ~core_samples_mask]\n",
    "        plt.scatter(xy_border[:, 0], xy_border[:, 1], marker=\"o\",\n",
    "            c=[col], s=30, edgecolors=\"k\", linewidth=0.6, alpha=0.7)\n",
    "        \n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(f\"DBSCAN with {n_clusters_} clusters\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_dbscan_clusters(X_scaled, epsilon=0.4, min_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50603e7b",
   "metadata": {},
   "source": [
    "***\n",
    "### üí° Reflect and Run\n",
    "\n",
    "- How many clusters do you see in the data, and why? Read about DBSCAN.\n",
    "\n",
    "- Try modifying the DBSCAN clustering by setting the `epsilon` parameter to 0.2 and rerun the code. Observe how this change affects the number and shape of the clusters. \n",
    "\n",
    "- Find the most optimal `epsilon` value for your dataset with writing your code. You can use metrics like the silhouette to compare performance and determine which `epsilon` produces the \"best\" number of clusters.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e66b45",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models (GMM)\n",
    "\n",
    "Here we want to use GMM for clustering our scaled data, X. Before that, I've created a function to plot the results. After that, we use `GaussianMixture` from Scikit Learn to create clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm_clusters(X_scaled, labels, title):\n",
    "    plt.figure()\n",
    "\n",
    "    for k in set(labels):\n",
    "        mask = labels == k\n",
    "        plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1], marker=\"o\",\n",
    "                    color=colors[k % len(colors)], s=50, edgecolors=\"k\", linewidth=0.6, \n",
    "                    alpha=0.5, label=f\"Cluster {k}\")\n",
    "\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# GMM\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "gmm_n = gmm.n_components\n",
    "\n",
    "plot_gmm_clusters(X_scaled, gmm_labels, f\"GMM Clustering (k = {gmm_n})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58139cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_with_distributions(X, gmm, labels):\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot clusters\n",
    "    for k in set(labels):\n",
    "        mask = labels == k\n",
    "        plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1], marker=\"o\",\n",
    "                    color=colors[k % len(colors)], s=50, edgecolors=\"k\", linewidth=0.5, \n",
    "                    alpha=0.5, label=f\"Cluster {k}\")\n",
    "\n",
    "    # Grid over data bounds\n",
    "    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n",
    "\n",
    "    xs = np.linspace(x_min, x_max, 200)\n",
    "    ys = np.linspace(y_min, y_max, 200)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "\n",
    "    # Compute densities\n",
    "    scores = gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    scores = np.exp(scores)\n",
    "\n",
    "    # Plot density contours using percentiles\n",
    "    contour_levels = np.linspace(np.min(scores), np.max(scores), 8)\n",
    "    plt.contourf(xx, yy, scores.reshape(xx.shape), levels=contour_levels, cmap=\"gray_r\", alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot with distribusions\n",
    "plot_clusters_with_distributions(X_scaled, gmm, gmm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065907f8",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- For GMM, we need to identify `n_components`. Change it to the best k value you obtained earlier and re-run the GMM again and compare results with those obtained from KMeans and DBSCAN.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf954358",
   "metadata": {},
   "source": [
    "Instead of GMM, we use BGMM and the code section below does it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "# Bayesian GMM\n",
    "bgmm = BayesianGaussianMixture(n_components=10, random_state=42)\n",
    "bgmm_labels = bgmm.fit_predict(X_scaled)\n",
    "bgmm_n = len(set(bgmm_labels)) - (1 if -1 in bgmm_labels else 0)\n",
    "\n",
    "plot_gmm_clusters(X_scaled, bgmm_labels, f\"Bayesian GMM Clustering (k = {bgmm_n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064dee3",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚úÖ Check your understanding\n",
    "\n",
    "- How well each model (GMM and BGMM) separates the data into distinct clusters? \n",
    "\n",
    "- You may have noticed that `n_components` is set to 10 for `BayesianGaussianMixture`, while the actual number of resulting clusters (len(set(bgmm_labels))) is fewer, and in our example is 7. Why is that? What do we actually identify with setting `n_components` in `BayesianGaussianMixture`?\n",
    "\n",
    "**NOTE:** GMM relies on a fixed number of clusters, while BGMM can automatically infer the number of effective clusters, making it more flexible, especially when the true number of clusters is unknown.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a84780",
   "metadata": {},
   "source": [
    "# Image Segmentation\n",
    "\n",
    "Image segmentation is a technique in image processing that helps break an image into `\"meaningful\"` regions or segments. It plays an important role in understanding the content of an image. For example, separating different objects in a photo so we can tell whether there is a chair, a person, or any other item. This step usually comes before more advanced tasks like object recognition, feature extraction, or even image compression, which are beyond the scope of our course.\n",
    "\n",
    "The main idea behind image segmentation is to group together pixels that share similar characteristics, such as color or brightness, so that each group corresponds to a different region or object in the image. One of the most popular and simple methods to do this is clustering, especially using the `K-Means` algorithm. Yes, you read it correctly! K-Means works very well for identifying clusters of similar pixels and assigning each pixel to the nearest cluster center. It is fast and efficient.\n",
    "\n",
    "By using segmentation, we no longer need to analyze the whole image at once. Instead, we focus only on the important parts. This makes processing faster and more effective. \n",
    "\n",
    "In the next section, we briefly demonstrate how K-Means can successfully group similar pixels together and use this clustering to segment different components of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fb112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage import io\n",
    "\n",
    "# Original image filename and number of clusters\n",
    "original_filename = \"../figures/TF.jpg\"  # You can change this to any image file\n",
    "n_clusters = 3\n",
    "\n",
    "# Load image\n",
    "img = io.imread(original_filename)\n",
    "\n",
    "# Convert from RGBA or grayscale to RGB if needed\n",
    "if img.ndim == 2:\n",
    "    # Grayscale to RGB\n",
    "    img = np.stack([img] * 3, axis=-1)\n",
    "elif img.shape[2] == 4:\n",
    "    # RGBA to RGB\n",
    "    img = img[:, :, :3]\n",
    "\n",
    "# Resize image to half to speed up processing\n",
    "height, width = img.shape[:2]\n",
    "img = cv2.resize(img, (width // 2, height // 2))\n",
    "\n",
    "# Get original shape and reshape image to a 2D array of pixels\n",
    "# Normalize pixel values to [0, 1] for better clustering performance\n",
    "original_shape = img.shape\n",
    "all_pixels = img.reshape(-1, 3) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "print(f\"Original image shape: {original_shape},\\t Total pixels: {all_pixels.shape[0]}\")\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters, random_state=42)\n",
    "kmeans.fit(all_pixels)\n",
    "\n",
    "# Get cluster centers and labels\n",
    "colors = kmeans.cluster_centers_  # still in [0, 1]\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Build new image using cluster colors\n",
    "new_img = np.zeros_like(all_pixels)\n",
    "for ix in range(new_img.shape[0]):\n",
    "    new_img[ix] = colors[labels[ix]]\n",
    "\n",
    "# Rescale back to [0, 255] and reshape\n",
    "new_img = (new_img*255).astype('uint8').reshape(original_shape)\n",
    "\n",
    "# Plot segmented image\n",
    "plt.figure()\n",
    "plt.imshow(new_img)\n",
    "plt.title(\"KMeans Segmentation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9577be",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚ö° Mandatory submission\n",
    "\n",
    "You can try a different figure and change the number of clusters. Check out images in the \"figures\" folder.\n",
    "\n",
    "For example,\n",
    "```python\n",
    "original_filename = \"../figures/gotland.jpg\"\n",
    "n_clusters = 5\n",
    "```\n",
    "\n",
    "- How many clusters do you see in the image, and why?\n",
    "\n",
    "- Perform Silhouette analysis for your chosen number of clusters and report the average silhouette score.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c6cc7",
   "metadata": {},
   "source": [
    "Similar to our earlier code, the code below runs the KMeans on our image's pixel data for a range of cluster numbers (from 2 to 30). For each value of k, it fits a model and stores both the model itself and its WCSS value. These values are later used to identify the optimal number of clusters using the elbow method.\n",
    "\n",
    "We take advantage of `tqdm`, which is a Python library that adds a progress bar to loops, making it easier to monitor the progress of long-running operations. This is especially helpful when training models, processing large datasets, or performing tasks that take noticeable time. Read about it and try using it in your codes that contain loops and iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ed57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a library for progress bars\n",
    "import tqdm\n",
    "\n",
    "# Run KMeans for multiple k\n",
    "n_clusters = range(2, 31)\n",
    "wcss   = []\n",
    "models = []\n",
    "\n",
    "# Initialize lists to store WCSS and models\n",
    "# tqdm.trange is used to create a progress bar for the for loop\n",
    "for k in tqdm.trange(len(n_clusters), desc=\"Clustering\"):\n",
    "    model = KMeans(n_clusters=n_clusters[k], random_state=42)\n",
    "    model.fit(all_pixels)\n",
    "    wcss.append(model.inertia_)\n",
    "    models.append(model)\n",
    "\n",
    "# Plot inertia and elbow\n",
    "plt.figure()\n",
    "plt.plot(n_clusters, wcss, \"--\", marker=\"o\", color=\"RoyalBlue\", linewidth=1.0, alpha=0.7)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (or Inertia)\")\n",
    "plt.title(\"Elbow method to find optimal k\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d66a2",
   "metadata": {},
   "source": [
    "***\n",
    "### ‚õ∑Ô∏è Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd515d7",
   "metadata": {},
   "source": [
    "It is your turn now to work on a clustering task. I want you to work on a Customer Clustering task, which you can either download from Kaggle (see how to do it below), or use the \"customer_data.csv\" in your \"datasers\" folder.\n",
    "\n",
    "Your goal is to group similar customers together using unsupervised learning and reflect on what these clusters might represent in a business context.\n",
    "\n",
    "* Load and inspect the dataset. Focus on understanding what each column might represent. Think about what kind of patterns or segments might exist among the customers. Handle any missing values appropriately. If necessary, normalize the data so features contribute equally.\n",
    "\n",
    "* Use at least **three clustering algorithms**: one centroid-based (like KMeans), one density-based (like DBSCAN), and one probabilistic (like GMM). For each, decide how to set the number of clusters (or equivalent parameters) using visual or statistical techniques like the Elbow method, Silhouette scores, or BIC/AIC.\n",
    "\n",
    "* Also try hierarchical clustering with different linkage criteria: single, complete, average, and ward. You may also try Hierarchical DBSCAN, known as **HDBSCAN**. Plot the dendrogram to find a reasonable cut for clusters. Read about it at `sklearn.cluster.AgglomerativeClustering`.\n",
    "\n",
    "* Visualize the resulting clusters and discuss how they differ. Are there consistent patterns across models? Which model gives the most meaningful grouping? Can you describe the characteristics of each cluster? You are not expected to be precise ‚Äî just try to interpret what the model is finding.\n",
    "\n",
    "* Which clustering method did you find most useful, and why? What could be the potential use of these clusters for a company (e.g. marketing and targeting)?\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "* You do not need to write perfect code. Focus on **reasoning and analysis**.\n",
    "* Avoid over-engineering. Use **visual intuition** where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafa7c1",
   "metadata": {},
   "source": [
    "# Install Kaggle API\n",
    "\n",
    "To install the Kaggle API, run the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Upgrade pip\n",
    "pip install --upgrade pip\n",
    "\n",
    "# Install Kaggle API\n",
    "pip install kaggle\n",
    "```\n",
    "\n",
    "### Get Your Kaggle API Token\n",
    "\n",
    "1. Go to your Kaggle Account Settings: https://www.kaggle.com/settings\n",
    "2. Scroll down to the **API** section and click on **Create New API Token**. This will download a file named `kaggle.json`.\n",
    "\n",
    "### Save the API Token\n",
    "\n",
    "#### For Linux or macOS:\n",
    "1. Move the `kaggle.json` file to the `.kaggle` directory:\n",
    "   ```bash\n",
    "   mkdir -p ~/.kaggle\n",
    "   mv /path/to/downloaded/kaggle.json ~/.kaggle/\n",
    "   ```\n",
    "2. Set the permissions of the `kaggle.json` file:\n",
    "   ```bash\n",
    "   chmod 600 ~/.kaggle/kaggle.json\n",
    "   ```\n",
    "\n",
    "#### For Windows (I am not a Windows User, so this might not work):\n",
    "1. Move the `kaggle.json` file to the `.kaggle` directory:\n",
    "   - Create a directory named `.kaggle` in your user folder (e.g., `C:\\Users\\YourUsername\\.kaggle`).\n",
    "   - Move the `kaggle.json` file to this directory. You can do this using File Explorer or by running the following command in the Command Prompt:\n",
    "     ```cmd\n",
    "     mkdir C:\\Users\\YourUsername\\.kaggle\n",
    "     move C:\\path\\to\\downloaded\\kaggle.json C:\\Users\\YourUsername\\.kaggle\\\n",
    "     ```\n",
    "2. Ensure the permissions are set correctly:\n",
    "   - In Windows, you typically don‚Äôt need to set permissions, but you can ensure that the file is not accessible by others by right-clicking the file, selecting **Properties**, going to the **Security** tab, and changing the permissions as necessary.\n",
    "\n",
    "\n",
    "### Get/Download a Kaggle dataset\n",
    "```python\n",
    "    import os\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Example: to download CERN electron collision dataset from \n",
    "    #      https://www.kaggle.com/datasets/fedesoriano/cern-electron-collision-data\n",
    "    #  you need to do the following.\n",
    "    #  Only need part of the URL for the code below:\n",
    "    api.dataset_download_files(\"fedesoriano/cern-electron-collision-data\", \n",
    "        path='/local_path_to_store_data/', unzip=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9d126",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
