{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6172e44",
   "metadata": {},
   "source": [
    "**Author:** Shahab Fatemi\n",
    "\n",
    "**Email:** shahab.fatemi@umu.se   ;   shahab.fatemi@amitiscode.com\n",
    "\n",
    "**Created:** 2024-11-xx\n",
    "\n",
    "**Last update:** 2025-08-24\n",
    "\n",
    "**MIT License** — Shahab Fatemi (2025); For use in the *Machine Learning in Physics* course, Umeå University, Sweden; See the full license text in the parent folder.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0766fe7",
   "metadata": {},
   "source": [
    "# Practical Data Analysis\n",
    "\n",
    "In this notebook, we want to work with the data we have collected and perform some analysis on it. This will involve cleaning the data, exploring it visually, and applying some statistical methods to gain insights. We keep everything simple and focuse on the key aspects of the data.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3074d7",
   "metadata": {},
   "source": [
    "We aim to analyze the weather data collected for Chicago, Illinois, over a period of nearly nine years, from January 1, 2015, to December 31, 2023. This dataset provides information on the temperature trends, precipitation patterns, wind conditions, and more.\n",
    "\n",
    "### Data Source\n",
    "\n",
    "I collected this data from [Meteostat](https://meteostat.net).\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "The dataset consists of the following columns:\n",
    "\n",
    "| Column | Description                       |\n",
    "|--------|---------------------------------  |\n",
    "| `date` | Date of the recorded weather data |\n",
    "| `tavg` | Average Temperature (°C)          |\n",
    "| `tmin` | Minimum Temperature (°C)          |\n",
    "| `tmax` | Maximum Temperature (°C)          |\n",
    "| `prcp` | Total Precipitation (mm)          |\n",
    "| `snow` | Snow Depth (cm)                   |\n",
    "| `wdir` | Wind Direction (degrees)          |\n",
    "| `wspd` | Wind Speed (km/h)                 |\n",
    "| `pres` | Air Pressure (hPa)                |\n",
    "| `weather` | Weather Condition (descriptive)|\n",
    "\n",
    "### Tools and methods\n",
    "\n",
    "To conduct this analysis, we will utilize various tools and libraries, including:\n",
    "\n",
    "- **Pandas**: To handle and analyze the dataset.\n",
    "- **Matplotlib and Seaborn**: For data visualization.\n",
    "\n",
    "We did not explicitly discuss `Pandas` earlier. However, through this practical project, we will also learn basics of `Pandas`.\n",
    "\n",
    "### Main Steps\n",
    "\n",
    "- **Data Loading**: Load the dataset into a Pandas DataFrame.\n",
    "- **Data Cleaning**: Perform necessary data cleaning steps, such as handling missing values and converting data types.\n",
    "- **Exploratory Data Analysis (EDA)**: Conduct an EDA to understand the dataset better and derive preliminary insights.\n",
    "- **Visualization**: Create visualizations to communicate findings effectively.\n",
    "- **Conclusion**: Summarize the analysis and discuss potential future work based on the results.\n",
    "\n",
    "Let's get started!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aee707",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "In this step, we will load the weather dataset into a Pandas DataFrame for analysis.\n",
    "\n",
    "`Pandas` is a very important library in Python, designed to provide flexible and powerful data structures that facilitate working with structured data, such as time series and tabular data. One of its primary features is the `DataFrame`, a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure that organizes data into rows and columns. Each column can contain different data types, and both rows and columns are labeled, making it easy to access and manipulate data. Pandas is widely used in data science, machine learning, and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9685732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the weather dataset from a CSV file and ignore commented lines denoted by '#' in the file.\n",
    "weather_data = pd.read_csv('../datasets/Chicago_weather.csv', comment='#')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af352953",
   "metadata": {},
   "source": [
    "### Data viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the first 5 rows of the data\n",
    "weather_data.head()\n",
    "\n",
    "# OR\n",
    "weather_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a12d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the last 5 rows\n",
    "weather_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac95d8e",
   "metadata": {},
   "source": [
    "Next, we use `info()` to get a concise summary of the DataFrame, including the number of non-null entries, data types of each column, and memory usage. This is useful for understanding the structure of the data, especially with regards to missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059c6d8",
   "metadata": {},
   "source": [
    "⚠️ We immediately notice that `data` and `weather` attribute have the data type of \"object\". We will come back to this issue later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf99d5a",
   "metadata": {},
   "source": [
    "Now, we use `describe()` to generate descriptive statistics for numeric columns, including count, mean, standard deviation, min, max, and quartiles. This helps in getting a quick statistical overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b87c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d5c4e",
   "metadata": {},
   "source": [
    "### Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f737c",
   "metadata": {},
   "source": [
    "You can access a specific column by its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_temperature = weather_data['tavg']  # Access the 'tavg' column\n",
    "\n",
    "avg_temperature.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07a063",
   "metadata": {},
   "source": [
    "You can access specific rows using loc (label-based) or iloc (integer-based) indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = weather_data.loc[0]  # Access the first row\n",
    "first_row_by_index = weather_data.iloc[0]  # Also accesses the first row\n",
    "\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03aef6c",
   "metadata": {},
   "source": [
    "### Data Filtering\n",
    "\n",
    "You can filter the DataFrame based on conditions. For example, to find rows where the average temperature is above 30 C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_days = weather_data[weather_data['tavg'] > 30]\n",
    "\n",
    "hot_days.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec641354",
   "metadata": {},
   "source": [
    "### Modifying the DataFrame\n",
    "\n",
    "You can create new columns based on existing data. For example, create a column for the temperature in Fahrenheit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['tavg_f'] = (weather_data['tavg'] * 9/5) + 32\n",
    "\n",
    "# Display the updated DataFrame, and see the new column appearing as the last column.\n",
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab39e1",
   "metadata": {},
   "source": [
    "You can remove unwanted columns using the drop() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa23b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.drop(columns=['tavg_f'])  # Drop our newly made 'tavg_f' column\n",
    "\n",
    "weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f23120",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "We noted earlier that `weather` attribute has the data type of \"object\". In data analysis and Machine Learning algorithms, we work with numbers. Therefore, any non-number value should be converted to number.\n",
    "\n",
    "In order to do this, we need to find the unique occurrences of each distinct value in the `weather` column. One way is through: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['weather'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f3785",
   "metadata": {},
   "source": [
    "Now, we know there are 4 unique weather conditions: 'Sunny', 'Rainy', 'Snowy', and 'Misty' in our dataset. We can map these conditions to numerical values. One way to do this is by using the `factorize` function from pandas. In this method, each unique category in `weather` is assigned a unique integer. The `factorize` function also returns the unique values found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2542175",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['weather_encoded'], weather_categories = pd.factorize(weather_data['weather'])\n",
    "\n",
    "print(\"Unique encoded values:\", weather_categories)\n",
    "\n",
    "weather_data['weather_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e4d4e0",
   "metadata": {},
   "source": [
    "In case you did not notice earlier, our dataset contains NaNs and Nulls. You could see it earlier when we explored the dataset using `info()`. We should handle these missing values before proceeding with data analysis. Depending on the purpose of the analysis, we might choose to fill these missing values or drop them. A common approach is to fill the missing values with a specific value, such as the mean or median of the column. Let's fill the missing values with the mean of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0957e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of each column for numerical features only.\n",
    "weather_data = weather_data.fillna(weather_data.mean(numeric_only=True))\n",
    "\n",
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ef03a",
   "metadata": {},
   "source": [
    "⚠️ Depending on the purpose of the analysis, you might need to keep the original data and create a copy for analysis. This way, you can always refer back to the original dataset if needed. However, since I did not want to make it complicated here, I work with the modified dataset directly. BUT, keep in mind that this approach may not be suitable for all situations, especially if you need to compare the results with the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c43d5",
   "metadata": {},
   "source": [
    "### Data augmentation \n",
    "\n",
    "We can augment our dataset by creating new samples from the existing ones. This is particularly useful in Machine Learning where we have limited data. Let's create an extra column to the data that consists of the season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column based on the month extracted from the 'date' column.\n",
    "# Not an accurate representation of the seasons!\n",
    "def get_season(date_str):\n",
    "    month = int(date_str.split('-')[1])\n",
    "    if month in [12, 1, 2]:\n",
    "        return 4   # Winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 1   # Spring\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 2   # Summer\n",
    "    else:\n",
    "        return 3   # Autumn\n",
    "\n",
    "# Create a new column in the DataFrame by applying the get_season function to the 'date' column\n",
    "weather_data['season'] = weather_data['date'].apply(get_season)\n",
    "\n",
    "weather_data['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fae6d",
   "metadata": {},
   "source": [
    "Note that the season function is not accurate, as it does not account for the actual start and end dates of each season. I wanted to keep the function simple, but for a more accurate representation, consider using the actual seasonal dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a01c5",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5f826",
   "metadata": {},
   "source": [
    "Let's visualize the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.hist(bins=50, figsize=(12, 8))\n",
    "plt.tight_layout()\n",
    "\n",
    "# If you want to change the figure dpi, you can only do it when saving the figure:\n",
    "## plt.savefig(\"weather_hist.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966aa21c",
   "metadata": {},
   "source": [
    "In the code below, we plot the distribution of average temperatures in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f05625",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['tavg'].hist(bins=50, \n",
    "                          figsize=(6, 4), \n",
    "                          color='skyblue', \n",
    "                          edgecolor='k')\n",
    "plt.xlabel(\"Average Temperature\")\n",
    "plt.ylabel(\"Data Frequency\")\n",
    "plt.title(\"Distribution of Average Temperature\")\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d09fe",
   "metadata": {},
   "source": [
    "Another way to visualize the distribution of average temperatures is by using the Kernel Density Estimate (KDE) plot in Seaborn.\n",
    "I, *personally*, prefer using Seaborn over the visualization functionalities existing in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "sns.histplot(weather_data['tavg'], bins=50, color=\"forestgreen\", kde=True)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9cc9a",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "We want to understand the relationships between different weather variables. Correlation analysis helps us identify which variables are positively or negatively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2a236",
   "metadata": {},
   "source": [
    "We can visualize the correlation matrix using Pandas' scatter_matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6baffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(weather_data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a41fa9",
   "metadata": {},
   "source": [
    "Or, we can use Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix (only numeric columns)\n",
    "corr = weather_data.corr(numeric_only=True)\n",
    "\n",
    "# Show heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, \n",
    "            annot=True, fmt=\".2f\", \n",
    "            vmax=1.0, vmin=-1.0, \n",
    "            linewidths=0.1,\n",
    "            cmap=\"coolwarm\", \n",
    "            square=True, cbar=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08435bde",
   "metadata": {},
   "source": [
    "There is a lot to see in the heatmap above. We can also only show those that are strongly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.3)],  # Just for example. We know the thresholds can be adjusted.\n",
    "            annot=True, fmt=\".2f\", \n",
    "            vmax=1.0, vmin=-1.0, \n",
    "            linewidths=0.1,\n",
    "            cmap=\"coolwarm\", \n",
    "            square=True, cbar=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a75ae4",
   "metadata": {},
   "source": [
    "Let's visualize the correlation between temperature and pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74345d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=weather_data, \n",
    "            x=\"tavg\", \n",
    "            y=\"pres\", \n",
    "            #color='forestgreen',\n",
    "            ci=90,  # Confidence interval for the regression line\n",
    "            line_kws={'color': 'darkred'})\n",
    "plt.title(\"Temperature vs. Pressure\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99fb91d",
   "metadata": {},
   "source": [
    "You can do a lot more, which you will learn the rest throughout the MLP course.\n",
    "\n",
    "***\n",
    "END\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
